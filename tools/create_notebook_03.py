#!/usr/bin/env python
"""
Generate Notebook 03: Population Geometry & Dimensionality Reduction
====================================================================

Assembles 5 sections into a single Colab-ready Jupyter notebook using
nbformat:
  1. DR API quick reference (MVData, DRMethod, ProximityGraph spectral)
  2. Method comparison (Swiss Roll benchmark)
  3. Autoencoder-based DR (AE, Beta-VAE)
  4. Circular manifold & dimensionality estimation
  5. INTENSE-guided DR (neuron selection for cleaner embeddings)
"""

import os
import nbformat

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def md_cell(source):
    """Create a markdown cell."""
    return nbformat.v4.new_markdown_cell(source.strip())


def code_cell(source):
    """Create a code cell."""
    return nbformat.v4.new_code_cell(source.strip())


# ---------------------------------------------------------------------------
# Build cells
# ---------------------------------------------------------------------------

cells = []

# ===== HEADER + SETUP =====================================================

cells.append(md_cell(
"# Population geometry & dimensionality reduction\n"
"\n"
"Individual neurons encode specific variables (Notebook 02), but the\n"
"population *as a whole* forms a low-dimensional manifold whose geometry\n"
"reflects the task.  [**DRIADA**](https://driada.readthedocs.io) provides\n"
"a unified DR toolkit to extract, compare, and evaluate these manifolds.\n"
"\n"
"| Step | Notebook | What it does |\n"
"|---|---|---|\n"
"| **Overview** | [00 -- DRIADA overview](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/00_driada_overview.ipynb) | Core data structures, quick tour of INTENSE, DR, networks |\n"
"| Neuron analysis | [01 -- Neuron analysis](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/01_data_loading_and_neurons.ipynb) | Spike reconstruction, kinetics optimization, quality metrics, surrogates |\n"
"| Single-neuron selectivity | [02 -- INTENSE](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/02_selectivity_detection_intense.ipynb) | Detect which neurons encode which behavioral variables |\n"
"| **Population geometry** | **03 -- this notebook** | Extract low-dimensional manifolds from population activity |\n"
"| Network analysis | [04 -- Networks](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/04_network_analysis.ipynb) | Build and analyze cell-cell interaction graphs |\n"
"| Putting it together | [05 -- Advanced](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/05_advanced_capabilities.ipynb) | Combine INTENSE + DR, leave-one-out importance, RSA, RNN analysis |\n"
"\n"
"**Sections:**\n"
"\n"
"1. **DR API quick reference** -- [`MVData`](https://driada.readthedocs.io/en/latest/api/dim_reduction/data_structures.html#driada.dim_reduction.data.MVData) wraps a matrix and provides\n"
"   one-line access to 15 DR methods.\n"
"2. **Method comparison** -- Benchmark PCA, Isomap, and UMAP on a Swiss\n"
"   roll with quality metrics.\n"
"3. **Autoencoder-based DR** -- Standard AE with `continue_learning`,\n"
"   Beta-VAE, compared against PCA. Requires PyTorch.\n"
"4. **Circular manifold & dimensionality estimation** -- Head direction\n"
"   cells encode a ring. Extract it via DR and estimate intrinsic\n"
"   dimensionality.\n"
"5. **INTENSE-guided DR** -- Use INTENSE to select neurons before DR.\n"
"   Selective neurons produce cleaner spatial embeddings."
))

cells.append(code_cell(
"# TODO: revert to '!pip install -q driada' after v1.0.0 PyPI release\n"
"!pip install -q git+https://github.com/iabs-neuro/driada.git@main\n"
"%matplotlib inline\n"
"\n"
"import numpy as np\n"
"import matplotlib.pyplot as plt\n"
"import time\n"
"import warnings\n"
"\n"
"from sklearn.datasets import make_swiss_roll\n"
"from scipy.sparse import csr_matrix\n"
"\n"
"# DRIADA dimensionality reduction\n"
"from driada.dim_reduction import (\n"
"    MVData,\n"
"    dr_sequence,\n"
"    knn_preservation_rate,\n"
"    trustworthiness,\n"
"    continuity,\n"
"    stress,\n"
")\n"
"from driada.dim_reduction.manifold_metrics import (\n"
"    compute_embedding_alignment_metrics,\n"
"    procrustes_analysis,\n"
")\n"
"\n"
"# DRIADA network analysis (used for ProximityGraph demo in Section 1.3)\n"
"from driada.network import Network\n"
"\n"
"# DRIADA experiment / synthetic data\n"
"from driada.experiment.synthetic import generate_circular_manifold_data\n"
"from driada.experiment import generate_circular_manifold_exp\n"
"\n"
"# DRIADA dimensionality estimation\n"
"from driada.dimensionality import (\n"
"    eff_dim,\n"
"    correlation_dimension,\n"
"    geodesic_dimension,\n"
"    pca_dimension,\n"
")\n"
"\n"
"# DRIADA INTENSE + mixed population\n"
"from driada import (\n"
"    compute_cell_feat_significance,\n"
"    generate_mixed_population_exp,\n"
")\n"
"from driada.utils import (\n"
"    compute_spatial_decoding_accuracy,\n"
"    compute_spatial_information,\n"
")\n"
"\n"
"# DRIADA visualization\n"
"from driada.utils.visual import (\n"
"    visualize_circular_manifold,\n"
"    plot_trajectories,\n"
"    plot_embeddings_grid,\n"
"    plot_neuron_selectivity_summary,\n"
"    DEFAULT_DPI,\n"
")\n"
"\n"
"# Suppress warnings for cleaner output\n"
"warnings.filterwarnings('ignore', category=UserWarning)"
))

# ===== SECTION 1: DR API QUICK REFERENCE ==================================

cells.append(md_cell(
"## 1. DR API quick reference\n"
"\n"
"[`MVData`](https://driada.readthedocs.io/en/latest/api/dim_reduction/data_structures.html#driada.dim_reduction.data.MVData) wraps a *(n_features x n_samples)* matrix and provides one-line\n"
"DR via [`get_embedding`](https://driada.readthedocs.io/en/latest/api/dim_reduction/data_structures.html#driada.dim_reduction.data.MVData.get_embedding).\n"
"For multi-step pipelines, see [`dr_sequence`](https://driada.readthedocs.io/en/latest/api/dim_reduction/algorithms.html#driada.dim_reduction.sequences.dr_sequence).\n"
"\n"
"Each method is described by a\n"
"[`DRMethod`](https://driada.readthedocs.io/en/latest/api/dim_reduction/data_structures.html#driada.dim_reduction.dr_base.DRMethod)\n"
"object that specifies whether it is linear, requires a proximity graph,\n"
"distance matrix, or neural network.\n"
"\n"
"| Method | Type | Graph | Disconnected |\n"
"|---|---|---|---|\n"
"| `pca` | Linear | -- | -- |\n"
"| `le` / `auto_le` | Spectral | k-NN | No |\n"
"| `dmaps` / `auto_dmaps` | Spectral | k-NN (weighted) | No |\n"
"| `isomap` | Geodesic | k-NN | No |\n"
"| `lle` / `hlle` | Local linear | k-NN | No |\n"
"| `mvu` | SDP | k-NN | No |\n"
"| `mds` | Distance | -- (needs dist matrix) | -- |\n"
"| `tsne` | Probabilistic | -- | -- |\n"
"| `umap` | Topological | k-NN | **Yes** |\n"
"| `ae` / `vae` / `flexible_ae` | Neural network | -- | -- |\n"
"\n"
"Graph-based methods (9 of 15) construct a\n"
"[`ProximityGraph`](https://driada.readthedocs.io/en/latest/api/dim_reduction/data_structures.html#driada.dim_reduction.graph.ProximityGraph)\n"
"that inherits from\n"
"[`Network`](https://driada.readthedocs.io/en/latest/api/network/core.html#driada.network.net_base.Network),\n"
"giving access to spectral analysis, entropy, and community detection\n"
"(see cells below and [Notebook 04](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/04_network_analysis.ipynb))."
))

cells.append(code_cell(
"# Generate Swiss roll data for demonstration\n"
"n_samples = 1000\n"
"X_raw, color = make_swiss_roll(n_samples, noise=0.1, random_state=42)\n"
"X = X_raw.T  # Transpose to match MVData format (features x samples)\n"
"\n"
"# Create MVData object\n"
"mvdata = MVData(X)\n"
"\n"
"emb_pca = mvdata.get_embedding(method='pca')\n"
"print(f'PCA: {emb_pca.coords.shape}')\n"
"\n"
"emb_iso = mvdata.get_embedding(method='isomap')\n"
"print(f'Isomap: {emb_iso.coords.shape}')\n"
"\n"
"emb_le = mvdata.get_embedding(method='le')\n"
"print(f'Laplacian Eigenmaps: {emb_le.coords.shape}')"
))

cells.append(code_cell(
"emb_umap = mvdata.get_embedding(method='umap', n_neighbors=50, min_dist=0.3)\n"
"print(f'UMAP: {emb_umap.coords.shape}')"
))

cells.append(code_cell(
"# Visualize PCA, Isomap, UMAP, LE side by side\n"
"fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n"
"axes = axes.ravel()\n"
"\n"
"for i, (emb, name) in enumerate(zip(\n"
"    [emb_pca, emb_iso, emb_umap, emb_le],\n"
"    ['PCA', 'Isomap', 'UMAP', 'Laplacian Eigenmaps'],\n"
")):\n"
"    ax = axes[i]\n"
"    coords = emb.coords\n"
"\n"
"    scatter = ax.scatter(\n"
"        coords[0, :], coords[1, :], c=color, cmap='viridis', s=20, alpha=0.7\n"
"    )\n"
"    ax.set_title(f'{name} Embedding')\n"
"    ax.set_xlabel('Component 1')\n"
"    ax.set_ylabel('Component 2')\n"
"\n"
"    # Add colorbar to first subplot\n"
"    if i == 0:\n"
"        plt.colorbar(scatter, ax=ax, label='Position on roll')\n"
"\n"
"plt.tight_layout()\n"
"plt.show()"
))

cells.append(md_cell(
"### Advanced: sequential DR, custom metrics"
))

cells.append(code_cell(
"# Sequential dimensionality reduction (PCA -> UMAP)\n"
"\n"
"# Generate high-dimensional data\n"
"high_dim_data = np.random.randn(100, 500)  # 100 features, 500 samples\n"
"mvdata_highdim = MVData(high_dim_data)\n"
"\n"
"print('\\n1. High-dimensional data:')\n"
"emb_10d = mvdata_highdim.get_embedding(method='pca', dim=10)\n"
"print(f'   -> 10D embedding shape: {emb_10d.coords.shape}')\n"
"\n"
"mvdata_10d = MVData(emb_10d.coords)\n"
"emb_2d = mvdata_10d.get_embedding(method='umap')\n"
"print(f'   -> Final 2D embedding shape: {emb_2d.coords.shape}')\n"
"\n"
"print('\\n2. Using custom metrics:')\n"
"emb_cosine = mvdata.get_embedding(method='isomap', metric='cosine')\n"
"print(f'   -> Cosine metric embedding shape: {emb_cosine.coords.shape}')\n"
"\n"
"print('\\n3. Handling sparse data:')\n"
"sparse_data = csr_matrix(X)\n"
"print(f'   -> Sparse matrix shape: {sparse_data.shape}')\n"
"mvdata_sparse = MVData(sparse_data)\n"
"emb_sparse = mvdata_sparse.get_embedding(method='pca')\n"
"print(f'   -> Sparse data embedding shape: {emb_sparse.coords.shape}')\n"
"\n"
"print('\\n4. Sequential dimensionality reduction (use high-dim data):')\n"
"print('   Method 1 (intuitive - manual chaining):')\n"
"emb1_seq = mvdata_highdim.get_embedding(method='pca', dim=20)\n"
"mvdata2_seq = MVData(emb1_seq.coords)\n"
"emb2_seq = mvdata2_seq.get_embedding(method='umap', dim=2)\n"
"print(f'   -> Result shape: {emb2_seq.coords.shape}')\n"
"\n"
"print('\\n   Method 2 (recommended - using dr_sequence):')\n"
"emb_seq = dr_sequence(mvdata_highdim, steps=[\n"
"    ('pca', {'dim': 20}),\n"
"    ('umap', {'dim': 2})\n"
"])\n"
"print(f'   -> Result shape: {emb_seq.coords.shape}')"
))

cells.append(md_cell(
"### Graph structure behind DR\n"
"\n"
"Graph-based DR methods (Isomap, LLE, Laplacian Eigenmaps) don't just\n"
"produce coordinates -- they construct an internal **proximity graph** where\n"
"nodes are data points and edges connect neighbors. In DRIADA, this graph\n"
"is a [`ProximityGraph`](https://driada.readthedocs.io/en/latest/api/dim_reduction/data_structures.html#driada.dim_reduction.graph.ProximityGraph)\n"
"that **inherits from [`Network`](https://driada.readthedocs.io/en/latest/api/network/core.html#driada.network.net_base.Network)**,\n"
"giving you access to spectral decomposition, entropy, community detection,\n"
"and all other `Network` analysis methods.\n"
"\n"
"Access it via `embedding.graph` after running any graph-based method.\n"
"For a full treatment of network spectral analysis, see\n"
"[Notebook 04 -- Network analysis](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/04_network_analysis.ipynb)."
))

cells.append(code_cell(
"pgraph = emb_iso.graph\n"
"\n"
"print(f\"Type: {type(pgraph).__name__}\")\n"
"print(f\"  inherits from Network: {isinstance(pgraph, Network)}\")\n"
"print(f\"Nodes: {pgraph.n}\")\n"
"print(f\"Edges: {pgraph.adj.nnz // 2}\")\n"
"print(f\"Mean degree: {pgraph.deg.mean():.1f}\")\n"
"print(f\"Metric used: {pgraph.metric}\")"
))

cells.append(code_cell(
"pgraph.diagonalize(mode='nlap')\n"
"nlap_spectrum = pgraph.get_spectrum('nlap')\n"
"ipr = pgraph.get_ipr('nlap')\n"
"\n"
"fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n"
"\n"
"# Normalized Laplacian spectrum\n"
"sorted_spec = np.sort(np.real(nlap_spectrum))\n"
"axes[0].plot(sorted_spec, 'o', markersize=2)\n"
"axes[0].set_xlabel('Index')\n"
"axes[0].set_ylabel('Eigenvalue')\n"
"axes[0].set_title('Normalized Laplacian spectrum')\n"
"axes[0].grid(True, alpha=0.3)\n"
"\n"
"# IPR -- eigenvector localization\n"
"axes[1].plot(np.sort(ipr), 'o', markersize=2)\n"
"axes[1].axhline(1.0 / pgraph.n, color='r', linestyle='--',\n"
"                label=f'1/N = {1.0/pgraph.n:.4f}')\n"
"axes[1].set_xlabel('Eigenvector index (sorted)')\n"
"axes[1].set_ylabel('IPR')\n"
"axes[1].set_title('Inverse participation ratio')\n"
"axes[1].legend(fontsize=9)\n"
"axes[1].grid(True, alpha=0.3)\n"
"\n"
"# Thermodynamic entropy\n"
"tlist = np.logspace(-2, 2, 50)\n"
"entropy = pgraph.calculate_thermodynamic_entropy(tlist, norm=True)\n"
"axes[2].semilogx(tlist, entropy, linewidth=2)\n"
"axes[2].set_xlabel('Temperature')\n"
"axes[2].set_ylabel('Entropy (bits)')\n"
"axes[2].set_title('Von Neumann entropy S(t)')\n"
"axes[2].grid(True, alpha=0.3)\n"
"\n"
"plt.suptitle('Spectral analysis of Isomap k-NN graph', fontsize=14)\n"
"plt.tight_layout()\n"
"plt.show()\n"
"\n"
"print(f'Fiedler value: {sorted_spec[1]:.4f}')\n"
"print(f'Spectral gap: {sorted_spec[1] - sorted_spec[0]:.4f}')\n"
"print(f'Max entropy: {np.max(entropy):.2f} bits '\n"
"      f'(upper bound = log2(N) = {np.log2(pgraph.n):.2f})')"
))

cells.append(md_cell(
"The Laplacian spectrum reveals the graph's connectivity structure:\n"
"a large spectral gap indicates the graph is well-connected, while\n"
"clustered eigenvalues near zero suggest loosely connected components.\n"
"The IPR shows whether eigenvectors are delocalized (spread across\n"
"all nodes) or localized (concentrated on a few).\n"
"\n"
"These same spectral tools apply to *any* [`Network`](https://driada.readthedocs.io/en/latest/api/network/core.html#driada.network.net_base.Network) -- functional\n"
"connectivity from INTENSE, structural connectomes, or correlation\n"
"networks. See [Notebook 04](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/04_network_analysis.ipynb)\n"
"for the full spectral analysis toolkit."
))

# ===== SECTION 2: METHOD COMPARISON =======================================

cells.append(md_cell(
"## 2. Method comparison\n"
"\n"
"The API handles the mechanics. But which method should you use? The following\n"
"benchmark on a Swiss Roll manifold reveals the trade-offs.\n"
"\n"
"DRIADA provides four quality metrics to evaluate DR embeddings:\n"
"\n"
"- [`knn_preservation_rate`](https://driada.readthedocs.io/en/latest/api/dim_reduction/manifold_metrics.html#driada.dim_reduction.manifold_metrics.knn_preservation_rate) -- fraction of original k nearest neighbors preserved.\n"
"- [`trustworthiness`](https://driada.readthedocs.io/en/latest/api/dim_reduction/manifold_metrics.html#driada.dim_reduction.manifold_metrics.trustworthiness) -- are embedding neighbors real neighbors?\n"
"- [`continuity`](https://driada.readthedocs.io/en/latest/api/dim_reduction/manifold_metrics.html#driada.dim_reduction.manifold_metrics.continuity) -- do true neighbors stay close?\n"
"- [`stress`](https://driada.readthedocs.io/en/latest/api/dim_reduction/manifold_metrics.html#driada.dim_reduction.manifold_metrics.stress) -- normalized distance distortion.\n"
"\n"
"We compare PCA, Isomap, and UMAP on a Swiss roll."
))

cells.append(code_cell(
"# Compare PCA, Isomap, UMAP on Swiss roll\n"
"X_swiss, color_swiss = make_swiss_roll(1000, noise=0.05, random_state=42)\n"
"\n"
"methods_cmp = {\n"
"    'PCA': {},\n"
"    'Isomap': {'n_neighbors': 10, 'max_deleted_nodes': 0.3},  # 10 neighbors: local geodesics\n"
"    'UMAP': {'n_neighbors': 15, 'min_dist': 0.1},  # min_dist: tighter clusters\n"
"}\n"
"cmp_results = {}\n"
"\n"
"for name, params in methods_cmp.items():\n"
"    mvd = MVData(X_swiss.T)\n"
"    t0 = time.time()\n"
"    emb = mvd.get_embedding(method=name.lower(), **params)\n"
"    dt = time.time() - t0\n"
"    coords = emb.coords.T\n"
"\n"
"    # Handle lost nodes for graph-based methods\n"
"    data_cmp, color_cmp = X_swiss, color_swiss\n"
"    if hasattr(emb, 'graph') and hasattr(emb.graph, 'lost_nodes') and len(emb.graph.lost_nodes) > 0:\n"
"        mask = np.ones(len(X_swiss), dtype=bool)\n"
"        mask[emb.graph.lost_nodes] = False\n"
"        data_cmp, color_cmp = X_swiss[mask], color_swiss[mask]\n"
"\n"
"    cmp_results[name] = {\n"
"        'coords': coords, 'color': color_cmp, 'time': dt,\n"
"        'knn': knn_preservation_rate(data_cmp, coords, k=10),\n"
"        'trust': trustworthiness(data_cmp, coords, k=10),\n"
"        'cont': continuity(data_cmp, coords, k=10),\n"
"        'stress': stress(data_cmp, coords, normalized=True),\n"
"    }\n"
"    print(f'{name:8s}  k-NN={cmp_results[name][\"knn\"]:.3f}  '\n"
"          f'Trust={cmp_results[name][\"trust\"]:.3f}  '\n"
"          f'Stress={cmp_results[name][\"stress\"]:.3f}  '\n"
"          f'Time={dt:.2f}s')"
))

cells.append(code_cell(
"# Visualize Swiss roll embeddings\n"
"fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n"
"\n"
"for ax, (name, res) in zip(axes, cmp_results.items()):\n"
"    sc = ax.scatter(\n"
"        res['coords'][:, 0], res['coords'][:, 1],\n"
"        c=res['color'], cmap='viridis', s=20, alpha=0.7,\n"
"    )\n"
"    ax.set_title(\n"
"        f'{name}\\n'\n"
"        f'k-NN={res[\"knn\"]:.3f}  Trust={res[\"trust\"]:.3f}'\n"
"    )\n"
"    ax.set_xlabel('Dim 1')\n"
"    ax.set_ylabel('Dim 2')\n"
"\n"
"plt.suptitle('Swiss roll embeddings', fontsize=14)\n"
"plt.tight_layout()\n"
"plt.show()"
))

cells.append(code_cell(
"print('Recommendations:')\n"
"print('  - Exploratory visualization: UMAP')\n"
"print('  - Distance preservation: Isomap')\n"
"print('  - Linear relationships: PCA (fast, interpretable)')\n"
"print('  - Large datasets: PCA or UMAP')"
))

# ===== SECTION 3: AUTOENCODER-BASED DR ====================================

cells.append(md_cell(
"## 3. Autoencoder-based DR\n"
"\n"
"Linear and graph-based methods assume fixed neighborhoods or geodesics.\n"
"Autoencoders learn a flexible nonlinear mapping — at the cost of more\n"
"hyperparameters and training time.\n"
"\n"
"Neural network DR alternatives via [`flexible_ae`](https://driada.readthedocs.io/en/latest/api/dim_reduction/neural_methods.html): **standard autoencoder** (AE) with\n"
"`continue_learning`, and **Beta-VAE** with KL divergence, compared against PCA.\n"
"Key parameters: `architecture` (`'ae'` or `'vae'`) selects the model type,\n"
"and `continue_learning` resumes training without resetting weights.\n"
"Requires PyTorch."
))

cells.append(code_cell(
"try:\n"
"    import torch  # noqa: F401\n"
"    HAS_TORCH = True\n"
"    print('PyTorch available -- autoencoder examples will run.')\n"
"except ImportError:\n"
"    HAS_TORCH = False\n"
"    print(\n"
"        'PyTorch not found. Install with: pip install torch\\n'\n"
"        'Autoencoder cells will be skipped.'\n"
"    )"
))

cells.append(code_cell(
"if HAS_TORCH:\n"
"    print('[1] Generating synthetic head direction cell data')\n"
"    print('-' * 40)\n"
"    calcium_ae, head_direction_ae, preferred_dirs_ae, rates_ae = (\n"
"        generate_circular_manifold_data(\n"
"            n_neurons=100,\n"
"            kappa=4.0,\n"
"            duration=600,\n"
"            seed=42,\n"
"            verbose=True,\n"
"        )\n"
"    )\n"
"    print(f'  Calcium shape: {calcium_ae.shape}')\n"
"    print(f'  Head direction shape: {head_direction_ae.shape}')\n"
"\n"
"    mvdata_ae = MVData(calcium_ae, downsampling=5, verbose=False)\n"
"    color_ae = head_direction_ae[::5]  # angle for coloring, match downsampling"
))

cells.append(code_cell(
"if HAS_TORCH:\n"
"    print('\\n[2] Standard autoencoder')\n"
"    print('-' * 40)\n"
"\n"
"    # Train for 5 epochs (not fully converged)\n"
"    emb_ae = mvdata_ae.get_embedding(\n"
"        method='flexible_ae',\n"
"        dim=2,\n"
"        architecture='ae',\n"
"        inter_dim=64,  # bottleneck width\n"
"        epochs=5,  # under-trained for demo\n"
"        lr=1e-3,\n"
"        feature_dropout=0.1,\n"
"        loss_components=[{'name': 'reconstruction', 'weight': 1.0, 'loss_type': 'mse'}],\n"
"        verbose=False,\n"
"    )\n"
"    print(f'  After 5 epochs   - loss: {emb_ae.nn_loss:.4f}')\n"
"\n"
"    # Continue training for 25 more epochs\n"
"    emb_ae.continue_learning(25, lr=1e-3, verbose=False)\n"
"    print(f'  After 25 more    - loss: {emb_ae.nn_loss:.4f}')\n"
"\n"
"    # Fine-tune with lower learning rate\n"
"    emb_ae.continue_learning(20, lr=1e-4, verbose=False)\n"
"    print(f'  After 20 fine-tune - loss: {emb_ae.nn_loss:.4f}')\n"
"\n"
"    print('\\n[3] Beta-VAE (beta=4.0)')\n"
"    print('-' * 40)\n"
"    emb_vae = mvdata_ae.get_embedding(\n"
"        method='flexible_ae',\n"
"        dim=2,\n"
"        architecture='vae',\n"
"        inter_dim=64,\n"
"        epochs=150,\n"
"        lr=1e-3,\n"
"        feature_dropout=0.1,\n"
"        loss_components=[\n"
"            {'name': 'reconstruction', 'weight': 1.0, 'loss_type': 'mse'},\n"
"            {'name': 'beta_vae', 'weight': 1.0, 'beta': 4.0},  # beta > 1: encourages disentanglement\n"
"        ],\n"
"        verbose=False,\n"
"    )\n"
"    print(f'  Embedding shape: {emb_vae.coords.shape}')\n"
"    print(f'  Test loss (reconstruction + KL): {emb_vae.nn_loss:.4f}')\n"
"\n"
"    print('\\n[4] PCA (for comparison)')\n"
"    print('-' * 40)\n"
"    emb_pca_ae = mvdata_ae.get_embedding(method='pca', dim=2)\n"
"    print(f'  Embedding shape: {emb_pca_ae.coords.shape}')"
))

cells.append(code_cell(
"if HAS_TORCH:\n"
"    print('[5] Creating comparison plot')\n"
"    print('-' * 40)\n"
"\n"
"    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n"
"    embeddings_ae = [\n"
"        (emb_pca_ae, 'PCA'),\n"
"        (emb_ae, 'AE (5 + 25 + 20 epochs)'),\n"
"        (emb_vae, 'Beta-VAE'),\n"
"    ]\n"
"\n"
"    for ax, (emb, title) in zip(axes, embeddings_ae):\n"
"        coords = emb.coords  # (dim, n_samples)\n"
"        sc = ax.scatter(\n"
"            coords[0], coords[1], c=color_ae, cmap='hsv',\n"
"            s=2, alpha=0.5, vmin=0, vmax=2 * np.pi\n"
"        )\n"
"        ax.set_title(title)\n"
"        ax.set_xlabel('Dim 1')\n"
"        ax.set_ylabel('Dim 2')\n"
"\n"
"    fig.colorbar(sc, ax=axes[-1], label='Head direction (rad)')\n"
"    plt.suptitle('Circular manifold recovery (colored by head direction)')\n"
"    plt.tight_layout()\n"
"    plt.show()"
))

# ===== SECTION 4: CIRCULAR MANIFOLD & DIMENSIONALITY ESTIMATION ===========

cells.append(md_cell(
"## 4. Circular manifold & dimensionality estimation\n"
"\n"
"Synthetic benchmarks show what methods *can* do; real neural data shows what\n"
"they *must* handle. Head direction cells encode a circular variable — a\n"
"topology that some DR methods preserve better than others.\n"
"\n"
"Head direction cells encode a ring. We generate 100 HD cells with\n"
"[`generate_circular_manifold_exp`](https://driada.readthedocs.io/en/latest/api/experiment/synthetic.html#driada.experiment.generate_circular_manifold_exp)\n"
"and estimate intrinsic dimensionality using the\n"
"[`driada.dimensionality`](https://driada.readthedocs.io/en/latest/api/dimensionality/index.html) module.\n"
"\n"
"This module provides several estimators:\n"
"- **Linear:** [`pca_dimension`](https://driada.readthedocs.io/en/latest/api/dimensionality/linear.html#driada.dimensionality.linear.pca_dimension) (variance threshold), [`effective_rank`](https://driada.readthedocs.io/en/latest/api/dimensionality/linear.html#driada.dimensionality.linear.effective_rank), [`pca_dimension_profile`](https://driada.readthedocs.io/en/latest/api/dimensionality/linear.html#driada.dimensionality.linear.pca_dimension_profile)\n"
"- **Intrinsic:** [`correlation_dimension`](https://driada.readthedocs.io/en/latest/api/dimensionality/intrinsic.html#driada.dimensionality.intrinsic.correlation_dimension), [`geodesic_dimension`](https://driada.readthedocs.io/en/latest/api/dimensionality/intrinsic.html#driada.dimensionality.intrinsic.geodesic_dimension), [`nn_dimension`](https://driada.readthedocs.io/en/latest/api/dimensionality/intrinsic.html#driada.dimensionality.intrinsic.nn_dimension) (TWO-NN)\n"
"- **Effective:** [`eff_dim`](https://driada.readthedocs.io/en/latest/api/dimensionality/effective.html#driada.dimensionality.effective.eff_dim) (Renyi entropy of eigenvalues)\n"
"\n"
"Below we use a subset of these, compare real vs shuffled data,\n"
"and extract the circular manifold via DR."
))

cells.append(code_cell(
"def estimate_dimensionality(neural_data, methods=None, ds=1):\n"
"    \"\"\"Estimate intrinsic dimensionality using multiple DRIADA methods.\"\"\"\n"
"    if methods is None:\n"
"        methods = [\n"
"            'pca_90', 'pca_95', 'participation_ratio',\n"
"            'correlation_dim', 'geodesic_dim',\n"
"        ]\n"
"\n"
"    dim_estimates = {}\n"
"\n"
"    # Downsample data if requested\n"
"    if ds > 1:\n"
"        neural_data_ds = neural_data[:, ::ds]\n"
"        print(f'  Downsampled: {neural_data.shape} -> {neural_data_ds.shape}')\n"
"    else:\n"
"        neural_data_ds = neural_data\n"
"\n"
"    # Transpose data for methods that expect (n_samples, n_features)\n"
"    data_transposed = neural_data_ds.T\n"
"\n"
"    # Linear methods\n"
"    if 'pca_90' in methods:\n"
"        dim_estimates['pca_90'] = pca_dimension(data_transposed, threshold=0.90)\n"
"    if 'pca_95' in methods:\n"
"        dim_estimates['pca_95'] = pca_dimension(data_transposed, threshold=0.95)\n"
"\n"
"    # Nonlinear intrinsic methods\n"
"    if 'correlation_dim' in methods:\n"
"        try:\n"
"            print('  Computing correlation dimension...')\n"
"            dim_estimates['correlation_dim'] = correlation_dimension(data_transposed)\n"
"        except Exception as e:\n"
"            print(f'  Warning: correlation_dimension failed: {e}')\n"
"            dim_estimates['correlation_dim'] = np.nan\n"
"\n"
"    if 'geodesic_dim' in methods:\n"
"        try:\n"
"            print('  Computing geodesic dimension (this may take time)...')\n"
"            dim_estimates['geodesic_dim'] = geodesic_dimension(\n"
"                data_transposed, k=20, mode='fast', factor=4\n"
"            )\n"
"        except Exception as e:\n"
"            print(f'  Warning: geodesic_dimension failed: {e}')\n"
"            dim_estimates['geodesic_dim'] = np.nan\n"
"\n"
"    # Effective dimensionality (participation ratio)\n"
"    if 'participation_ratio' in methods:\n"
"        dim_estimates['participation_ratio'] = eff_dim(\n"
"            neural_data_ds.T, enable_correction=False, q=2\n"
"        )\n"
"\n"
"    return dim_estimates"
))

cells.append(code_cell(
"print('1. Generating head direction cell population...')\n"
"\n"
"# Generate synthetic head direction cells\n"
"exp_circ, info_circ = generate_circular_manifold_exp(\n"
"    n_neurons=100,\n"
"    duration=600,  # 10 minutes\n"
"    kappa=4.0,     # Tuning width\n"
"    seed=42,\n"
"    verbose=True,\n"
"    return_info=True,\n"
")\n"
"\n"
"# Extract neural activity and true head directions\n"
"neural_data_circ = exp_circ.calcium.scdata  # Shape: (n_neurons, n_timepoints)\n"
"true_angles = info_circ['head_direction']   # Ground truth angles\n"
"\n"
"print(f'\\nGenerated {neural_data_circ.shape[0]} neurons, '\n"
"      f'{neural_data_circ.shape[1]} timepoints')\n"
"print(f'Neural activity shape: {neural_data_circ.shape}')"
))

cells.append(code_cell(
"# Estimate intrinsic dimensionality\n"
"print('\\n2. Estimating intrinsic dimensionality of neural population...')\n"
"print('-' * 50)\n"
"\n"
"dim_methods = [\n"
"    'pca_90', 'pca_95', 'participation_ratio',\n"
"    'correlation_dim', 'geodesic_dim',\n"
"]\n"
"\n"
"# Use ds=5 downsampling for faster computation\n"
"dim_estimates = estimate_dimensionality(\n"
"    neural_data_circ, methods=dim_methods, ds=5\n"
")\n"
"\n"
"print('Dimensionality estimates:')\n"
"for method, estimate in dim_estimates.items():\n"
"    print(f'  {method:20s}: {estimate:.2f}')\n"
"\n"
"print('\\nNote: Head direction cells should have intrinsic dimensionality ~ 1')\n"
"print('      (circular manifold), but finite sampling may increase estimates')\n"
"\n"
"# Compare with temporally shuffled data to demonstrate manifold structure\n"
"print('\\n2b. Comparing with temporally shuffled data (destroys manifold)...')\n"
"print('-' * 50)\n"
"\n"
"# Get shuffled calcium data from experiment\n"
"shuffled_calcium = exp_circ.get_multicell_shuffled_calcium()\n"
"\n"
"# Estimate dimensionality on shuffled data\n"
"dim_estimates_shuffled = estimate_dimensionality(\n"
"    shuffled_calcium, methods=dim_methods, ds=5\n"
")\n"
"\n"
"print('\\nDimensionality estimates (SHUFFLED data):')\n"
"for method, estimate in dim_estimates_shuffled.items():\n"
"    print(f'  {method:20s}: {estimate:.2f}')\n"
"\n"
"print('\\nComparison (Real vs Shuffled):')\n"
"print(f'{\"Method\":<20s} {\"Real\":>8s} {\"Shuffled\":>8s} {\"Increase\":>10s}')\n"
"print('-' * 50)\n"
"for method in dim_methods:\n"
"    real = dim_estimates[method]\n"
"    shuffled = dim_estimates_shuffled[method]\n"
"    increase = ((shuffled - real) / real) * 100\n"
"    print(f'{method:<20s} {real:8.2f} {shuffled:8.2f} {increase:+9.1f}%')\n"
"\n"
"print('\\nInterpretation: Temporal shuffling destroys the circular manifold structure,')\n"
"print('                dramatically increasing dimensionality.')"
))

cells.append(code_cell(
"# Plot eigenvalue spectrum\n"
"print('\\n3. Plotting eigenvalue spectrum...')\n"
"\n"
"# Compute correlation matrix\n"
"data_centered = neural_data_circ - np.mean(neural_data_circ, axis=1, keepdims=True)\n"
"corr_mat = np.corrcoef(data_centered)\n"
"\n"
"# Get eigenvalues\n"
"eigenvalues = np.linalg.eigvalsh(corr_mat)[::-1]  # Descending order\n"
"eigenvalues = eigenvalues[eigenvalues > 0]\n"
"\n"
"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n"
"\n"
"# Eigenvalue spectrum\n"
"ax1.plot(eigenvalues, 'o-', markersize=4)\n"
"ax1.set_xlabel('Component')\n"
"ax1.set_ylabel('Eigenvalue')\n"
"ax1.set_title('Eigenvalue spectrum')\n"
"ax1.set_yscale('log')\n"
"ax1.grid(True, alpha=0.3)\n"
"\n"
"# Cumulative variance explained\n"
"cumvar = np.cumsum(eigenvalues) / np.sum(eigenvalues)\n"
"ax2.plot(cumvar, 'o-', markersize=4)\n"
"ax2.axhline(0.9, color='r', linestyle='--', label='90% variance')\n"
"ax2.axhline(0.95, color='orange', linestyle='--', label='95% variance')\n"
"ax2.set_xlabel('Number of Components')\n"
"ax2.set_ylabel('Cumulative Variance Explained')\n"
"ax2.set_title('Cumulative variance explained')\n"
"ax2.legend()\n"
"ax2.grid(True, alpha=0.3)\n"
"\n"
"plt.tight_layout()\n"
"plt.show()"
))

cells.append(code_cell(
"# Apply dimensionality reduction using MVData\n"
"print('\\n4. Applying dimensionality reduction methods using MVData...')\n"
"print('-' * 50)\n"
"\n"
"# Create MVData object from calcium data with downsampling\n"
"downsampling_circ = 10  # use every 10th frame for speed\n"
"mvdata_circ = MVData(neural_data_circ, downsampling=downsampling_circ)\n"
"\n"
"# Downsample true angles to match\n"
"true_angles_ds = true_angles[::downsampling_circ]\n"
"\n"
"# Dictionary to store embeddings\n"
"embeddings_dict_circ = {}\n"
"\n"
"# PCA\n"
"print('- PCA...')\n"
"pca_embedding_circ = mvdata_circ.get_embedding(method='pca', dim=2)\n"
"embeddings_dict_circ['PCA'] = pca_embedding_circ.coords.T\n"
"print(\n"
"    f'  First 2 PCs explain '\n"
"    f'{100 * sum(pca_embedding_circ.reducer_.explained_variance_ratio_):.1f}% of variance'\n"
")\n"
"\n"
"# Isomap\n"
"print('- Isomap...')\n"
"isomap_embedding_circ = mvdata_circ.get_embedding(\n"
"    method='isomap', dim=2, n_neighbors=50\n"
")\n"
"embeddings_dict_circ['Isomap'] = isomap_embedding_circ.coords.T\n"
"\n"
"# UMAP with increased parameters for better global structure\n"
"print('- UMAP...')\n"
"umap_embedding_circ = mvdata_circ.get_embedding(\n"
"    method='umap', n_components=2, n_neighbors=100, min_dist=0.5\n"
")\n"
"embeddings_dict_circ['UMAP'] = umap_embedding_circ.coords.T"
))

cells.append(code_cell(
"# Visualize extracted manifolds\n"
"print('\\n5. Visualizing extracted manifolds...')\n"
"\n"
"# Create embedding comparison visualization\n"
"embeddings_list_circ = [\n"
"    embeddings_dict_circ[m] for m in ['PCA', 'Isomap', 'UMAP']\n"
"]\n"
"fig_embedding = visualize_circular_manifold(\n"
"    embeddings_list_circ, true_angles_ds, ['PCA', 'Isomap', 'UMAP']\n"
")\n"
"plt.show()\n"
"\n"
"# Trajectory visualization\n"
"print('\\n6. Analyzing temporal continuity of extracted manifolds...')\n"
"\n"
"# Use only first 1000 timepoints for trajectory visualization\n"
"traj_len = min(1000, embeddings_dict_circ['PCA'].shape[0])\n"
"trajectories_dict = {\n"
"    method: emb[:traj_len] for method, emb in embeddings_dict_circ.items()\n"
"}\n"
"\n"
"fig3 = plot_trajectories(\n"
"    embeddings=trajectories_dict,\n"
"    trajectory_kwargs={'arrow_spacing': 50, 'linewidth': 0.5, 'alpha': 0.5},\n"
"    figsize=(15, 5),\n"
"    dpi=DEFAULT_DPI,\n"
")\n"
"plt.show()"
))

cells.append(code_cell(
"# Summary statistics\n"
"print('\\n7. Summary of manifold extraction quality:')\n"
"print('-' * 60)\n"
"print(f'{\"Method\":10s} | {\"Correlation\":12s} | {\"Mean Error\":12s} | {\"Quality\":8s}')\n"
"print('-' * 60)\n"
"\n"
"for method, embedding in embeddings_dict_circ.items():\n"
"    # Use manifold metrics API\n"
"    alignment_metrics = compute_embedding_alignment_metrics(\n"
"        embedding, true_angles_ds, 'circular'\n"
"    )\n"
"    r = alignment_metrics['correlation']\n"
"    error = alignment_metrics['error']\n"
"\n"
"    # Quality assessment\n"
"    if abs(r) > 0.95:\n"
"        quality_str = 'Excellent'\n"
"    elif abs(r) > 0.85:\n"
"        quality_str = 'Good'\n"
"    elif abs(r) > 0.70:\n"
"        quality_str = 'Fair'\n"
"    else:\n"
"        quality_str = 'Poor'\n"
"\n"
"    print(f'{method:10s} | {r:12.3f} | {error:9.3f} rad | {quality_str:8s}')"
))

cells.append(md_cell(
"### Conclusions\n"
"\n"
"- Head direction cells encode a 1D ring, but finite sampling, noise, and\n"
"  calcium dynamics inflate dimensionality estimates above the true value.\n"
"- Temporal shuffling destroys manifold structure (dimensionality increases),\n"
"  confirming the manifold is real.\n"
"- Nonlinear methods (Isomap, UMAP) better preserve circular topology.\n"
"- PCA captures variance but may distort circular structure.\n"
"- Higher `n_neighbors` helps preserve global structure."
))

# ===== SECTION 5: INTENSE-GUIDED DR =======================================

cells.append(md_cell(
"## 5. INTENSE-guided DR\n"
"\n"
"Not all neurons carry manifold information. If we first identify\n"
"feature-selective cells with INTENSE, we can build cleaner embeddings from\n"
"the relevant subpopulation.\n"
"\n"
"In real experiments, only a subset of neurons encode any given variable.\n"
"Including non-selective neurons in DR adds noise and distorts the manifold.\n"
"\n"
"Here we use\n"
"[`compute_cell_feat_significance`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html#driada.intense.pipelines.compute_cell_feat_significance)\n"
"(INTENSE) to identify spatially selective neurons from a\n"
"[`generate_mixed_population_exp`](https://driada.readthedocs.io/en/latest/api/experiment/synthetic.html#driada.experiment.generate_mixed_population_exp)\n"
"dataset, then compare DR quality when using **all neurons** vs\n"
"**INTENSE-selected spatial neurons** vs a **random subset** vs\n"
"**non-selective neurons**."
))

cells.append(code_cell(
"from scipy.spatial.distance import pdist\n"
"\n"
"\n"
"def compute_spatial_correspondence_metrics(embedding, true_positions):\n"
"    \"\"\"\n"
"    Compute spatial-specific metrics for evaluating embedding quality.\n"
"\n"
"    Parameters\n"
"    ----------\n"
"    embedding : ndarray, shape (n_samples, n_dims)\n"
"        Low-dimensional embedding\n"
"    true_positions : ndarray, shape (n_samples, n_spatial_dims)\n"
"        True spatial positions\n"
"\n"
"    Returns\n"
"    -------\n"
"    metrics : dict\n"
"        Dictionary containing spatial correspondence metrics\n"
"    \"\"\"\n"
"    metrics = {}\n"
"\n"
"    # 1. SPATIAL DECODING ACCURACY\n"
"    decoding_metrics = compute_spatial_decoding_accuracy(\n"
"        embedding.T,\n"
"        true_positions,\n"
"        test_size=0.5,\n"
"        n_estimators=20,\n"
"        max_depth=3,\n"
"        min_samples_leaf=50,\n"
"        random_state=42,\n"
"    )\n"
"    metrics.update(decoding_metrics)\n"
"\n"
"    # 2. SPATIAL INFORMATION CONTENT\n"
"    mi_metrics = compute_spatial_information(\n"
"        embedding.T, true_positions\n"
"    )\n"
"    metrics.update(mi_metrics)\n"
"\n"
"    # 3. DISTANCE CORRELATION\n"
"    try:\n"
"        dist_embed = pdist(embedding)\n"
"        dist_true = pdist(true_positions)\n"
"        metrics['distance_correlation'] = np.corrcoef(dist_embed, dist_true)[0, 1]\n"
"    except Exception:\n"
"        metrics['distance_correlation'] = 0.0\n"
"\n"
"    # 4. PROCRUSTES ANALYSIS\n"
"    try:\n"
"        embedding_2d = embedding[:, :2] if embedding.shape[1] >= 2 else embedding\n"
"        _, disparity, _ = procrustes_analysis(\n"
"            true_positions, embedding_2d, scaling=True, reflection=True\n"
"        )\n"
"        metrics['procrustes_disparity'] = disparity\n"
"    except Exception:\n"
"        metrics['procrustes_disparity'] = 1.0\n"
"\n"
"    return metrics"
))

cells.append(code_cell(
"print('1. Generating mixed population with spatial and non-spatial neurons...')\n"
"\n"
"n_neurons_intense = 50   # Minimal for notebook execution speed\n"
"duration_intense = 600   # 10 minutes -- longer improves detection power\n"
"n_shuffles_1 = 100       # FFT makes shuffle count cheap\n"
"n_shuffles_2 = 5000      # Better statistics with minimal overhead\n"
"ds_intense = 5\n"
"\n"
"exp_intense = generate_mixed_population_exp(\n"
"    n_neurons=n_neurons_intense,\n"
"    manifold_type='2d_spatial',\n"
"    manifold_fraction=0.5,  # 1/2 place cells, 1/2 feature cells\n"
"    n_discrete_features=3,\n"
"    n_continuous_features=3,\n"
"    duration=duration_intense,\n"
"    seed=42,\n"
"    verbose=True,\n"
")\n"
"print(f'  Created experiment with {exp_intense.n_cells} neurons, '\n"
"      f'{exp_intense.n_frames} timepoints')\n"
"print(f'  Available features: {list(exp_intense.dynamic_features.keys())}')"
))

cells.append(md_cell(
"### How INTENSE works\n"
"\n"
"[`compute_cell_feat_significance`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html#driada.intense.pipelines.compute_cell_feat_significance)\n"
"tests whether each neuron's activity is significantly related to a\n"
"behavioral variable (here, 2D position) using mutual information (MI)\n"
"with circular time-shift shuffling to build a null distribution.\n"
"\n"
"The two-stage test:\n"
"1. **Stage 1** (fast screen): compute MI for each neuron-feature pair,\n"
"   compare against 100 shuffles, keep pairs where MI exceeds the null.\n"
"2. **Stage 2** (precise): for surviving pairs, compute 5000 shuffles and\n"
"   fit a gamma distribution to obtain a p-value.\n"
"\n"
"Neurons passing the p-value threshold are classified as **selective**.\n"
"For details, see\n"
"[Notebook 02](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/02_selectivity_detection_intense.ipynb)."
))

cells.append(md_cell(
"We generated a mixed population above: half place cells encoding spatial\n"
"position, half selective for discrete/continuous features. INTENSE separates\n"
"them so we can compare DR on each subgroup."
))

cells.append(code_cell(
"# 2. Run INTENSE with position_2d MultiTimeSeries\n"
"print('\\n2. Running INTENSE analysis on 2D position (MultiTimeSeries)...')\n"
"stats_i, significance_i, info_i, results_i = compute_cell_feat_significance(\n"
"    exp_intense,\n"
"    feat_bunch=['position_2d'],  # Using MultiTimeSeries only\n"
"    find_optimal_delays=False,\n"
"    mode='two_stage',\n"
"    n_shuffles_stage1=n_shuffles_1,\n"
"    n_shuffles_stage2=n_shuffles_2,\n"
"    ds=ds_intense,\n"
"    pval_thr=0.01,\n"
"    multicomp_correction=None,\n"
"    verbose=True,\n"
")"
))

cells.append(md_cell(
"### Interpreting detection results\n"
"\n"
"Not all true place cells will be detected. The default MI estimator\n"
"(GCMI) measures rank correlation, which is conservative for spatially\n"
"localized place fields. Longer recordings and higher shuffle counts\n"
"improve detection power. See\n"
"the [information module docs](https://driada.readthedocs.io/en/latest/api/information/index.html)\n"
"for estimator trade-offs."
))

cells.append(code_cell(
"# 3. Categorize neurons by selectivity\n"
"print('\\n3. Categorizing neurons by selectivity...')\n"
"\n"
"# Get neurons selective to spatial position\n"
"sig_neurons_2d = list(exp_intense.get_significant_neurons(fbunch='position_2d').keys())\n"
"spatial_neurons_i = sig_neurons_2d\n"
"\n"
"print(f'  Spatial neurons (position_2d): {len(sig_neurons_2d)}')\n"
"print(f'  Non-spatial neurons: {exp_intense.n_cells - len(spatial_neurons_i)}')\n"
"\n"
"# Check if we have enough spatial neurons\n"
"if len(spatial_neurons_i) < 5:\n"
"    print('\\nWARNING: Not enough spatial neurons detected!')\n"
"    print('Try running with more neurons or adjusting parameters.')\n"
"\n"
"# Extract true positions\n"
"position_2d_i = exp_intense.dynamic_features['position_2d'].data\n"
"x_pos_i = position_2d_i[0, :]\n"
"y_pos_i = position_2d_i[1, :]\n"
"true_positions_i = np.column_stack([x_pos_i, y_pos_i])\n"
"\n"
"# Downsample positions to match calcium data\n"
"if ds_intense > 1:\n"
"    true_positions_i = true_positions_i[::ds_intense]\n"
"\n"
"# VERIFICATION: Check ground truth vs detected spatial neurons\n"
"print('\\n[CHECK] Analyzing ground truth vs detected spatial neurons...')\n"
"\n"
"# Get ground truth spatial neurons (first 50% are spatial by construction)\n"
"n_true_spatial = int(exp_intense.n_cells * 0.5)\n"
"true_spatial_neurons = list(range(n_true_spatial))\n"
"true_nonspatial_neurons = list(range(n_true_spatial, exp_intense.n_cells))\n"
"\n"
"print(f'  Ground truth spatial neurons: {n_true_spatial} '\n"
"      f'(indices 0-{n_true_spatial - 1})')\n"
"print(f'  Ground truth non-spatial neurons: '\n"
"      f'{exp_intense.n_cells - n_true_spatial} '\n"
"      f'(indices {n_true_spatial}-{exp_intense.n_cells - 1})')\n"
"\n"
"# Check detection accuracy\n"
"detected_spatial_set = set(spatial_neurons_i)\n"
"true_spatial_set = set(true_spatial_neurons)\n"
"true_nonspatial_set = set(true_nonspatial_neurons)\n"
"\n"
"true_positives = detected_spatial_set & true_spatial_set\n"
"false_positives = detected_spatial_set & true_nonspatial_set\n"
"false_negatives = true_spatial_set - detected_spatial_set\n"
"\n"
"print(f'  True positives (correctly detected spatial): {len(true_positives)}')\n"
"print(f'  False positives (non-spatial detected as spatial): {len(false_positives)}')\n"
"print(f'  False negatives (spatial missed): {len(false_negatives)}')\n"
"\n"
"precision_i = (\n"
"    len(true_positives) / len(detected_spatial_set) if detected_spatial_set else 0\n"
")\n"
"recall_i = len(true_positives) / len(true_spatial_set) if true_spatial_set else 0\n"
"f1_i = (\n"
"    2 * precision_i * recall_i / (precision_i + recall_i)\n"
"    if (precision_i + recall_i) > 0 else 0\n"
")\n"
"\n"
"print(f'  Detection Precision: {precision_i:.3f}')\n"
"print(f'  Detection Recall: {recall_i:.3f}')\n"
"print(f'  Detection F1-score: {f1_i:.3f}')"
))

cells.append(md_cell(
"### Comparing neuron subsets for DR\n"
"\n"
"We test four scenarios to show the benefit of INTENSE-guided neuron\n"
"selection for dimensionality reduction:\n"
"\n"
"- **All neurons** -- full population (spatial + non-spatial + noise)\n"
"- **Spatial neurons** -- only INTENSE-detected spatial cells\n"
"- **Random half** -- random subset (same size as spatial, for comparison)\n"
"- **Non-selective** -- neurons not selective to any feature\n"
"\n"
"If INTENSE correctly identifies spatial neurons, their embeddings should\n"
"better preserve the 2D spatial structure."
))

cells.append(code_cell(
"# 4. Create scenarios to demonstrate benefit\n"
"print('\\n4. Creating test scenarios...')\n"
"\n"
"# Get all neurons\n"
"calcium_all_i = exp_intense.calcium.scdata[:, ::ds_intense]\n"
"\n"
"# Get spatial neurons (detected by INTENSE)\n"
"calcium_spatial_i = exp_intense.calcium.scdata[spatial_neurons_i, ::ds_intense]\n"
"\n"
"# Get non-selective neurons\n"
"all_neurons_i = set(range(exp_intense.n_cells))\n"
"selective_neurons_i = set(spatial_neurons_i)\n"
"for feat in ['d_feat_0', 'd_feat_1', 'd_feat_2',\n"
"             'c_feat_0', 'c_feat_1', 'c_feat_2']:\n"
"    try:\n"
"        feat_neurons = exp_intense.get_significant_neurons(fbunch=feat)\n"
"        if feat_neurons:\n"
"            selective_neurons_i.update(feat_neurons.keys())\n"
"    except Exception:\n"
"        pass\n"
"non_selective_neurons_i = list(all_neurons_i - selective_neurons_i)\n"
"calcium_non_selective_i = (\n"
"    exp_intense.calcium.scdata[non_selective_neurons_i, ::ds_intense]\n"
"    if non_selective_neurons_i else None\n"
")\n"
"\n"
"# Get random half of all neurons\n"
"np.random.seed(42)\n"
"random_half_idx_i = np.random.choice(\n"
"    exp_intense.n_cells, size=exp_intense.n_cells // 2, replace=False\n"
")\n"
"calcium_random_half_i = exp_intense.calcium.scdata[random_half_idx_i, ::ds_intense]\n"
"\n"
"print(f'  All neurons: {calcium_all_i.shape[0]} neurons')\n"
"print(f'  Spatial neurons (INTENSE): {calcium_spatial_i.shape[0]} neurons')\n"
"print(f'  Random half: {calcium_random_half_i.shape[0]} neurons')\n"
"print(f'  Non-selective neurons: {len(non_selective_neurons_i)} neurons')"
))

cells.append(code_cell(
"# 5. Define DR methods and scenarios\n"
"print('\\n5. Applying dimensionality reduction methods...')\n"
"\n"
"dr_methods_i = {\n"
"    'PCA': {'method': 'pca', 'params': {'dim': 2}},\n"
"    'Isomap': {'method': 'isomap', 'params': {'dim': 2, 'n_neighbors': 30}},\n"
"    'UMAP': {\n"
"        'method': 'umap',\n"
"        'params': {\n"
"            'dim': 2, 'n_neighbors': 80,\n"
"            'min_dist': 0.8, 'random_state': 42,\n"
"        },\n"
"    },\n"
"}\n"
"\n"
"results_intense = {}\n"
"\n"
"scenarios_i = [\n"
"    ('All neurons', calcium_all_i),\n"
"    ('Spatial neurons', calcium_spatial_i),\n"
"    ('Random half', calcium_random_half_i),\n"
"    ('Non-selective', calcium_non_selective_i),\n"
"]"
))

cells.append(code_cell(
"# Run DR methods on each scenario and compute spatial metrics\n"
"for method_name, method_config in dr_methods_i.items():\n"
"    print(f'\\n  {method_name}:')\n"
"    results_intense[method_name] = {}\n"
"\n"
"    for scenario_name, calcium_data in scenarios_i:\n"
"        if calcium_data is None:\n"
"            print(f'    - {scenario_name}: No neurons in this category, skipping')\n"
"            continue\n"
"        if calcium_data.shape[0] < 3:\n"
"            print(f'    - {scenario_name}: Too few neurons ({calcium_data.shape[0]}), skipping')\n"
"            continue\n"
"\n"
"        print(f'    - {scenario_name}...')\n"
"\n"
"        try:\n"
"            mvdata_i = MVData(calcium_data)\n"
"\n"
"            # Adjust n_neighbors for smaller datasets\n"
"            params = method_config['params'].copy()\n"
"            if 'n_neighbors' in params:\n"
"                params['n_neighbors'] = min(\n"
"                    params['n_neighbors'], calcium_data.shape[1] // 10\n"
"                )\n"
"\n"
"            embedding_obj = mvdata_i.get_embedding(\n"
"                method=method_config['method'], **params\n"
"            )\n"
"            embedding_i = embedding_obj.coords.T\n"
"\n"
"            metrics_i = compute_spatial_correspondence_metrics(\n"
"                embedding_i, true_positions_i\n"
"            )\n"
"\n"
"            results_intense[method_name][scenario_name] = {\n"
"                'embedding': embedding_i,\n"
"                'metrics': metrics_i,\n"
"            }\n"
"\n"
"            print(\n"
"                f\"      Spatial decoding R^2: {metrics_i['r2_avg']:.3f}, \"\n"
"                f\"Distance corr: {metrics_i['distance_correlation']:.3f}, \"\n"
"                f\"MI: {metrics_i['mi_total']:.3f}\"\n"
"            )\n"
"\n"
"        except Exception as e:\n"
"            print(f'      Failed: {e}')\n"
"            results_intense[method_name][scenario_name] = None\n"
"\n"
"    # Calculate improvements\n"
"    if (\n"
"        'All neurons' in results_intense[method_name]\n"
"        and 'Spatial neurons' in results_intense[method_name]\n"
"    ):\n"
"        if (\n"
"            results_intense[method_name]['All neurons']\n"
"            and results_intense[method_name]['Spatial neurons']\n"
"        ):\n"
"            r2_all_i = results_intense[method_name]['All neurons']['metrics']['r2_avg']\n"
"            r2_spatial_i = results_intense[method_name]['Spatial neurons']['metrics']['r2_avg']\n"
"            improvement_i = (r2_spatial_i / max(r2_all_i, 0.001) - 1) * 100\n"
"            print(f'    Spatial vs All improvement: {improvement_i:+.1f}%')"
))

cells.append(code_cell(
"# 6. Visualize results\n"
"print('\\n6. Creating visualizations...')\n"
"\n"
"# Prepare embeddings for grid plot\n"
"grid_embeddings = {}\n"
"grid_metrics = {}\n"
"for method_name in dr_methods_i.keys():\n"
"    grid_embeddings[method_name] = {}\n"
"    grid_metrics[method_name] = {}\n"
"    for scenario in ['All neurons', 'Spatial neurons', 'Random half', 'Non-selective']:\n"
"        if scenario in results_intense[method_name] and results_intense[method_name][scenario]:\n"
"            grid_embeddings[method_name][scenario] = (\n"
"                results_intense[method_name][scenario]['embedding']\n"
"            )\n"
"            grid_metrics[method_name][scenario] = {\n"
"                'R^2': results_intense[method_name][scenario]['metrics']['r2_avg']\n"
"            }\n"
"\n"
"labels_i = np.arange(len(true_positions_i))\n"
"\n"
"fig1 = plot_embeddings_grid(\n"
"    embeddings=grid_embeddings,\n"
"    labels=labels_i,\n"
"    metrics=grid_metrics,\n"
"    colormap='viridis',\n"
"    figsize=(18, 12),\n"
"    n_cols=4,\n"
"    dpi=DEFAULT_DPI,\n"
")\n"
"\n"
"fig1.suptitle(\n"
"    'INTENSE-Guided DR: Benefit of Spatial Neuron Selection', fontsize=14\n"
")\n"
"plt.show()\n"
"\n"
"# Create neuron selectivity summary\n"
"selectivity_counts = {\n"
"    'Spatial\\n(any)': len(spatial_neurons_i),\n"
"    'Non-spatial': exp_intense.n_cells - len(spatial_neurons_i),\n"
"}\n"
"\n"
"fig_summary = plot_neuron_selectivity_summary(\n"
"    selectivity_counts=selectivity_counts,\n"
"    total_neurons=exp_intense.n_cells,\n"
"    figsize=(8, 6),\n"
"    dpi=DEFAULT_DPI,\n"
")\n"
"plt.show()"
))

cells.append(code_cell(
"# 7. Quality metrics comparison figure\n"
"print('\\n7. Creating quality metrics comparison...')\n"
"\n"
"fig2, axes = plt.subplots(2, 2, figsize=(12, 10))\n"
"fig2.suptitle('Dimensionality reduction quality metrics comparison', fontsize=16)\n"
"\n"
"metrics_to_show = [\n"
"    ('r2_avg', 'Spatial Decoding R^2'),\n"
"    ('distance_correlation', 'Distance Correlation'),\n"
"    ('mi_total', 'Spatial Information (MI)'),\n"
"    ('procrustes_disparity', 'Procrustes Disparity'),\n"
"]\n"
"\n"
"scenarios_to_show = [\n"
"    ('All neurons', 'All'),\n"
"    ('Spatial neurons', 'Spatial'),\n"
"    ('Random half', 'Random'),\n"
"    ('Non-selective', 'Non-sel'),\n"
"]\n"
"\n"
"for idx, (metric_key, metric_title) in enumerate(metrics_to_show):\n"
"    ax = axes[idx // 2, idx % 2]\n"
"    method_names = list(dr_methods_i.keys())\n"
"\n"
"    x = np.arange(len(scenarios_to_show))\n"
"    width = 0.25\n"
"\n"
"    for i, method in enumerate(method_names):\n"
"        values = []\n"
"        for scenario, _ in scenarios_to_show:\n"
"            if scenario in results_intense[method] and results_intense[method][scenario]:\n"
"                value = results_intense[method][scenario]['metrics'][metric_key]\n"
"                values.append(value)\n"
"            else:\n"
"                values.append(0)\n"
"\n"
"        offset = (i - len(method_names) / 2 + 0.5) * width\n"
"        bars = ax.bar(x + offset, values, width, label=method, alpha=0.8)\n"
"\n"
"        # Add value labels on bars\n"
"        for bar, value in zip(bars, values):\n"
"            if value != 0:\n"
"                height = bar.get_height()\n"
"                ax.text(\n"
"                    bar.get_x() + bar.get_width() / 2.0, height,\n"
"                    f'{value:.2f}', ha='center', va='bottom', fontsize=8,\n"
"                )\n"
"\n"
"    ax.set_ylabel(metric_title)\n"
"    ax.set_title(metric_title)\n"
"    ax.set_xticks(x)\n"
"    ax.set_xticklabels(\n"
"        [label for _, label in scenarios_to_show], rotation=45, ha='right'\n"
"    )\n"
"    ax.legend(fontsize=8)\n"
"    ax.grid(True, alpha=0.3)\n"
"\n"
"    if 'r2' in metric_key:\n"
"        ax.set_ylim(0, 1.0)\n"
"\n"
"plt.tight_layout()\n"
"plt.show()"
))

cells.append(code_cell(
"# 8. Summary statistics\n"
"print('Best performing method for spatial reconstruction:')\n"
"best_method_i = None\n"
"best_score_i = -1\n"
"for method_name in results_intense.keys():\n"
"    if (\n"
"        'Spatial neurons' in results_intense[method_name]\n"
"        and results_intense[method_name]['Spatial neurons']\n"
"    ):\n"
"        score = results_intense[method_name]['Spatial neurons']['metrics']['r2_avg']\n"
"        if score > best_score_i:\n"
"            best_score_i = score\n"
"            best_method_i = method_name\n"
"\n"
"if best_method_i:\n"
"    print(f'  {best_method_i} with spatial neurons')\n"
"    print(f'  Spatial decoding R^2: {best_score_i:.3f}')\n"
"\n"
"print('\\nSpatial decoding R^2 comparison:')\n"
"for method_name in results_intense.keys():\n"
"    print(f'\\n  {method_name}:')\n"
"    scenarios_order = [\n"
"        'All neurons', 'Spatial neurons', 'Random half', 'Non-selective',\n"
"    ]\n"
"    for scenario in scenarios_order:\n"
"        if scenario in results_intense[method_name] and results_intense[method_name][scenario]:\n"
"            r2 = results_intense[method_name][scenario]['metrics']['r2_avg']\n"
"            print(f'    {scenario:20s}: {r2:.3f}')\n"
"\n"
"    # Calculate key comparisons\n"
"    if (\n"
"        'All neurons' in results_intense[method_name]\n"
"        and results_intense[method_name]['All neurons']\n"
"    ):\n"
"        r2_all_s = results_intense[method_name]['All neurons']['metrics']['r2_avg']\n"
"\n"
"        if (\n"
"            'Spatial neurons' in results_intense[method_name]\n"
"            and results_intense[method_name]['Spatial neurons']\n"
"        ):\n"
"            r2_spatial_s = results_intense[method_name]['Spatial neurons']['metrics']['r2_avg']\n"
"            imp_s = (r2_spatial_s / max(r2_all_s, 0.001) - 1) * 100\n"
"            print(f'    -> Spatial vs All improvement: {imp_s:+.1f}%')\n"
"\n"
"        if (\n"
"            'Random half' in results_intense[method_name]\n"
"            and results_intense[method_name]['Random half']\n"
"        ):\n"
"            r2_random_s = results_intense[method_name]['Random half']['metrics']['r2_avg']\n"
"            ratio_s = r2_random_s / max(r2_all_s, 0.001)\n"
"            print(f'    -> Random half / All ratio: {ratio_s:.2f}')"
))

# ---------------------------------------------------------------------------
# Assemble & write
# ---------------------------------------------------------------------------

nb = nbformat.v4.new_notebook()
nb["cells"] = cells

# Standard Colab metadata
nb["metadata"] = {
    "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3",
    },
    "language_info": {
        "name": "python",
        "version": "3.10.0",
    },
    "colab": {
        "provenance": [],
        "toc_visible": True,
    },
}

out_path = os.path.join(
    os.path.dirname(__file__), "..", "notebooks", "03_population_geometry_dr.ipynb"
)
out_path = os.path.normpath(out_path)

os.makedirs(os.path.dirname(out_path), exist_ok=True)
with open(out_path, "w", encoding="utf-8") as f:
    nbformat.write(nb, f)

print(f"Wrote {len(cells)} cells to {out_path}")
