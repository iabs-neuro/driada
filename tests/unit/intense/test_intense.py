from driada.intense.intense_base import compute_me_stats
from driada.information.info_base import TimeSeries, MultiTimeSeries
from driada.utils.data import retrieve_relevant_from_nested_dict, create_correlated_gaussian_data
from driada.intense.pipelines import compute_cell_feat_significance
# Experiment imports are handled by synthetic module
import numpy as np
import pytest


def create_correlated_ts(n=100,
                         binarize_first_half=False,
                         binarize_second_half=False,
                         T=2000,
                         noise_scale=0.2):

    # Use the utility function with custom correlation pattern
    correlation_pairs = [(1, n-1, 0.9), (2, n-2, 0.8), (5, n-5, 0.7)]
    signals, _ = create_correlated_gaussian_data(
        n_features=n,
        n_samples=T,
        correlation_pairs=correlation_pairs,
        seed=42
    )

    # cutting coherency windows, setting to 0 outside them
    w=100
    starts = np.random.choice(np.arange(w, T-w), size=10)
    nnz_time_inds = []
    for st in starts:
        nnz_time_inds.extend([st + _ for _ in range(w)])

    cropped_signals = np.zeros((n, T))
    cropped_signals[:, np.array(nnz_time_inds)] = signals[:, np.array(nnz_time_inds)]

    # add noise to remove coinciding values
    small_noise = np.random.multivariate_normal(np.zeros(n),
                                                np.eye(n),
                                                size=T,
                                                check_valid='raise').T*noise_scale

    cropped_signals += small_noise

    tslist1 = [TimeSeries(sig, discrete=False) for sig in cropped_signals[:n // 2,:]]
    tslist2 = [TimeSeries(sig, discrete=False) for sig in cropped_signals[n // 2:, :]]

    if binarize_first_half:
        tslist1 = [binarize_ts(ts, 'av') for ts in tslist1]

    if binarize_second_half:
        tslist2 = [binarize_ts(ts, 'av') for ts in tslist2]

    for ts in tslist1:
        ts.shuffle_mask[:50] = 0
    for ts in tslist2:
        ts.shuffle_mask[:50] = 0

    return tslist1, tslist2


def binarize_ts(ts, thr='av'):
    if not ts.discrete:
        if thr == 'av':
            thr = np.mean(ts.data)
        bin_data = np.zeros(len(ts.data))
        bin_data[ts.data >= thr] = 1

    else:
        raise ValueError('binarize_ts called on discrete TimeSeries')

    return TimeSeries(bin_data, discrete=True)


def test_stage1(correlated_ts_medium, fast_test_params):
    """Test stage1 mode of compute_me_stats."""
    tslist1, tslist2, n = correlated_ts_medium
    k = n // 2
    
    computed_stats, computed_significance, info = compute_me_stats(
        tslist1,
        tslist2,
        mode='stage1',
        n_shuffles_stage1=fast_test_params['n_shuffles_stage1'],
        joint_distr=False,
        metric_distr_type='gamma',
        noise_ampl=fast_test_params['noise_ampl'],
        ds=fast_test_params['ds'],
        topk1=1,
        verbose=fast_test_params['verbose'],
        enable_parallelization=fast_test_params['enable_parallelization']
    )
    
    rel_stats_pairs = retrieve_relevant_from_nested_dict(computed_stats, 'pre_rval', 1)
    rel_sig_pairs = retrieve_relevant_from_nested_dict(computed_significance, 'stage1', True)
    assert rel_sig_pairs == rel_stats_pairs


def test_two_stage(correlated_ts_medium, strict_test_params):
    """Test two-stage mode of compute_me_stats."""
    tslist1, tslist2, n = correlated_ts_medium
    k = n // 2  # num of ts in one block
    
    computed_stats, computed_significance, info = compute_me_stats(
        tslist1,
        tslist2,
        mode='two_stage',
        n_shuffles_stage1=strict_test_params['n_shuffles_stage1'],
        n_shuffles_stage2=strict_test_params['n_shuffles_stage2'],
        joint_distr=False,
        metric_distr_type='gamma',
        noise_ampl=strict_test_params['noise_ampl'],
        ds=strict_test_params['ds'],
        topk1=1,
        topk2=3,  # Reduced from 5 to be more selective
        multicomp_correction=strict_test_params['multicomp_correction'],
        pval_thr=strict_test_params['pval_thr'],
        verbose=strict_test_params['verbose'],
        enable_parallelization=strict_test_params['enable_parallelization']
    )

    rel_sig_pairs = retrieve_relevant_from_nested_dict(
        computed_significance,
        'stage2',
        True,
        allow_missing_keys=True
    )

    # With stricter parameters, we should find exactly the true correlations
    # or at most one additional false positive
    expected_pairs = {(1, k-1), (2, k-2), (5, k-5)}
    found_pairs = set(rel_sig_pairs)
    
    # Assert we found all expected pairs
    assert expected_pairs.issubset(found_pairs), f"Missing expected pairs: {expected_pairs - found_pairs}"
    
    # Allow at most 1 false positive with strict parameters
    false_positives = found_pairs - expected_pairs
    assert len(false_positives) <= 1, f"Too many false positives: {false_positives}"


def aggregate_two_ts(ts1, ts2):
    # add small noise to break degeneracy
    mod_lts1 = TimeSeries(ts1.data + np.random.random(size=len(ts1.data)) * 1e-6)
    mod_lts2 = TimeSeries(ts2.data + np.random.random(size=len(ts2.data)) * 1e-6)
    mts = MultiTimeSeries([mod_lts1, mod_lts2])  # add last two TS into a single 2-d MTS
    return mts

def test_mixed_dimensions(correlated_ts_medium, aggregate_two_ts_func, strict_test_params):
    """Test mixed dimensions with MultiTimeSeries."""
    tslist1, tslist2, n = correlated_ts_medium
    k = n // 2  # num of ts in one block
    
    lts1, lts2 = tslist2[-2:]
    mts = aggregate_two_ts_func(lts1, lts2)
    
    # we expect the correlation between this multi-ts (index k) and ts with indices 1,2
    mod_tslist2 = tslist2 + [mts]
    
    computed_stats, computed_significance, info = compute_me_stats(
        tslist1,
        mod_tslist2,
        mode='two_stage',
        n_shuffles_stage1=strict_test_params['n_shuffles_stage1'],
        n_shuffles_stage2=strict_test_params['n_shuffles_stage2'],
        joint_distr=False,
        allow_mixed_dimensions=True,
        metric_distr_type='gamma',
        noise_ampl=strict_test_params['noise_ampl'],
        ds=strict_test_params['ds'],
        topk1=1,
        topk2=5,
        multicomp_correction=strict_test_params['multicomp_correction'],
        pval_thr=strict_test_params['pval_thr'],
        verbose=strict_test_params['verbose'],
        enable_parallelization=strict_test_params['enable_parallelization']
    )
    
    rel_sig_pairs = retrieve_relevant_from_nested_dict(
        computed_significance,
        'stage2',
        True,
        allow_missing_keys=True
    )
    
    # Expected correlations
    expected_pairs = {(1, k-1), (2, k-2), (5, k-5), (1, k), (2, k)}
    found_pairs = set(rel_sig_pairs)
    
    # Assert we found all expected pairs
    assert expected_pairs.issubset(found_pairs), f"Missing expected pairs: {expected_pairs - found_pairs}"
    
    # Allow at most 2 false positives for mixed dimensions
    false_positives = found_pairs - expected_pairs
    assert len(false_positives) <= 2, f"Too many false positives: {false_positives}"


def test_mirror(correlated_ts_medium, aggregate_two_ts_func, strict_test_params):
    """Test INTENSE of a TimeSeries and MultiTimeSeries set with itself."""
    tslist1, tslist2, n = correlated_ts_medium
    k = n // 2  # num of ts in one block
    
    lts1, lts2 = tslist2[-2:]
    mts1 = aggregate_two_ts_func(lts1, lts2)
    fts1, fts2 = tslist2[:2]
    mts2 = aggregate_two_ts_func(fts1, fts2)
    
    mod_tslist2 = tslist2 + [mts1, mts2]
    # we expect the correlation between mts1 (index k) and ts with indices k-2, k-1
    # we expect the correlation between mts2 (index k+1) and ts with indices 0, 1
    
    computed_stats, computed_significance, info = compute_me_stats(
        mod_tslist2,
        mod_tslist2,
        mode='two_stage',
        n_shuffles_stage1=strict_test_params['n_shuffles_stage1'],
        n_shuffles_stage2=strict_test_params['n_shuffles_stage2'],
        joint_distr=False,
        allow_mixed_dimensions=True,
        metric_distr_type='gamma',
        noise_ampl=strict_test_params['noise_ampl'],
        ds=strict_test_params['ds'],
        topk1=1,
        topk2=5,
        multicomp_correction=strict_test_params['multicomp_correction'],
        pval_thr=strict_test_params['pval_thr'],
        enable_parallelization=strict_test_params['enable_parallelization'],
        seed=1,
        verbose=strict_test_params['verbose']
    )
    
    rel_sig_pairs = retrieve_relevant_from_nested_dict(
        computed_significance,
        'stage2',
        True,
        allow_missing_keys=True
    )
    
    # Expected pairs (both directions for mirror test)
    expected_pairs = {
        (k, k-1), (k-2, k), (k+1, 1), (k, k-2), (0, k+1),
        (k-1, k), (1, k+1), (k+1, 0)
    }
    found_pairs = set(rel_sig_pairs)
    
    # Assert we found all expected pairs
    assert expected_pairs.issubset(found_pairs), f"Missing expected pairs: {expected_pairs - found_pairs}"
    
    # Allow at most 2 false positives for mirror test
    false_positives = found_pairs - expected_pairs
    assert len(false_positives) <= 2, f"Too many false positives: {false_positives}"


def test_two_stage_corr(correlated_ts_small, fast_test_params):
    """Test two-stage mode with correlation metric."""
    tslist1, tslist2, n = correlated_ts_small
    k = n // 2
    
    computed_stats, computed_significance, info = compute_me_stats(
        tslist1,
        tslist2,
        metric='spearmanr',
        mode='two_stage',
        n_shuffles_stage1=fast_test_params['n_shuffles_stage1'],
        n_shuffles_stage2=fast_test_params['n_shuffles_stage2'],
        joint_distr=False,
        metric_distr_type='norm',  # Use normal for correlation metrics
        noise_ampl=1e-4,
        ds=2,  # Downsample for speed
        topk1=1,
        topk2=5,
        multicomp_correction='holm',
        pval_thr=0.01,
        verbose=fast_test_params['verbose'],
        enable_parallelization=fast_test_params['enable_parallelization']
    )
    
    rel_sig_pairs = retrieve_relevant_from_nested_dict(
        computed_significance,
        'stage2',
        True,
        allow_missing_keys=True
    )
    
    # retrieve correlated signals, false positives are likely
    assert set([(1, k-1), (2, k-2)]).issubset(set(rel_sig_pairs))


def test_two_stage_avsignal(correlated_ts_binarized, balanced_test_params):
    """Test two-stage mode with average signal metric."""
    tslist1, tslist2, n = correlated_ts_binarized
    k = n // 2  # num of ts in one block
    
    computed_stats, computed_significance, info = compute_me_stats(
        tslist1,
        tslist2,
        metric='av',
        mode='two_stage',
        n_shuffles_stage1=balanced_test_params['n_shuffles_stage1'],
        n_shuffles_stage2=balanced_test_params['n_shuffles_stage2'],
        joint_distr=False,
        metric_distr_type='norm',  # Use normal for av metric
        noise_ampl=1e-4,
        ds=balanced_test_params['ds'],
        topk1=1,
        topk2=5,
        multicomp_correction='holm',
        pval_thr=0.1,
        verbose=balanced_test_params['verbose'],
        enable_parallelization=balanced_test_params['enable_parallelization']
    )
    
    rel_sig_pairs = retrieve_relevant_from_nested_dict(
        computed_significance,
        'stage2',
        True,
        allow_missing_keys=True
    )
    
    # For av metric with binarized data, we expect to find some correlations
    # but exact pairs may vary, so just check we found something
    assert len(rel_sig_pairs) > 0


# Additional unit tests for better coverage
import pytest
import scipy.stats
from driada.experiment.synthetic import generate_synthetic_exp
from driada.intense.intense_base import (
    validate_time_series_bunches,
    validate_metric,
    validate_common_parameters,
    get_multicomp_correction_thr,
    IntenseResults,
    calculate_optimal_delays,
    scan_pairs,
    scan_pairs_router,
)
from driada.intense.stats import (
    chebyshev_ineq,
    get_lognormal_p,
    get_gamma_p,
    get_distribution_function,
    get_mi_distr_pvalue,
    get_mask,
    stats_not_empty,
    criterion1,
    criterion2,
    get_all_nonempty_pvals,
    get_table_of_stats,
    merge_stage_stats,
    merge_stage_significance,
)


def test_calculate_optimal_delays():
    """Test optimal delay calculation with ground truth."""
    # Create correlated time series with known delay
    length = 1000
    delay_frames = 10  # Known delay
    
    # Create base signal
    base_signal = np.random.randn(length + 50)
    
    # Create delayed version
    signal1 = base_signal[:length]
    signal2 = base_signal[delay_frames:length + delay_frames]
    
    # Add some noise to make it more realistic
    signal1 += 0.5 * np.random.randn(length)
    signal2 += 0.5 * np.random.randn(length)
    
    ts1 = [TimeSeries(signal1)]
    ts2 = [TimeSeries(signal2)]
    
    delays = calculate_optimal_delays(ts1, ts2, metric='mi', 
                                    shift_window=50, ds=1, verbose=False)
    
    assert delays.shape == (1, 1)
    assert np.abs(delays[0, 0]) <= 50  # Within shift window
    # The detected delay should be close to the true delay
    assert np.abs(delays[0, 0] - delay_frames) <= 2  # Allow small error


def test_validate_time_series_bunches_empty():
    """Test validation with empty lists."""
    with pytest.raises(ValueError, match="ts_bunch1 cannot be empty"):
        validate_time_series_bunches([], [TimeSeries(np.random.randn(100))])
        
    with pytest.raises(ValueError, match="ts_bunch2 cannot be empty"):
        validate_time_series_bunches([TimeSeries(np.random.randn(100))], [])


def test_validate_metric():
    """Test metric validation."""
    # Built-in metrics
    assert validate_metric('mi') == 'mi'
    
    # Special metrics
    assert validate_metric('av') == 'special'
    assert validate_metric('fast_pearsonr') == 'special'
    
    # Common correlation metrics
    assert validate_metric('spearman') == 'correlation' 
    assert validate_metric('pearson') == 'correlation'
    assert validate_metric('kendall') == 'correlation'
    
    # Full scipy names
    assert validate_metric('spearmanr') == 'scipy'
    assert validate_metric('pearsonr') == 'scipy'
    assert validate_metric('kendalltau') == 'scipy'
    
    # Invalid metric should raise ValueError
    with pytest.raises(ValueError, match="Unsupported metric"):
        validate_metric('invalid_metric')


def test_validate_common_parameters():
    """Test common parameter validation."""
    # Valid parameters
    validate_common_parameters(shift_window=100, ds=2, nsh=1000, noise_const=0.001)
    
    # Invalid parameters
    with pytest.raises(ValueError, match="shift_window must be non-negative"):
        validate_common_parameters(shift_window=-1)
        
    with pytest.raises(ValueError, match="ds must be positive"):
        validate_common_parameters(ds=0)


def test_multicomp_correction():
    """Test multiple comparison correction methods."""
    pvals = [0.001, 0.01, 0.05, 0.1, 0.5]
    
    # No correction
    thr = get_multicomp_correction_thr(0.05, mode=None)
    assert thr == 0.05
    
    # Bonferroni
    thr = get_multicomp_correction_thr(0.05, mode='bonferroni', nhyp=len(pvals))
    assert thr == 0.05 / len(pvals)
    
    # Holm - the critical threshold is determined by the sorted p-values
    thr = get_multicomp_correction_thr(0.05, mode='holm', all_pvals=pvals)
    # For these p-values, Holm will find the critical threshold
    # The algorithm stops at the first p-value that exceeds its adjusted threshold
    assert isinstance(thr, float)
    assert 0 < thr <= 0.05
    
    # FDR
    thr = get_multicomp_correction_thr(0.05, mode='fdr_bh', all_pvals=pvals)
    assert isinstance(thr, float)
    assert 0 <= thr <= 0.05


def test_intense_results():
    """Test IntenseResults class."""
    results = IntenseResults()
    
    # Test update
    test_data = {'test': 'data'}
    results.update('info', test_data)
    assert results.info == test_data
    
    # Test update_multiple
    test_data_combined = {
        'stats': {'cell1': {'feat1': {'pval': 0.01}}},
        'significance': {'cell1': {'feat1': True}}
    }
    
    results.update_multiple(test_data_combined)
    
    assert 'cell1' in results.stats
    assert results.stats['cell1']['feat1']['pval'] == 0.01
    assert results.significance['cell1']['feat1'] == True


def test_stats_functions():
    """Test statistical functions."""
    # Chebyshev inequality
    data = np.random.randn(1000)
    mean = np.mean(data)
    std = np.std(data)
    val = mean + 2 * std
    p_bound = chebyshev_ineq(data, val)
    assert abs(p_bound - 0.25) < 1e-10
    
    # Test log-normal p-value
    data = np.random.lognormal(0, 1, 1000)
    val = np.percentile(data, 95)
    p_val = get_lognormal_p(data, val)
    assert 0 <= p_val <= 1
    
    # Test gamma p-value
    data = np.random.gamma(2, 2, 1000)
    val = np.percentile(data, 95)
    p_val = get_gamma_p(data, val)
    assert 0 <= p_val <= 1
    
    # Distribution functions
    assert get_distribution_function('gamma') == scipy.stats.gamma
    with pytest.raises(ValueError):
        get_distribution_function('invalid_dist')
    
    # Test MI distribution p-value with different distributions
    data = np.random.gamma(2, 2, 1000)
    val = np.percentile(data, 95)
    p_val = get_mi_distr_pvalue(data, val, 'gamma')
    assert 0 <= p_val <= 1
    
    # Mask creation
    ptable = np.array([[0.001, 0.05], [0.1, 0.5]])
    rtable = np.array([[0.99, 0.95], [0.9, 0.8]])
    mask = get_mask(ptable, rtable, pval_thr=0.05, rank_thr=0.95)
    # Only the first element passes both criteria (p<=0.05 AND r>=0.95)
    expected = np.array([[1, 1], [0, 0]])  # Second element: p=0.05 and r=0.95 both pass
    np.testing.assert_array_equal(mask, expected)
    
    # Test stats_not_empty
    stats1 = {
        'data_hash': 'hash123',
        'pre_rval': 0.99,
        'pre_pval': 0.001
    }
    assert stats_not_empty(stats1, 'hash123', stage=1)
    assert not stats_not_empty(stats1, 'different_hash', stage=1)
    
    # Test criteria functions
    stats_pass = {'pre_rval': 0.995}  # Must be > 1 - 1/(100+1) = 0.99009
    assert criterion1(stats_pass, nsh1=100, topk=1)
    
    stats_pass2 = {'rval': 0.996, 'pval': 0.001}  # Must be > 1 - 5/(1000+1) = 0.995005
    assert criterion2(stats_pass2, nsh2=1000, pval_thr=0.01, topk=5)
    
    # Test get_all_nonempty_pvals
    stats = {
        'cell1': {
            'feat1': {'pval': 0.01},
            'feat2': {'pval': 0.05}
        },
        'cell2': {
            'feat1': {'pval': None},
            'feat2': {'pval': 0.02}
        }
    }
    pvals = get_all_nonempty_pvals(stats, ['cell1', 'cell2'], ['feat1', 'feat2'])
    assert len(pvals) == 3
    assert 0.01 in pvals


def test_get_table_of_stats():
    """Test conversion of metric table to statistics."""
    from scipy.stats import rankdata
    # Create synthetic metric table (3 pairs, 2x2 matrix, 100 shuffles)
    n1, n2, nsh = 2, 2, 100
    metable = np.zeros((n1, n2, nsh + 1))
    
    # Set true values higher than shuffles
    metable[:, :, 0] = 0.5  # True MI values
    metable[:, :, 1:] = np.random.gamma(2, 0.05, size=(n1, n2, nsh))  # Shuffle values
    
    optimal_delays = np.zeros((n1, n2))
    
    # Test stage 1 stats
    stage1_stats = get_table_of_stats(
        metable, optimal_delays, 
        metric_distr_type='gamma', nsh=nsh, stage=1
    )
    
    assert len(stage1_stats) == n1
    assert len(stage1_stats[0]) == n2
    assert 'pre_rval' in stage1_stats[0][0]
    assert 'pre_pval' in stage1_stats[0][0]
    assert stage1_stats[0][0]['pre_rval'] > 0.9  # True value should rank high
    
    # Test stage 2 stats
    stage2_stats = get_table_of_stats(
        metable, optimal_delays,
        metric_distr_type='gamma', nsh=nsh, stage=2
    )
    
    assert 'rval' in stage2_stats[0][0]
    assert 'pval' in stage2_stats[0][0]
    assert 'me' in stage2_stats[0][0]
    assert stage2_stats[0][0]['me'] == 0.5


def test_merge_stage_stats():
    """Test merging statistics from two stages."""
    stage1_stats = {
        0: {0: {'pre_rval': 0.99, 'pre_pval': 0.001}},
        1: {0: {'pre_rval': 0.95, 'pre_pval': 0.05}}
    }
    
    stage2_stats = {
        0: {0: {'rval': 0.995, 'pval': 0.0001, 'me': 0.8}},
        1: {0: {'rval': 0.96, 'pval': 0.04, 'me': 0.3}}
    }
    
    merged = merge_stage_stats(stage1_stats, stage2_stats)
    
    # Check merging preserves all values
    assert merged[0][0]['pre_rval'] == 0.99
    assert merged[0][0]['pre_pval'] == 0.001
    assert merged[0][0]['rval'] == 0.995
    assert merged[0][0]['pval'] == 0.0001
    assert merged[0][0]['me'] == 0.8


def test_merge_stage_significance():
    """Test merging significance results from two stages."""
    stage1_sig = {
        0: {0: {'stage1': True}},
        1: {0: {'stage1': False}}
    }
    
    stage2_sig = {
        0: {0: {'stage2': True}},
        1: {0: {'stage2': False}}
    }
    
    merged = merge_stage_significance(stage1_sig, stage2_sig)
    
    assert merged[0][0]['stage1'] == True
    assert merged[0][0]['stage2'] == True
    assert merged[1][0]['stage1'] == False
    assert merged[1][0]['stage2'] == False


def test_scan_pairs():
    """Test pairwise scanning of time series."""
    # Create test data
    n = 3
    length = 100
    ts_bunch1 = [TimeSeries(np.random.randn(length)) for _ in range(n)]
    ts_bunch2 = [TimeSeries(np.random.randn(length)) for _ in range(n)]
    
    # Create optimal delays array (required parameter)
    optimal_delays = np.zeros((n, n), dtype=int)
    
    # Basic scan with required parameters
    random_shifts, result = scan_pairs(
        ts_bunch1, ts_bunch2,
        metric='mi',
        nsh=10,
        optimal_delays=optimal_delays,
        joint_distr=False,
        ds=1,
        noise_const=1e-3,
        allow_mixed_dimensions=False,
        enable_progressbar=False
    )
    
    assert result.shape == (n, n, 11)  # n x n x (1 true + 10 shuffles)
    assert np.all(result >= 0)  # MI values are non-negative


def test_scan_pairs_router():
    """Test router function for parallel/sequential execution."""
    # Create small test data
    n = 2
    length = 50
    ts_bunch1 = [TimeSeries(np.random.randn(length)) for _ in range(n)]
    ts_bunch2 = [TimeSeries(np.random.randn(length)) for _ in range(n)]
    
    # Create optimal delays array
    optimal_delays = np.zeros((n, n), dtype=int)
    
    # Test sequential execution
    random_shifts_seq, result_seq = scan_pairs_router(
        ts_bunch1, ts_bunch2,
        metric='mi',
        nsh=5,
        optimal_delays=optimal_delays,
        joint_distr=False,
        allow_mixed_dimensions=False,
        ds=1,
        noise_const=1e-3,
        enable_parallelization=True
    )
    
    assert result_seq.shape == (n, n, 6)  # n x n x (1 true + 5 shuffles)
    assert np.all(result_seq >= 0)  # MI values are non-negative


def test_intenseresults_save_load(tmp_path):
    """Test IntenseResults save and load functionality."""
    from driada.utils.data import read_hdf5_to_dict
    
    results = IntenseResults()
    
    # Add test data
    test_stats = {
        'cell1': {'feat1': {'pval': 0.01, 'rval': 0.99}},
        'cell2': {'feat1': {'pval': 0.05, 'rval': 0.95}}
    }
    test_sig = {
        'cell1': {'feat1': True},
        'cell2': {'feat1': False}
    }
    test_info = {'method': 'mi', 'nshuffles': 1000}
    
    results.update('stats', test_stats)
    results.update('significance', test_sig)
    results.update('info', test_info)
    
    # Save to file using the correct method
    save_path = tmp_path / "test_results.h5"
    results.save_to_hdf5(str(save_path))
    
    # Load from file using utils function
    loaded_data = read_hdf5_to_dict(str(save_path))
    
    # Verify loaded data matches original
    assert loaded_data['stats'] == test_stats
    assert loaded_data['significance'] == test_sig
    assert loaded_data['info'] == test_info


def test_validate_common_parameters_edge_cases():
    """Test edge cases for parameter validation."""
    # Test with minimal valid parameters
    validate_common_parameters(shift_window=1, ds=1, nsh=1, noise_const=1e-10)
    
    # Test invalid noise constant
    with pytest.raises(ValueError, match="noise_const must be non-negative"):
        validate_common_parameters(noise_const=-0.001)
    
    # Test invalid number of shuffles  
    with pytest.raises(ValueError, match="nsh must be positive"):
        validate_common_parameters(nsh=-1)


def test_validate_metric_scipy_functions():
    """Test metric validation for scipy functions."""
    # Test that chi2_contingency is recognized as scipy function
    assert validate_metric('chi2_contingency') == 'scipy'
    
    # Test disabling scipy - pearsonr should fail when scipy is disabled
    # but it's also a full scipy name, so let's use a different one
    with pytest.raises(ValueError, match="Unsupported metric"):
        validate_metric('chi2_contingency', allow_scipy=False)


def test_get_mi_distr_pvalue_edge_cases():
    """Test edge cases for MI distribution p-value calculation."""
    # Test with extreme values
    data = np.random.gamma(2, 0.1, 1000)
    
    # Very high value - should give small p-value
    p_high = get_mi_distr_pvalue(data, np.max(data) * 2, 'gamma')
    assert p_high < 0.01
    
    # Very low value - should give large p-value
    p_low = get_mi_distr_pvalue(data, 0, 'gamma')
    assert p_low > 0.9
    
    # Test with normal distribution
    norm_data = np.random.normal(0, 1, 1000)
    p_norm = get_mi_distr_pvalue(norm_data, 2, 'norm')
    assert 0 <= p_norm <= 1


def test_validate_time_series_bunches_mixed_dimensions():
    """Test validation with mixed dimensions."""
    ts1 = TimeSeries(np.random.randn(100))
    mts1 = MultiTimeSeries([ts1, ts1])
    
    # Should fail without allow_mixed_dimensions
    with pytest.raises(ValueError, match="MultiTimeSeries found"):
        validate_time_series_bunches([ts1], [mts1], allow_mixed_dimensions=False)
    
    # Should pass with allow_mixed_dimensions
    validate_time_series_bunches([ts1], [mts1], allow_mixed_dimensions=True)


def test_get_multicomp_correction_fdr():
    """Test FDR correction specifically."""
    # Test case where some discoveries should be made
    pvals = [0.001, 0.002, 0.003, 0.02, 0.8]
    thr = get_multicomp_correction_thr(0.05, mode='fdr_bh', all_pvals=pvals)
    
    # Should allow some discoveries
    assert thr > 0
    assert thr >= 0.003  # At least 3 discoveries expected


# Integration tests for pipelines
from driada.intense.pipelines import compute_cell_feat_significance


def test_compute_cell_feat_significance_integration(small_experiment):
    """Integration test for compute_cell_feat_significance."""
    # Use fixture for synthetic experiment
    exp = small_experiment
    
    # Get the first available feature from the experiment
    available_features = list(exp.dynamic_features.keys())
    if not available_features:
        pytest.skip("No dynamic features available in synthetic experiment")
    
    test_feature = available_features[0]
    
    # Run the pipeline with minimal parameters for speed
    stats, significance, info, results = compute_cell_feat_significance(
        exp,
        cell_bunch=[0, 1],  # Just 2 cells
        feat_bunch=[test_feature],  # Just 1 feature
        metric='mi',
        mode='stage1',  # Just stage 1 for speed
        n_shuffles_stage1=10,  # Minimal shuffles
        verbose=False,
        enable_parallelization=True,
        use_precomputed_stats=False,
        save_computed_stats=False,
        find_optimal_delays=False
    )
    
    # Verify output structure
    assert isinstance(stats, dict)
    assert isinstance(significance, dict)
    assert isinstance(info, dict)
    assert isinstance(results, IntenseResults)
    
    # Check that stats were computed for requested cells/features
    # In INTENSE, cell names are typically string indices
    assert '0' in stats or 0 in stats
    assert '1' in stats or 1 in stats
    
    # Get the actual key format used
    cell_key_0 = '0' if '0' in stats else 0
    cell_key_1 = '1' if '1' in stats else 1
    
    assert test_feature in stats[cell_key_0]
    assert test_feature in stats[cell_key_1]
    
    # Check stats contain expected fields
    assert 'pre_rval' in stats[cell_key_0][test_feature]
    assert 'pre_pval' in stats[cell_key_0][test_feature]


def test_stats_not_empty_stage2():
    """Test stats_not_empty for stage 2."""
    stats2 = {
        'data_hash': 'hash456',
        'rval': 0.99,
        'pval': 0.001,
        'me': 0.5
    }
    assert stats_not_empty(stats2, 'hash456', stage=2)


def test_parallel_mi_equality(correlated_ts_small, fast_test_params):
    """Test that parallel and serial scan_pairs produce identical results."""
    from driada.intense.intense_base import scan_pairs, scan_pairs_parallel
    
    tslist1, tslist2, n = correlated_ts_small
    
    # Generate random optimal delays
    np.random.seed(42)
    optd = np.random.randint(-40, 40, size=(len(tslist1), len(tslist2)))
    
    # Run serial version
    rshifts1, mitable1 = scan_pairs(
        tslist1,
        tslist2,
        metric='mi',
        nsh=fast_test_params['n_shuffles_stage1'],
        optimal_delays=optd,
        ds=fast_test_params['ds'],
        joint_distr=False,
        noise_const=fast_test_params['noise_ampl'],
        seed=42
    )
    
    # Run parallel version
    rshifts2, mitable2 = scan_pairs_parallel(
        tslist1,
        tslist2,
        'mi',
        fast_test_params['n_shuffles_stage1'],
        optd,
        ds=fast_test_params['ds'],
        joint_distr=False,
        n_jobs=2,  # Use 2 jobs for stability
        noise_const=fast_test_params['noise_ampl'],
        seed=42
    )
    
    # Verify shapes and values match
    assert rshifts1.shape == rshifts2.shape
    assert mitable1.shape == mitable2.shape
    assert np.allclose(rshifts1, rshifts2)
    assert np.allclose(mitable1, mitable2)


def test_parallel_router(correlated_ts_small, fast_test_params):
    """Test scan_pairs_router with and without parallelization."""
    from driada.intense.intense_base import scan_pairs_router
    
    tslist1, tslist2, n = correlated_ts_small
    
    # Generate random optimal delays
    np.random.seed(42)
    optd = np.random.randint(-40, 40, size=(len(tslist1), len(tslist2)))
    
    # Run with parallelization
    rshifts1, mitable1 = scan_pairs_router(
        tslist1,
        tslist2,
        'mi',
        fast_test_params['n_shuffles_stage1'],
        optd,
        ds=fast_test_params['ds'],
        joint_distr=False,
        noise_const=fast_test_params['noise_ampl'],
        seed=42,
        enable_parallelization=True,
        n_jobs=2
    )
    
    # Run without parallelization
    rshifts2, mitable2 = scan_pairs_router(
        tslist1,
        tslist2,
        'mi',
        fast_test_params['n_shuffles_stage1'],
        optd,
        ds=fast_test_params['ds'],
        joint_distr=False,
        noise_const=fast_test_params['noise_ampl'],
        seed=42,
        enable_parallelization=False,
        n_jobs=-1
    )
    
    # Verify results match
    assert rshifts1.shape == rshifts2.shape
    assert mitable1.shape == mitable2.shape
    assert np.allclose(rshifts1, rshifts2)
    assert np.allclose(mitable1, mitable2)


def test_optimal_delays_parallel(correlated_ts_small, fast_test_params):
    """Test parallel vs serial optimal delay calculation."""
    from driada.intense.intense_base import (
        calculate_optimal_delays,
        calculate_optimal_delays_parallel
    )
    
    tslist1, tslist2, n = correlated_ts_small
    
    # Parameters
    shift_window = 20  # Reduced for faster testing
    ds = fast_test_params['ds']
    
    # Run parallel version
    optimal_delays1 = calculate_optimal_delays_parallel(
        tslist1,
        tslist2,
        'mi',
        shift_window,
        ds,
        verbose=False,
        n_jobs=2  # Use 2 jobs for stability
    )
    
    # Run serial version
    optimal_delays2 = calculate_optimal_delays(
        tslist1,
        tslist2,
        'mi',
        shift_window,
        ds,
        verbose=False
    )
    
    # Verify results match
    assert optimal_delays1.shape == optimal_delays2.shape
    assert optimal_delays1.shape == (len(tslist1), len(tslist2))
    assert np.allclose(optimal_delays1, optimal_delays2)
    
    # Missing required field
    stats2_incomplete = {
        'data_hash': 'hash456',
        'rval': 0.99,
        'pval': None,
        'me': 0.5
    }
    assert not stats_not_empty(stats2_incomplete, 'hash456', stage=2)


def test_criterion1_edge_cases():
    """Test criterion1 with edge cases."""
    # Test with None pre_rval
    stats_none = {'pre_rval': None}
    assert not criterion1(stats_none, nsh1=100)
    
    # Test with borderline values
    # For topk=1: need pre_rval > 1 - 1/(100+1) = 0.99009
    # For topk=2: need pre_rval > 1 - 2/(100+1) = 0.9802
    stats_border = {'pre_rval': 0.995}
    assert criterion1(stats_border, nsh1=100, topk=1)
    assert criterion1(stats_border, nsh1=100, topk=2)


def test_criterion2_edge_cases():
    """Test criterion2 with edge cases."""
    # Test with missing fields
    stats_missing = {'rval': None, 'pval': 0.001}
    assert not criterion2(stats_missing, nsh2=1000, pval_thr=0.01)
    
    # Test with low rank
    stats_low_rank = {'rval': 0.9, 'pval': 0.0001}
    assert not criterion2(stats_low_rank, nsh2=1000, pval_thr=0.01, topk=5)


def test_compute_me_stats_stage2_only():
    """Test compute_me_stats with stage2 mode."""
    # Create test data
    n = 2
    length = 100
    ts_bunch1 = [TimeSeries(np.random.randn(length)) for _ in range(n)]
    ts_bunch2 = [TimeSeries(np.random.randn(length)) for _ in range(n)]
    
    # Run stage2 only
    stats, significance, info = compute_me_stats(
        ts_bunch1, ts_bunch2,
        mode='stage2',
        n_shuffles_stage1=10,
        n_shuffles_stage2=20,
        metric='mi',
        verbose=False,
        enable_parallelization=True
    )
    
    # Check results
    assert isinstance(stats, dict)
    assert isinstance(significance, dict)
    assert 'optimal_delays' in info
    
    # Stage 2 should have full stats
    assert 'rval' in stats[0][0]
    assert 'pval' in stats[0][0]
    assert 'me' in stats[0][0]
    # Should also have stage1 marked as True


def test_correlation_detection_scaled(scaled_correlated_ts, fast_test_params):
    """Test correlation detection at different scales.
    
    Migrated from test_intense_fast.py to consolidate test suites.
    Tests that correlation detection works across different data sizes.
    """
    tslist1, tslist2, n, T, expected_pairs = scaled_correlated_ts
    
    computed_stats, computed_significance, info = compute_me_stats(
        tslist1,
        tslist2,
        mode='stage1',
        n_shuffles_stage1=fast_test_params['n_shuffles_stage1'],
        ds=fast_test_params['ds'],
        verbose=fast_test_params['verbose'],
        enable_parallelization=fast_test_params['enable_parallelization']
    )
    
    # Should detect at least expected_pairs correlations
    sig_pairs = retrieve_relevant_from_nested_dict(computed_significance, 'stage1', True)
    assert len(sig_pairs) >= expected_pairs
    
    # Verify that detected pairs are marked as significant
    for pair in sig_pairs:
        i, j = pair
        assert computed_significance[i][j]['stage1'] == True


def test_get_calcium_feature_me_profile_cbunch_fbunch(small_experiment):
    """Test get_calcium_feature_me_profile with cbunch/fbunch support."""
    from driada.intense.intense_base import get_calcium_feature_me_profile
    
    # Use fixture
    exp = small_experiment
    
    # Test backward compatibility - old style single cell/feature
    me0, shifted_me = get_calcium_feature_me_profile(exp, 0, 'd_feat_0', window=20, ds=2)
    assert isinstance(me0, float)
    assert isinstance(shifted_me, list)
    assert len(shifted_me) == 20  # window=20, ds=2
    
    # Test new style with cbunch/fbunch
    results = get_calcium_feature_me_profile(
        exp, 
        cbunch=[0, 1], 
        fbunch=['d_feat_0', 'c_feat_0'],
        window=20,
        ds=2
    )
    
    # Check structure
    assert isinstance(results, dict)
    assert len(results) == 2  # 2 cells
    assert 0 in results and 1 in results
    assert len(results[0]) == 2  # 2 features
    assert 'd_feat_0' in results[0] and 'c_feat_0' in results[0]
    assert 'me0' in results[0]['d_feat_0']
    assert 'shifted_me' in results[0]['d_feat_0']
    
    # Test cbunch=None (all cells)
    results_all = get_calcium_feature_me_profile(exp, cbunch=None, fbunch=['d_feat_0'], window=10, ds=2)
    assert len(results_all) == exp.n_cells  # All cells in fixture
    
    # Test invalid cell index
    with pytest.raises(ValueError, match="out of range"):
        get_calcium_feature_me_profile(exp, cbunch=[10], fbunch=['d_feat_0'])


def test_intense_handles_no_significant_neurons(balanced_test_params):
    """Test that INTENSE handles cases with no significant neurons gracefully."""
    # Create truly non-selective neurons by using a different approach
    import numpy as np
    from driada import Experiment, TimeSeries
    
    # Generate random calcium signals and random features
    np.random.seed(42)
    n_neurons = 10  # Reduced from 20
    duration = 300  # Increased for better statistics
    fps = 20.0
    n_frames = int(duration * fps)
    
    # Create pure noise calcium signals (no selectivity)
    calcium_signals = np.random.normal(0, 0.1, size=(n_neurons, n_frames))
    calcium_signals = np.maximum(0, calcium_signals)  # Make non-negative
    
    # Create a random continuous feature
    feature_data = np.random.normal(0, 1, size=n_frames)
    
    # Create experiment
    exp = Experiment(
        'RandomNoise',
        calcium_signals,
        None,  # No spikes
        {},    # No identificators
        {'fps': fps},
        {'c_feat_0': TimeSeries(feature_data, discrete=False)},
        reconstruct_spikes=None
    )
    
    # Run INTENSE - should complete without errors
    stats, significance, info, results = compute_cell_feat_significance(
        exp,
        n_shuffles_stage1=balanced_test_params['n_shuffles_stage1'],
        n_shuffles_stage2=balanced_test_params['n_shuffles_stage2'],
        multicomp_correction='holm',  # Ensure multiple comparison correction
        pval_thr=0.001,  # Stricter threshold
        metric_distr_type='gamma',  # Use gamma distribution (theoretically appropriate for MI)
        verbose=False,  # Disable verbose for speed
        ds=balanced_test_params['ds'],  # Use balanced downsampling
        enable_parallelization=balanced_test_params['enable_parallelization']
    )
    
    # Should handle empty results gracefully
    sig_neurons = exp.get_significant_neurons()
    assert isinstance(sig_neurons, dict)
    
    # With stricter threshold (0.001) and Holm correction, we should get very few false positives
    assert len(sig_neurons) <= 1  # At most 1 false positive expected