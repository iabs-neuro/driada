{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc8c1d1",
   "metadata": {},
   "source": [
    "# Population geometry & dimensionality reduction\n",
    "\n",
    "Individual neurons encode specific variables (Notebook 02), but the\n",
    "population *as a whole* forms a low-dimensional manifold whose geometry\n",
    "reflects the task.  [**DRIADA**](https://driada.readthedocs.io) provides\n",
    "a unified DR toolkit to extract, compare, and evaluate these manifolds.\n",
    "\n",
    "| Step | Notebook | What it does |\n",
    "|---|---|---|\n",
    "| Load & inspect | [01 -- Data loading](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/01_data_loading_and_neurons.ipynb) | Wrap your recording into an `Experiment`, reconstruct spikes, assess quality |\n",
    "| Single-neuron selectivity | [02 -- INTENSE](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/02_selectivity_detection_intense.ipynb) | Detect which neurons encode which behavioral variables |\n",
    "| **Population geometry** | **03 -- this notebook** | Extract low-dimensional manifolds from population activity |\n",
    "| Network analysis | [04 -- Networks](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/04_network_analysis.ipynb) | Build and analyze cell-cell interaction graphs |\n",
    "| Putting it together | [05 -- Advanced](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/05_advanced_capabilities.ipynb) | Combine INTENSE + DR, leave-one-out importance, RSA, RNN analysis |\n",
    "\n",
    "**Sections:**\n",
    "\n",
    "1. **DR API quick reference** -- `MVData` wraps a matrix and provides\n",
    "   one-line access to 7 DR methods.\n",
    "2. **Method comparison** -- Systematic benchmark on synthetic datasets\n",
    "   with quality metrics (k-NN preservation, trustworthiness, continuity,\n",
    "   normalized stress).\n",
    "3. **Sequential DR on neural data** -- PCA first (denoise), then UMAP.\n",
    "   Often better than direct UMAP on high-dimensional neural recordings.\n",
    "4. **Autoencoder-based DR** -- Standard AE with `continue_learning`,\n",
    "   Beta-VAE, and PCA baseline on a circular manifold. Requires PyTorch.\n",
    "5. **Circular manifold & dimensionality estimation** -- Head direction\n",
    "   cells encode a ring. Extract it via DR and estimate intrinsic\n",
    "   dimensionality.\n",
    "6. **INTENSE-guided DR** -- Use INTENSE to select neurons before DR.\n",
    "   Selective neurons produce cleaner spatial embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: revert to '!pip install -q driada' after v1.0.0 PyPI release\n",
    "!pip install -q git+https://github.com/iabs-neuro/driada.git@main\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import tracemalloc\n",
    "import warnings\n",
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_swiss_roll, make_s_curve, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# DRIADA dimensionality reduction\n",
    "from driada.dim_reduction import (\n",
    "    MVData,\n",
    "    dr_sequence,\n",
    "    knn_preservation_rate,\n",
    "    trustworthiness,\n",
    "    continuity,\n",
    "    stress,\n",
    ")\n",
    "from driada.dim_reduction.manifold_metrics import (\n",
    "    manifold_preservation_score,\n",
    "    compute_embedding_alignment_metrics,\n",
    "    procrustes_analysis,\n",
    ")\n",
    "\n",
    "# DRIADA network analysis (used for ProximityGraph demo in Section 1.3)\n",
    "from driada.network import Network\n",
    "\n",
    "# DRIADA experiment / synthetic data\n",
    "from driada.experiment.synthetic import (\n",
    "    generate_2d_manifold_data,\n",
    "    generate_circular_manifold_data,\n",
    ")\n",
    "from driada.experiment import generate_circular_manifold_exp\n",
    "\n",
    "# DRIADA dimensionality estimation\n",
    "from driada.dimensionality import (\n",
    "    eff_dim,\n",
    "    correlation_dimension,\n",
    "    geodesic_dimension,\n",
    "    pca_dimension,\n",
    ")\n",
    "\n",
    "# DRIADA INTENSE + mixed population\n",
    "from driada import (\n",
    "    compute_cell_feat_significance,\n",
    "    generate_mixed_population_exp,\n",
    ")\n",
    "from driada.utils import (\n",
    "    compute_spatial_decoding_accuracy,\n",
    "    compute_spatial_information,\n",
    ")\n",
    "\n",
    "# DRIADA visualization\n",
    "from driada.utils.visual import (\n",
    "    visualize_circular_manifold,\n",
    "    plot_trajectories,\n",
    "    plot_embeddings_grid,\n",
    "    plot_neuron_selectivity_summary,\n",
    "    DEFAULT_DPI,\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4fbee",
   "metadata": {},
   "source": [
    "## 1. DR API quick reference\n",
    "\n",
    "`MVData` wraps a *(n_features x n_samples)* matrix and provides one-line\n",
    "DR via [`get_embedding`](https://driada.readthedocs.io/en/latest/api/dim_reduction/data_structures.html). Seven methods are available: **PCA**, **Isomap**, **LLE**,\n",
    "**Laplacian Eigenmaps**, and **UMAP**. For multi-step\n",
    "pipelines, see [`dr_sequence`](https://driada.readthedocs.io/en/latest/api/dim_reduction/algorithms.html).\n",
    "\n",
    "```python\n",
    "emb = mvdata.get_embedding(method='pca')          # defaults\n",
    "emb = mvdata.get_embedding(method='umap',          # with params\n",
    "                           n_neighbors=30, min_dist=0.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5963d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Swiss roll data for demonstration\n",
    "n_samples = 1000\n",
    "X_raw, color = make_swiss_roll(n_samples, noise=0.1, random_state=42)\n",
    "X = X_raw.T  # Transpose to match MVData format (features x samples)\n",
    "\n",
    "# Create MVData object\n",
    "mvdata = MVData(X)\n",
    "\n",
    "print('=' * 60)\n",
    "print('DIMENSIONALITY REDUCTION METHODS')\n",
    "print('=' * 60)\n",
    "\n",
    "# Linear methods\n",
    "print('\\n--- Linear Methods ---')\n",
    "\n",
    "print('\\n1. PCA (Principal Component Analysis):')\n",
    "print('   Default usage:')\n",
    "print(\"     emb = mvdata.get_embedding(method='pca')\")\n",
    "print('   With parameters:')\n",
    "print(\"     emb = mvdata.get_embedding(method='pca', dim=3)\")\n",
    "emb_pca = mvdata.get_embedding(method='pca')\n",
    "print(f'   -> Result shape: {emb_pca.coords.shape}')\n",
    "\n",
    "# Manifold learning methods\n",
    "print('\\n--- Manifold Learning Methods ---')\n",
    "\n",
    "print('\\n2. Isomap (Isometric Mapping):')\n",
    "print('   Default usage:')\n",
    "print(\"     emb = mvdata.get_embedding(method='isomap')\")\n",
    "print('   With parameters:')\n",
    "print(\"     emb = mvdata.get_embedding(method='isomap', n_neighbors=30, dim=3)\")\n",
    "emb_iso = mvdata.get_embedding(method='isomap')\n",
    "print(f'   -> Result shape: {emb_iso.coords.shape}')\n",
    "\n",
    "print('\\n3. LLE (Locally Linear Embedding):')\n",
    "print('   Default usage:')\n",
    "print(\"     emb = mvdata.get_embedding(method='lle')\")\n",
    "emb_lle = mvdata.get_embedding(method='lle')\n",
    "print(f'   -> Result shape: {emb_lle.coords.shape}')\n",
    "\n",
    "print('\\n4. Laplacian Eigenmaps:')\n",
    "print('   Default usage:')\n",
    "print(\"     emb = mvdata.get_embedding(method='le')\")\n",
    "emb_le = mvdata.get_embedding(method='le')\n",
    "print(f'   -> Result shape: {emb_le.coords.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e849647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization methods\n",
    "print('--- Visualization Methods ---')\n",
    "\n",
    "print('\\n5. UMAP (Uniform Manifold Approximation and Projection):')\n",
    "print('   Default usage:')\n",
    "print(\"     emb = mvdata.get_embedding(method='umap')\")\n",
    "print('   With parameters:')\n",
    "print(\"     emb = mvdata.get_embedding(method='umap', n_neighbors=50, min_dist=0.3)\")\n",
    "emb_umap = mvdata.get_embedding(method='umap', n_neighbors=50, min_dist=0.3)\n",
    "print(f'   -> Result shape: {emb_umap.coords.shape}')\n",
    "\n",
    "# Show parameter options\n",
    "print('\\n' + '=' * 60)\n",
    "print('COMMON PARAMETERS')\n",
    "print('=' * 60)\n",
    "print('\\nAll methods accept:')\n",
    "print('  dim: int - Number of output dimensions (default: 2)')\n",
    "print('\\nGraph-based methods accept:')\n",
    "print('  n_neighbors: int - Number of nearest neighbors')\n",
    "print('\\nUMAP specific:')\n",
    "print('  min_dist: float - Minimum distance between points in embedding')\n",
    "print('\\nDiffusion maps specific:')\n",
    "print('  dm_alpha: float - Diffusion map alpha parameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a01d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA, Isomap, UMAP, LE side by side\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (emb, name) in enumerate(zip(\n",
    "    [emb_pca, emb_iso, emb_umap, emb_le],\n",
    "    ['PCA', 'Isomap', 'UMAP', 'Laplacian Eigenmaps'],\n",
    ")):\n",
    "    ax = axes[i]\n",
    "    coords = emb.coords\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        coords[0, :], coords[1, :], c=color, cmap='viridis', s=20, alpha=0.7\n",
    "    )\n",
    "    ax.set_title(f'{name} Embedding')\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "\n",
    "    # Add colorbar to first subplot\n",
    "    if i == 0:\n",
    "        plt.colorbar(scatter, ax=ax, label='Position on roll')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d3410",
   "metadata": {},
   "source": [
    "### Advanced: sequential DR, custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a93dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential dimensionality reduction (PCA -> UMAP)\n",
    "print('ADVANCED USAGE PATTERNS')\n",
    "print('=' * 60)\n",
    "\n",
    "# Generate high-dimensional data\n",
    "high_dim_data = np.random.randn(100, 500)  # 100 features, 500 samples\n",
    "mvdata_highdim = MVData(high_dim_data)\n",
    "\n",
    "print('\\n1. High-dimensional data:')\n",
    "emb_10d = mvdata_highdim.get_embedding(method='pca', dim=10)\n",
    "print(f'   -> 10D embedding shape: {emb_10d.coords.shape}')\n",
    "\n",
    "mvdata_10d = MVData(emb_10d.coords)\n",
    "emb_2d = mvdata_10d.get_embedding(method='umap')\n",
    "print(f'   -> Final 2D embedding shape: {emb_2d.coords.shape}')\n",
    "\n",
    "print('\\n2. Using custom metrics:')\n",
    "emb_cosine = mvdata.get_embedding(method='isomap', metric='cosine')\n",
    "print(f'   -> Cosine metric embedding shape: {emb_cosine.coords.shape}')\n",
    "\n",
    "print('\\n3. Handling sparse data:')\n",
    "sparse_data = csr_matrix(X)\n",
    "print(f'   -> Sparse matrix shape: {sparse_data.shape}')\n",
    "mvdata_sparse = MVData(sparse_data)\n",
    "emb_sparse = mvdata_sparse.get_embedding(method='pca')\n",
    "print(f'   -> Sparse data embedding shape: {emb_sparse.coords.shape}')\n",
    "\n",
    "print('\\n4. Sequential dimensionality reduction (use high-dim data):')\n",
    "print('   Method 1 (intuitive - manual chaining):')\n",
    "emb1_seq = mvdata_highdim.get_embedding(method='pca', dim=20)\n",
    "mvdata2_seq = MVData(emb1_seq.coords)\n",
    "emb2_seq = mvdata2_seq.get_embedding(method='umap', dim=2)\n",
    "print(f'   -> Result shape: {emb2_seq.coords.shape}')\n",
    "\n",
    "print('\\n   Method 2 (recommended - using dr_sequence):')\n",
    "emb_seq = dr_sequence(mvdata_highdim, steps=[\n",
    "    ('pca', {'dim': 20}),\n",
    "    ('umap', {'dim': 2})\n",
    "])\n",
    "print(f'   -> Result shape: {emb_seq.coords.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b1a12",
   "metadata": {},
   "source": [
    "### Graph structure behind DR\n",
    "\n",
    "Graph-based DR methods (Isomap, LLE, Laplacian Eigenmaps) don't just\n",
    "produce coordinates -- they construct an internal **proximity graph** where\n",
    "nodes are data points and edges connect neighbors. In DRIADA, this graph\n",
    "is a [`ProximityGraph`](https://driada.readthedocs.io/en/latest/api/dim_reduction/data_structures.html)\n",
    "that **inherits from [`Network`](https://driada.readthedocs.io/en/latest/api/network/core.html)**,\n",
    "giving you access to spectral decomposition, entropy, community detection,\n",
    "and all other `Network` analysis methods.\n",
    "\n",
    "Access it via `embedding.graph` after running any graph-based method.\n",
    "For a full treatment of network spectral analysis, see\n",
    "[Notebook 04 -- Network analysis](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/04_network_analysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ffa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Isomap embedding from Section 1 built a k-NN proximity graph internally\n",
    "pgraph = emb_iso.graph\n",
    "\n",
    "print(f\"Type: {type(pgraph).__name__}\")\n",
    "print(f\"  inherits from Network: {isinstance(pgraph, Network)}\")\n",
    "print(f\"Nodes: {pgraph.n}\")\n",
    "print(f\"Edges: {pgraph.adj.nnz // 2}\")\n",
    "print(f\"Mean degree: {pgraph.deg.mean():.1f}\")\n",
    "print(f\"Metric used: {pgraph.metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c493c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral analysis of the k-NN graph that powers Isomap\n",
    "pgraph.diagonalize(mode='nlap')\n",
    "nlap_spectrum = pgraph.get_spectrum('nlap')\n",
    "ipr = pgraph.get_ipr('nlap')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Normalized Laplacian spectrum\n",
    "sorted_spec = np.sort(np.real(nlap_spectrum))\n",
    "axes[0].plot(sorted_spec, 'o', markersize=2)\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Eigenvalue')\n",
    "axes[0].set_title('Normalized Laplacian spectrum')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# IPR -- eigenvector localization\n",
    "axes[1].plot(np.sort(ipr), 'o', markersize=2)\n",
    "axes[1].axhline(1.0 / pgraph.n, color='r', linestyle='--',\n",
    "                label=f'1/N = {1.0/pgraph.n:.4f}')\n",
    "axes[1].set_xlabel('Eigenvector index (sorted)')\n",
    "axes[1].set_ylabel('IPR')\n",
    "axes[1].set_title('Inverse participation ratio')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Thermodynamic entropy\n",
    "tlist = np.logspace(-2, 2, 50)\n",
    "entropy = pgraph.calculate_thermodynamic_entropy(tlist, norm=True)\n",
    "axes[2].semilogx(tlist, entropy, linewidth=2)\n",
    "axes[2].set_xlabel('Temperature')\n",
    "axes[2].set_ylabel('Entropy (bits)')\n",
    "axes[2].set_title('Von Neumann entropy S(t)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Spectral analysis of Isomap k-NN graph', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Fiedler value: {sorted_spec[1]:.4f}')\n",
    "print(f'Spectral gap: {sorted_spec[1] - sorted_spec[0]:.4f}')\n",
    "print(f'Max entropy: {np.max(entropy):.2f} bits '\n",
    "      f'(upper bound = log2(N) = {np.log2(pgraph.n):.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ad612",
   "metadata": {},
   "source": [
    "The Laplacian spectrum reveals the graph's connectivity structure:\n",
    "a large spectral gap indicates the graph is well-connected, while\n",
    "clustered eigenvalues near zero suggest loosely connected components.\n",
    "The IPR shows whether eigenvectors are delocalized (spread across\n",
    "all nodes) or localized (concentrated on a few).\n",
    "\n",
    "These same spectral tools apply to *any* `Network` -- functional\n",
    "connectivity from INTENSE, structural connectomes, or correlation\n",
    "networks. See [Notebook 04](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/04_network_analysis.ipynb)\n",
    "for the full spectral analysis toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a42b3",
   "metadata": {},
   "source": [
    "## 2. Method comparison\n",
    "\n",
    "Systematic benchmark on multiple synthetic datasets. Quality metrics:\n",
    "\n",
    "- [`knn_preservation_rate`](https://driada.readthedocs.io/en/latest/api/dim_reduction/manifold_metrics.html) -- fraction of original k nearest neighbors preserved in the embedding.\n",
    "- [`trustworthiness`](https://driada.readthedocs.io/en/latest/api/dim_reduction/manifold_metrics.html) -- fraction of embedding neighbors that are true neighbors in the original space.\n",
    "- [`continuity`](https://driada.readthedocs.io/en/latest/api/dim_reduction/manifold_metrics.html) -- fraction of true neighbors that remain neighbors in the embedding.\n",
    "- [`stress`](https://driada.readthedocs.io/en/latest/api/dim_reduction/manifold_metrics.html) -- normalized Frobenius distance between the original and embedded distance matrices.\n",
    "\n",
    "For a single composite score, use [`manifold_preservation_score`](https://driada.readthedocs.io/en/latest/api/dim_reduction/manifold_metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2173a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_datasets(n_samples=1000, noise=0.0, seed=42):\n",
    "    \"\"\"Generate various test datasets for DR method comparison.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    datasets = {}\n",
    "\n",
    "    # 1. Swiss Roll - classic nonlinear manifold\n",
    "    print('Generating Swiss Roll...')\n",
    "    X_swiss, color_swiss = make_swiss_roll(\n",
    "        n_samples=n_samples, noise=noise, random_state=seed\n",
    "    )\n",
    "    datasets['swiss_roll'] = (X_swiss, color_swiss)\n",
    "\n",
    "    # 2. S-Curve - another nonlinear manifold\n",
    "    print('Generating S-Curve...')\n",
    "    X_scurve, color_scurve = make_s_curve(\n",
    "        n_samples=n_samples, noise=noise, random_state=seed\n",
    "    )\n",
    "    datasets['s_curve'] = (X_scurve, color_scurve)\n",
    "\n",
    "    # 3. Circular manifold - tests circular topology\n",
    "    print('Generating Circular manifold...')\n",
    "    angles = np.linspace(0, 2 * np.pi, n_samples, endpoint=False)\n",
    "    circle_3d = np.column_stack([\n",
    "        np.cos(angles), np.sin(angles),\n",
    "        0.1 * np.random.randn(n_samples),\n",
    "    ])\n",
    "    datasets['circle_3d'] = (circle_3d, angles)\n",
    "\n",
    "    # 4. High-dimensional Gaussian (intrinsic dim ~5 in 50D)\n",
    "    print('Generating High-D Gaussian...')\n",
    "    U = np.random.randn(50, 5)\n",
    "    V = np.random.randn(5, n_samples)\n",
    "    X_gaussian = (U @ V + noise * np.random.randn(50, n_samples)).T\n",
    "    datasets['gaussian_50d'] = (X_gaussian, X_gaussian @ U[:, 0])\n",
    "\n",
    "    # 5. Clustered data (5 clusters in 20D)\n",
    "    print('Generating Clustered data...')\n",
    "    X_clusters, y_clusters = make_blobs(\n",
    "        n_samples=n_samples, n_features=20,\n",
    "        centers=5, cluster_std=0.5, random_state=seed,\n",
    "    )\n",
    "    datasets['clusters_20d'] = (X_clusters, y_clusters)\n",
    "\n",
    "    # 6. Noisy sphere\n",
    "    print('Generating Noisy sphere...')\n",
    "    phi = np.random.uniform(0, 2 * np.pi, n_samples)\n",
    "    theta = np.random.uniform(0, np.pi, n_samples)\n",
    "    sphere_3d = np.column_stack([\n",
    "        np.sin(theta) * np.cos(phi),\n",
    "        np.sin(theta) * np.sin(phi),\n",
    "        np.cos(theta),\n",
    "    ])\n",
    "    sphere_3d += noise * np.random.randn(n_samples, 3)\n",
    "    datasets['sphere_3d'] = (sphere_3d, phi)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Generate test datasets\n",
    "print('1. GENERATING TEST DATASETS')\n",
    "print('-' * 40)\n",
    "datasets = generate_test_datasets(n_samples=1000, noise=0.05)\n",
    "print(f'\\nGenerated {len(datasets)} test datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e5c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dr_method_configs() -> Dict[str, Dict]:\n",
    "    \"\"\"Get configuration parameters for each DR method.\"\"\"\n",
    "    configs = {}\n",
    "    configs['pca'] = {\n",
    "        'params': {},\n",
    "        'description': 'Linear projection maximizing variance',\n",
    "    }\n",
    "    configs['isomap'] = {\n",
    "        'params': {'n_neighbors': 10, 'max_deleted_nodes': 0.3},\n",
    "        'description': 'Preserves geodesic distances',\n",
    "    }\n",
    "    configs['umap'] = {\n",
    "        'params': {'n_neighbors': 15, 'min_dist': 0.1},\n",
    "        'description': 'Balances local and global structure',\n",
    "    }\n",
    "    return configs\n",
    "\n",
    "\n",
    "# Configure methods\n",
    "print('2. CONFIGURING DR METHODS')\n",
    "print('-' * 40)\n",
    "methods = get_dr_method_configs()\n",
    "print(f'Configured {len(methods)} DR methods: {\", \".join(methods.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dr_method(data, labels, method_name, method_config, k_neighbors=10):\n",
    "    \"\"\"Evaluate a single DR method on a dataset and return quality metrics.\"\"\"\n",
    "    results = {\n",
    "        'method': method_name,\n",
    "        'description': method_config.get('description', ''),\n",
    "        'success': False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        mvdata = MVData(data.T)  # MVData expects (n_features, n_samples)\n",
    "        tracemalloc.start()\n",
    "        start_time = time.time()\n",
    "        params = method_config['params']\n",
    "\n",
    "        if method_config.get('requires_distmat', False):\n",
    "            mvdata.get_distmat()\n",
    "\n",
    "        embedding = mvdata.get_embedding(method=method_name, **params)\n",
    "        runtime = time.time() - start_time\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "\n",
    "        coords = embedding.coords.T  # Shape: (n_samples, n_dims)\n",
    "\n",
    "        # Handle lost nodes if graph-based method\n",
    "        data_filtered, labels_filtered = data, labels\n",
    "        if hasattr(embedding, 'graph') and hasattr(embedding.graph, 'lost_nodes'):\n",
    "            lost_nodes = embedding.graph.lost_nodes\n",
    "            if len(lost_nodes) > 0:\n",
    "                kept_mask = np.ones(data.shape[0], dtype=bool)\n",
    "                kept_mask[lost_nodes] = False\n",
    "                data_filtered = data[kept_mask]\n",
    "                labels_filtered = labels[kept_mask]\n",
    "\n",
    "        # Compute quality metrics\n",
    "        if coords.shape[0] == data_filtered.shape[0]:\n",
    "            results.update({\n",
    "                'success': True,\n",
    "                'embedding': coords,\n",
    "                'labels': labels_filtered,\n",
    "                'runtime': runtime,\n",
    "                'memory_mb': peak / 1024 / 1024,\n",
    "                'knn_preservation': knn_preservation_rate(data_filtered, coords, k=k_neighbors),\n",
    "                'trustworthiness': trustworthiness(data_filtered, coords, k=k_neighbors),\n",
    "                'continuity': continuity(data_filtered, coords, k=k_neighbors),\n",
    "                'stress': stress(data_filtered, coords, normalized=True),\n",
    "                'n_samples': coords.shape[0],\n",
    "                'n_lost': data.shape[0] - coords.shape[0],\n",
    "            })\n",
    "        else:\n",
    "            results['error'] = (\n",
    "                f'Dimension mismatch: {coords.shape[0]} vs {data_filtered.shape[0]}'\n",
    "            )\n",
    "    except Exception as e:\n",
    "        results['error'] = f'{type(e).__name__}: {str(e)}'\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison(\n",
    "    datasets: Dict[str, Tuple[np.ndarray, np.ndarray]],\n",
    "    methods: Dict[str, Dict],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run systematic comparison of DR methods on all datasets.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    total_comparisons = len(datasets) * len(methods)\n",
    "    current = 0\n",
    "\n",
    "    for dataset_name, (data, labels) in datasets.items():\n",
    "        print(f'\\n{\"=\" * 60}')\n",
    "        print(f'Dataset: {dataset_name} (shape: {data.shape})')\n",
    "        print(f'{\"=\" * 60}')\n",
    "\n",
    "        # Standardize data\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "        for method_name, method_config in methods.items():\n",
    "            current += 1\n",
    "            print(\n",
    "                f'\\n[{current}/{total_comparisons}] Evaluating {method_name}...',\n",
    "                end='', flush=True,\n",
    "            )\n",
    "\n",
    "            result = evaluate_dr_method(data_scaled, labels, method_name, method_config)\n",
    "            result['dataset'] = dataset_name\n",
    "            result['n_features'] = data.shape[1]\n",
    "\n",
    "            if result['success']:\n",
    "                print(\n",
    "                    f' Done! (runtime: {result[\"runtime\"]:.2f}s, '\n",
    "                    + f'k-NN: {result[\"knn_preservation\"]:.3f}, '\n",
    "                    + f'trust: {result[\"trustworthiness\"]:.3f})'\n",
    "                )\n",
    "            else:\n",
    "                print(f' Failed! ({result.get(\"error\", \"Unknown error\")})')\n",
    "\n",
    "            all_results.append(result)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "print('\\n3. RUNNING SYSTEMATIC COMPARISON')\n",
    "print('-' * 40)\n",
    "results_df = run_comparison(datasets, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d18155",
   "metadata": {},
   "source": [
    "### Speed benchmark & quality summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "success_df = results_df[results_df['success']].copy()\n",
    "\n",
    "# Calculate summary statistics\n",
    "method_summary = (\n",
    "    success_df.groupby('method')\n",
    "    .agg({\n",
    "        'knn_preservation': 'mean',\n",
    "        'trustworthiness': 'mean',\n",
    "        'continuity': 'mean',\n",
    "        'stress': 'mean',\n",
    "        'runtime': 'mean',\n",
    "    })\n",
    "    .round(3)\n",
    ")\n",
    "\n",
    "# Best overall quality\n",
    "success_df['avg_quality'] = success_df[\n",
    "    ['knn_preservation', 'trustworthiness', 'continuity']\n",
    "].mean(axis=1)\n",
    "best_quality = success_df.groupby('method')['avg_quality'].mean().idxmax()\n",
    "\n",
    "# Fastest method\n",
    "fastest = method_summary['runtime'].idxmin()\n",
    "\n",
    "# Best for visualization (high trustworthiness)\n",
    "best_viz = method_summary['trustworthiness'].idxmax()\n",
    "\n",
    "print('[SUMMARY] METHOD SUMMARY:')\n",
    "print(method_summary)\n",
    "\n",
    "print(f'\\n[BEST] Best Overall Quality: {best_quality}')\n",
    "print(f'[BEST] Fastest: {fastest} (avg {method_summary.loc[fastest, \"runtime\"]:.3f}s)')\n",
    "print(f'[BEST] Best Visualization: {best_viz} '\n",
    "       f'(trustworthiness: {method_summary.loc[best_viz, \"trustworthiness\"]:.3f})')\n",
    "\n",
    "print('\\n[RECOMMENDATIONS] USE CASE RECOMMENDATIONS:')\n",
    "print('  - Exploratory visualization: UMAP')\n",
    "print('  - Distance preservation: Isomap')\n",
    "print('  - Linear relationships: PCA - fast, interpretable')\n",
    "print('  - Manifold learning: Isomap or UMAP')\n",
    "print('  - Large datasets: PCA or UMAP - computationally efficient')\n",
    "print('  - Geodesic distances: Isomap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations: quality metrics heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "metrics = ['knn_preservation', 'trustworthiness', 'continuity']\n",
    "metric_labels = {\n",
    "    'knn_preservation': 'k-NN preservation',\n",
    "    'trustworthiness': 'Trustworthiness',\n",
    "    'continuity': 'Continuity',\n",
    "}\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    pivot = success_df.pivot_table(\n",
    "        values=metric, index='method', columns='dataset', aggfunc='mean'\n",
    "    )\n",
    "    sns.heatmap(\n",
    "        pivot, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "        vmin=0, vmax=1,\n",
    "        cbar_kws={'label': metric_labels[metric]},\n",
    "    )\n",
    "    plt.title(f'{metric_labels[metric]} by method and dataset')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Method')\n",
    "\n",
    "# Runtime comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "runtime_pivot = success_df.pivot_table(\n",
    "    values='runtime', index='method', columns='dataset', aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(\n",
    "    np.log10(runtime_pivot + 0.001), annot=runtime_pivot.round(2),\n",
    "    fmt='g', cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'log10(Runtime in seconds)'},\n",
    ")\n",
    "plt.title('Runtime by method and dataset')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876514b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality vs Speed trade-off\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for dataset in success_df['dataset'].unique():\n",
    "    mask = success_df['dataset'] == dataset\n",
    "    plt.scatter(\n",
    "        success_df[mask]['runtime'],\n",
    "        success_df[mask]['avg_quality'],\n",
    "        label=dataset, s=100, alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # Add method labels\n",
    "    for _, row in success_df[mask].iterrows():\n",
    "        plt.annotate(\n",
    "            row['method'],\n",
    "            (row['runtime'], row['avg_quality']),\n",
    "            xytext=(5, 5), textcoords='offset points',\n",
    "            fontsize=8, alpha=0.7,\n",
    "        )\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Runtime (seconds, log scale)')\n",
    "plt.ylabel('Average Quality Score')\n",
    "plt.title('Quality vs speed trade-off')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example embeddings for Swiss Roll\n",
    "swiss_results = success_df[success_df['dataset'] == 'swiss_roll']\n",
    "\n",
    "if len(swiss_results) > 0:\n",
    "    n_methods = len(swiss_results)\n",
    "    fig, axes = plt.subplots(1, n_methods, figsize=(5 * n_methods, 5))\n",
    "    if n_methods == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, (_, result) in enumerate(swiss_results.iterrows()):\n",
    "        if i >= n_methods:\n",
    "            break\n",
    "\n",
    "        ax = axes[i]\n",
    "        embedding = result['embedding']\n",
    "        labels = result['labels']\n",
    "\n",
    "        scatter = ax.scatter(\n",
    "            embedding[:, 0], embedding[:, 1],\n",
    "            c=labels, cmap='viridis', s=20, alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{result['method']}\\n\"\n",
    "            f\"(k-NN: {result['knn_preservation']:.3f}, \"\n",
    "            f\"Trust: {result['trustworthiness']:.3f})\"\n",
    "        )\n",
    "        ax.set_xlabel('Dim 1')\n",
    "        ax.set_ylabel('Dim 2')\n",
    "\n",
    "    plt.suptitle('Swiss roll embeddings', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903135a8",
   "metadata": {},
   "source": [
    "## 3. Sequential DR on neural data\n",
    "\n",
    "PCA first (denoise) followed by UMAP. Often better than direct UMAP on\n",
    "high-dimensional neural data because PCA removes noise-dominated\n",
    "dimensions before the nonlinear step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_manifold_metrics(true_positions, embedding_coords, k=10):\n",
    "    \"\"\"Compute various manifold preservation metrics.\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # KNN preservation\n",
    "    metrics['knn_preservation'] = knn_preservation_rate(\n",
    "        true_positions.T, embedding_coords.T, k=k,\n",
    "    )\n",
    "\n",
    "    # Trustworthiness and continuity\n",
    "    metrics['trustworthiness'] = trustworthiness(\n",
    "        true_positions.T, embedding_coords.T, k=k\n",
    "    )\n",
    "    metrics['continuity'] = continuity(\n",
    "        true_positions.T, embedding_coords.T, k=k\n",
    "    )\n",
    "\n",
    "    # Overall manifold preservation score (returns dict)\n",
    "    manifold_scores = manifold_preservation_score(\n",
    "        true_positions.T, embedding_coords.T, k_neighbors=k\n",
    "    )\n",
    "    # Extract the overall score\n",
    "    metrics['manifold_score'] = manifold_scores['overall_score']\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "print('Generating synthetic neural data from 2D spatial environment...')\n",
    "\n",
    "# Generate synthetic data with 2D spatial manifold\n",
    "calcium, positions, place_field_centers, firing_rates = generate_2d_manifold_data(\n",
    "    n_neurons=100,\n",
    "    duration=800,  # seconds\n",
    "    fps=20.0,  # Hz\n",
    "    field_sigma=0.1,\n",
    "    step_size=0.02,\n",
    "    seed=123,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Extract neural activity and true positions\n",
    "neural_data = calcium  # (n_neurons, n_timepoints)\n",
    "true_positions = positions  # (2, n_timepoints)\n",
    "\n",
    "print(f'Neural data shape: {neural_data.shape}')\n",
    "print(f'True positions shape: {true_positions.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92bf74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MVData object with downsampling\n",
    "mvdata_neural = MVData(neural_data, downsampling=5)\n",
    "\n",
    "# Approach 1: Direct UMAP on all neurons\n",
    "print('\\n=== Approach 1: Direct UMAP ===')\n",
    "embedding_direct = mvdata_neural.get_embedding(\n",
    "    method='umap', dim=2, n_neighbors=50, min_dist=0.8\n",
    ")\n",
    "\n",
    "# Approach 2: PCA -> UMAP sequence\n",
    "print('\\n=== Approach 2: PCA -> UMAP ===')\n",
    "embedding_sequence = dr_sequence(\n",
    "    mvdata_neural,\n",
    "    steps=[\n",
    "        ('pca', {'dim': 20}),  # First reduce to 20 PCs\n",
    "        ('umap', {'dim': 2, 'n_neighbors': 50, 'min_dist': 0.8}),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Compute manifold preservation metrics\n",
    "print('\\n=== Manifold Preservation Metrics ===')\n",
    "\n",
    "# Downsample true positions to match the embeddings\n",
    "true_positions_ds = true_positions[:, ::5]\n",
    "\n",
    "metrics_direct = compute_manifold_metrics(\n",
    "    true_positions_ds, embedding_direct.coords\n",
    ")\n",
    "metrics_sequence = compute_manifold_metrics(\n",
    "    true_positions_ds, embedding_sequence.coords\n",
    ")\n",
    "\n",
    "print('\\nDirect UMAP:')\n",
    "for name, value in metrics_direct.items():\n",
    "    print(f'  {name}: {value:.4f}')\n",
    "\n",
    "print('\\nPCA -> UMAP:')\n",
    "for name, value in metrics_sequence.items():\n",
    "    print(f'  {name}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: side-by-side comparison\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot true positions\n",
    "ax1 = plt.subplot(131)\n",
    "scatter = ax1.scatter(\n",
    "    true_positions_ds[0], true_positions_ds[1],\n",
    "    c=np.arange(true_positions_ds.shape[1]),\n",
    "    cmap='viridis', alpha=0.6, s=20,\n",
    ")\n",
    "ax1.set_title('True 2D Positions')\n",
    "ax1.set_xlabel('X position')\n",
    "ax1.set_ylabel('Y position')\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Plot direct UMAP embedding\n",
    "ax2 = plt.subplot(132)\n",
    "ax2.scatter(\n",
    "    embedding_direct.coords[0], embedding_direct.coords[1],\n",
    "    c=np.arange(embedding_direct.coords.shape[1]),\n",
    "    cmap='viridis', alpha=0.6, s=20,\n",
    ")\n",
    "ax2.set_title(\n",
    "    f'Direct UMAP\\n(Manifold score: {metrics_direct[\"manifold_score\"]:.3f})'\n",
    ")\n",
    "ax2.set_xlabel('UMAP 1')\n",
    "ax2.set_ylabel('UMAP 2')\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "# Plot PCA->UMAP embedding\n",
    "ax3 = plt.subplot(133)\n",
    "ax3.scatter(\n",
    "    embedding_sequence.coords[0], embedding_sequence.coords[1],\n",
    "    c=np.arange(embedding_sequence.coords.shape[1]),\n",
    "    cmap='viridis', alpha=0.6, s=20,\n",
    ")\n",
    "ax3.set_title(\n",
    "    f'PCA -> UMAP\\n(Manifold score: {metrics_sequence[\"manifold_score\"]:.3f})'\n",
    ")\n",
    "ax3.set_xlabel('UMAP 1')\n",
    "ax3.set_ylabel('UMAP 2')\n",
    "ax3.set_aspect('equal')\n",
    "\n",
    "plt.colorbar(scatter, ax=[ax1, ax2, ax3], label='Time', fraction=0.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print improvement percentages\n",
    "print('\\n=== Method Comparison ===')\n",
    "print('PCA -> UMAP vs Direct UMAP:')\n",
    "for metric in ['knn_preservation', 'trustworthiness', 'continuity']:\n",
    "    improvement = (\n",
    "        (metrics_sequence[metric] - metrics_direct[metric])\n",
    "        / metrics_direct[metric] * 100\n",
    "    )\n",
    "    print(f'  {metric}: {improvement:+.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed metrics comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot true positions for reference\n",
    "time_points = np.arange(true_positions_ds.shape[1])\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(\n",
    "    true_positions_ds[0], true_positions_ds[1],\n",
    "    c=time_points, cmap='viridis', s=20, alpha=0.6,\n",
    ")\n",
    "ax.set_title('True 2D Positions')\n",
    "ax.set_xlabel('X position')\n",
    "ax.set_ylabel('Y position')\n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(scatter, ax=ax, label='Time')\n",
    "\n",
    "# Summary metrics comparison\n",
    "ax = axes[1]\n",
    "metrics_names = list(metrics_direct.keys())\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "values_direct = [metrics_direct[m] for m in metrics_names]\n",
    "values_sequence = [metrics_sequence[m] for m in metrics_names]\n",
    "\n",
    "ax.bar(x - width / 2, values_direct, width, label='Direct UMAP', alpha=0.8)\n",
    "ax.bar(x + width / 2, values_sequence, width, label='PCA -> UMAP', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Manifold preservation metrics')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(\n",
    "    [m.replace('_', '\\n') for m in metrics_names], rotation=45, ha='right'\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print improvement summary\n",
    "print('\\n=== Improvement Summary ===')\n",
    "for metric in metrics_names:\n",
    "    improvement = (\n",
    "        (metrics_sequence[metric] - metrics_direct[metric])\n",
    "        / metrics_direct[metric] * 100\n",
    "    )\n",
    "    print(f'{metric}: {improvement:+.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c08033",
   "metadata": {},
   "source": [
    "## 4. Autoencoder-based DR\n",
    "\n",
    "Neural network DR alternatives via [`flexible_ae`](https://driada.readthedocs.io/en/latest/api/dim_reduction/neural_methods.html): **standard autoencoder** (AE) with\n",
    "`continue_learning`, **Beta-VAE** with KL divergence, and a PCA baseline.\n",
    "Key parameters: `architecture` (`'ae'` or `'vae'`) selects the model type,\n",
    "and `continue_learning` resumes training without resetting weights.\n",
    "Requires PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf518bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch  # noqa: F401\n",
    "    HAS_TORCH = True\n",
    "    print('PyTorch available -- autoencoder examples will run.')\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\n",
    "        'PyTorch not found. Install with: pip install torch\\n'\n",
    "        'Autoencoder cells will be skipped.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39eb5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TORCH:\n",
    "    print('=' * 60)\n",
    "    print('DRIADA autoencoder DR example')\n",
    "    print('=' * 60)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. Generate synthetic data (head direction cells on circular manifold)\n",
    "    # ------------------------------------------------------------------\n",
    "    print('\\n[1] Generating synthetic head direction cell data')\n",
    "    print('-' * 40)\n",
    "    calcium_ae, head_direction_ae, preferred_dirs_ae, rates_ae = (\n",
    "        generate_circular_manifold_data(\n",
    "            n_neurons=200,\n",
    "            kappa=4.0,\n",
    "            duration=1200,\n",
    "            seed=42,\n",
    "            verbose=True,\n",
    "        )\n",
    "    )\n",
    "    print(f'  Calcium shape: {calcium_ae.shape}')\n",
    "    print(f'  Head direction shape: {head_direction_ae.shape}')\n",
    "\n",
    "    mvdata_ae = MVData(calcium_ae, verbose=False)\n",
    "    color_ae = head_direction_ae  # angle for coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f97a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TORCH:\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. Standard autoencoder with continue_learning\n",
    "    # ------------------------------------------------------------------\n",
    "    print('\\n[2] Standard autoencoder')\n",
    "    print('-' * 40)\n",
    "\n",
    "    # Train for 5 epochs (not fully converged)\n",
    "    emb_ae = mvdata_ae.get_embedding(\n",
    "        method='flexible_ae',\n",
    "        dim=2,\n",
    "        architecture='ae',\n",
    "        inter_dim=64,\n",
    "        epochs=5,\n",
    "        lr=1e-3,\n",
    "        feature_dropout=0.1,\n",
    "        loss_components=[{'name': 'reconstruction', 'weight': 1.0, 'loss_type': 'mse'}],\n",
    "        verbose=False,\n",
    "    )\n",
    "    print(f'  After 5 epochs   - loss: {emb_ae.nn_loss:.4f}')\n",
    "\n",
    "    # Continue training for 25 more epochs\n",
    "    emb_ae.continue_learning(25, lr=1e-3, verbose=False)\n",
    "    print(f'  After 25 more    - loss: {emb_ae.nn_loss:.4f}')\n",
    "\n",
    "    # Fine-tune with lower learning rate\n",
    "    emb_ae.continue_learning(20, lr=1e-4, verbose=False)\n",
    "    print(f'  After 20 fine-tune - loss: {emb_ae.nn_loss:.4f}')\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. Beta-VAE\n",
    "    # ------------------------------------------------------------------\n",
    "    print('\\n[3] Beta-VAE (beta=4.0)')\n",
    "    print('-' * 40)\n",
    "    emb_vae = mvdata_ae.get_embedding(\n",
    "        method='flexible_ae',\n",
    "        dim=2,\n",
    "        architecture='vae',\n",
    "        inter_dim=64,\n",
    "        epochs=150,\n",
    "        lr=1e-3,\n",
    "        feature_dropout=0.1,\n",
    "        loss_components=[\n",
    "            {'name': 'reconstruction', 'weight': 1.0, 'loss_type': 'mse'},\n",
    "            {'name': 'beta_vae', 'weight': 1.0, 'beta': 4.0},\n",
    "        ],\n",
    "        verbose=False,\n",
    "    )\n",
    "    print(f'  Embedding shape: {emb_vae.coords.shape}')\n",
    "    print(f'  Test loss (reconstruction + KL): {emb_vae.nn_loss:.4f}')\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. PCA baseline\n",
    "    # ------------------------------------------------------------------\n",
    "    print('\\n[4] PCA baseline')\n",
    "    print('-' * 40)\n",
    "    emb_pca_ae = mvdata_ae.get_embedding(method='pca', dim=2)\n",
    "    print(f'  Embedding shape: {emb_pca_ae.coords.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29472ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TORCH:\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. Side-by-side visualization\n",
    "    # ------------------------------------------------------------------\n",
    "    print('\\n[5] Creating comparison plot')\n",
    "    print('-' * 40)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    embeddings_ae = [\n",
    "        (emb_pca_ae, 'PCA'),\n",
    "        (emb_ae, 'AE (5 + 25 + 20 epochs)'),\n",
    "        (emb_vae, 'Beta-VAE'),\n",
    "    ]\n",
    "\n",
    "    for ax, (emb, title) in zip(axes, embeddings_ae):\n",
    "        coords = emb.coords  # (dim, n_samples)\n",
    "        sc = ax.scatter(\n",
    "            coords[0], coords[1], c=color_ae, cmap='hsv',\n",
    "            s=2, alpha=0.5, vmin=0, vmax=2 * np.pi\n",
    "        )\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Dim 1')\n",
    "        ax.set_ylabel('Dim 2')\n",
    "\n",
    "    fig.colorbar(sc, ax=axes[-1], label='Head direction (rad)')\n",
    "    plt.suptitle('Circular manifold recovery (colored by head direction)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Autoencoder DR example complete')\n",
    "    print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f0e1c",
   "metadata": {},
   "source": [
    "## 5. Circular manifold & dimensionality estimation\n",
    "\n",
    "Head direction cells encode a ring. We generate 100 HD cells with\n",
    "[`generate_circular_manifold_exp`](https://driada.readthedocs.io/en/latest/api/experiment/synthetic.html), estimate\n",
    "the intrinsic dimensionality ([`pca_dimension`](https://driada.readthedocs.io/en/latest/api/dimensionality/linear.html) scree,\n",
    "[`correlation_dimension`](https://driada.readthedocs.io/en/latest/api/dimensionality/intrinsic.html),\n",
    "[`geodesic_dimension`](https://driada.readthedocs.io/en/latest/api/dimensionality/intrinsic.html),\n",
    "[`eff_dim`](https://driada.readthedocs.io/en/latest/api/dimensionality/effective.html) participation ratio), compare real vs\n",
    "shuffled data, and extract the circular manifold via DR\n",
    "([`visualize_circular_manifold`](https://driada.readthedocs.io/en/latest/api/utils/visualization.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b57855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dimensionality(neural_data, methods=None, ds=1):\n",
    "    \"\"\"Estimate intrinsic dimensionality using multiple DRIADA methods.\"\"\"\n",
    "    if methods is None:\n",
    "        methods = [\n",
    "            'pca_90', 'pca_95', 'participation_ratio',\n",
    "            'correlation_dim', 'geodesic_dim',\n",
    "        ]\n",
    "\n",
    "    dim_estimates = {}\n",
    "\n",
    "    # Downsample data if requested\n",
    "    if ds > 1:\n",
    "        neural_data_ds = neural_data[:, ::ds]\n",
    "        print(f'  Downsampled: {neural_data.shape} -> {neural_data_ds.shape}')\n",
    "    else:\n",
    "        neural_data_ds = neural_data\n",
    "\n",
    "    # Transpose data for methods that expect (n_samples, n_features)\n",
    "    data_transposed = neural_data_ds.T\n",
    "\n",
    "    # Linear methods\n",
    "    if 'pca_90' in methods:\n",
    "        dim_estimates['pca_90'] = pca_dimension(data_transposed, threshold=0.90)\n",
    "    if 'pca_95' in methods:\n",
    "        dim_estimates['pca_95'] = pca_dimension(data_transposed, threshold=0.95)\n",
    "\n",
    "    # Nonlinear intrinsic methods\n",
    "    if 'correlation_dim' in methods:\n",
    "        try:\n",
    "            print('  Computing correlation dimension...')\n",
    "            dim_estimates['correlation_dim'] = correlation_dimension(data_transposed)\n",
    "        except Exception as e:\n",
    "            print(f'  Warning: correlation_dimension failed: {e}')\n",
    "            dim_estimates['correlation_dim'] = np.nan\n",
    "\n",
    "    if 'geodesic_dim' in methods:\n",
    "        try:\n",
    "            print('  Computing geodesic dimension (this may take time)...')\n",
    "            dim_estimates['geodesic_dim'] = geodesic_dimension(\n",
    "                data_transposed, k=20, mode='fast', factor=4\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f'  Warning: geodesic_dimension failed: {e}')\n",
    "            dim_estimates['geodesic_dim'] = np.nan\n",
    "\n",
    "    # Effective dimensionality (participation ratio)\n",
    "    if 'participation_ratio' in methods:\n",
    "        dim_estimates['participation_ratio'] = eff_dim(\n",
    "            neural_data_ds.T, enable_correction=False, q=2\n",
    "        )\n",
    "\n",
    "    return dim_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53657b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('CIRCULAR MANIFOLD EXTRACTION FROM HEAD DIRECTION CELLS')\n",
    "print('=' * 70)\n",
    "\n",
    "print('\\n1. Generating head direction cell population...')\n",
    "\n",
    "# Generate synthetic head direction cells\n",
    "exp_circ, info_circ = generate_circular_manifold_exp(\n",
    "    n_neurons=100,\n",
    "    duration=600,  # 10 minutes\n",
    "    kappa=4.0,     # Tuning width\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    "    return_info=True,\n",
    ")\n",
    "\n",
    "# Extract neural activity and true head directions\n",
    "neural_data_circ = exp_circ.calcium.scdata  # Shape: (n_neurons, n_timepoints)\n",
    "true_angles = info_circ['head_direction']   # Ground truth angles\n",
    "\n",
    "print(f'\\nGenerated {neural_data_circ.shape[0]} neurons, '\n",
    "      f'{neural_data_circ.shape[1]} timepoints')\n",
    "print(f'Neural activity shape: {neural_data_circ.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e84768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate intrinsic dimensionality\n",
    "print('\\n2. Estimating intrinsic dimensionality of neural population...')\n",
    "print('-' * 50)\n",
    "\n",
    "dim_methods = [\n",
    "    'pca_90', 'pca_95', 'participation_ratio',\n",
    "    'correlation_dim', 'geodesic_dim',\n",
    "]\n",
    "\n",
    "# Use ds=5 downsampling for faster computation\n",
    "dim_estimates = estimate_dimensionality(\n",
    "    neural_data_circ, methods=dim_methods, ds=5\n",
    ")\n",
    "\n",
    "print('Dimensionality estimates:')\n",
    "for method, estimate in dim_estimates.items():\n",
    "    print(f'  {method:20s}: {estimate:.2f}')\n",
    "\n",
    "print('\\nNote: Head direction cells should have intrinsic dimensionality ~ 1')\n",
    "print('      (circular manifold), but finite sampling may increase estimates')\n",
    "\n",
    "# Compare with temporally shuffled data to demonstrate manifold structure\n",
    "print('\\n2b. Comparing with temporally shuffled data (destroys manifold)...')\n",
    "print('-' * 50)\n",
    "\n",
    "# Get shuffled calcium data from experiment\n",
    "shuffled_calcium = exp_circ.get_multicell_shuffled_calcium()\n",
    "\n",
    "# Estimate dimensionality on shuffled data\n",
    "dim_estimates_shuffled = estimate_dimensionality(\n",
    "    shuffled_calcium, methods=dim_methods, ds=5\n",
    ")\n",
    "\n",
    "print('\\nDimensionality estimates (SHUFFLED data):')\n",
    "for method, estimate in dim_estimates_shuffled.items():\n",
    "    print(f'  {method:20s}: {estimate:.2f}')\n",
    "\n",
    "print('\\nComparison (Real vs Shuffled):')\n",
    "print(f'{\"Method\":<20s} {\"Real\":>8s} {\"Shuffled\":>8s} {\"Increase\":>10s}')\n",
    "print('-' * 50)\n",
    "for method in dim_methods:\n",
    "    real = dim_estimates[method]\n",
    "    shuffled = dim_estimates_shuffled[method]\n",
    "    increase = ((shuffled - real) / real) * 100\n",
    "    print(f'{method:<20s} {real:8.2f} {shuffled:8.2f} {increase:+9.1f}%')\n",
    "\n",
    "print('\\nInterpretation: Temporal shuffling destroys the circular manifold structure,')\n",
    "print('                dramatically increasing dimensionality.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79095c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot eigenvalue spectrum\n",
    "print('\\n3. Plotting eigenvalue spectrum...')\n",
    "\n",
    "# Compute correlation matrix\n",
    "data_centered = neural_data_circ - np.mean(neural_data_circ, axis=1, keepdims=True)\n",
    "corr_mat = np.corrcoef(data_centered)\n",
    "\n",
    "# Get eigenvalues\n",
    "eigenvalues = np.linalg.eigvalsh(corr_mat)[::-1]  # Descending order\n",
    "eigenvalues = eigenvalues[eigenvalues > 0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Eigenvalue spectrum\n",
    "ax1.plot(eigenvalues, 'o-', markersize=4)\n",
    "ax1.set_xlabel('Component')\n",
    "ax1.set_ylabel('Eigenvalue')\n",
    "ax1.set_title('Eigenvalue spectrum')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance explained\n",
    "cumvar = np.cumsum(eigenvalues) / np.sum(eigenvalues)\n",
    "ax2.plot(cumvar, 'o-', markersize=4)\n",
    "ax2.axhline(0.9, color='r', linestyle='--', label='90% variance')\n",
    "ax2.axhline(0.95, color='orange', linestyle='--', label='95% variance')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Variance Explained')\n",
    "ax2.set_title('Cumulative variance explained')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57574078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction using MVData\n",
    "print('\\n4. Applying dimensionality reduction methods using MVData...')\n",
    "print('-' * 50)\n",
    "\n",
    "# Create MVData object from calcium data with downsampling\n",
    "downsampling_circ = 10\n",
    "mvdata_circ = MVData(neural_data_circ, downsampling=downsampling_circ)\n",
    "\n",
    "# Downsample true angles to match\n",
    "true_angles_ds = true_angles[::downsampling_circ]\n",
    "\n",
    "# Dictionary to store embeddings\n",
    "embeddings_dict_circ = {}\n",
    "\n",
    "# PCA\n",
    "print('- PCA...')\n",
    "pca_embedding_circ = mvdata_circ.get_embedding(method='pca', dim=2)\n",
    "embeddings_dict_circ['PCA'] = pca_embedding_circ.coords.T\n",
    "print(\n",
    "    f'  First 2 PCs explain '\n",
    "    f'{100 * sum(pca_embedding_circ.reducer_.explained_variance_ratio_):.1f}% of variance'\n",
    ")\n",
    "\n",
    "# Isomap\n",
    "print('- Isomap...')\n",
    "isomap_embedding_circ = mvdata_circ.get_embedding(\n",
    "    method='isomap', dim=2, n_neighbors=50\n",
    ")\n",
    "embeddings_dict_circ['Isomap'] = isomap_embedding_circ.coords.T\n",
    "\n",
    "# UMAP with increased parameters for better global structure\n",
    "print('- UMAP...')\n",
    "umap_embedding_circ = mvdata_circ.get_embedding(\n",
    "    method='umap', n_components=2, n_neighbors=100, min_dist=0.5\n",
    ")\n",
    "embeddings_dict_circ['UMAP'] = umap_embedding_circ.coords.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize extracted manifolds\n",
    "print('\\n5. Visualizing extracted manifolds...')\n",
    "\n",
    "# Create embedding comparison visualization\n",
    "embeddings_list_circ = [\n",
    "    embeddings_dict_circ[m] for m in ['PCA', 'Isomap', 'UMAP']\n",
    "]\n",
    "fig_embedding = visualize_circular_manifold(\n",
    "    embeddings_list_circ, true_angles_ds, ['PCA', 'Isomap', 'UMAP']\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Trajectory visualization\n",
    "print('\\n6. Analyzing temporal continuity of extracted manifolds...')\n",
    "\n",
    "# Use only first 1000 timepoints for trajectory visualization\n",
    "traj_len = min(1000, embeddings_dict_circ['PCA'].shape[0])\n",
    "trajectories_dict = {\n",
    "    method: emb[:traj_len] for method, emb in embeddings_dict_circ.items()\n",
    "}\n",
    "\n",
    "fig3 = plot_trajectories(\n",
    "    embeddings=trajectories_dict,\n",
    "    trajectory_kwargs={'arrow_spacing': 50, 'linewidth': 0.5, 'alpha': 0.5},\n",
    "    figsize=(15, 5),\n",
    "    dpi=DEFAULT_DPI,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65954f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print('\\n7. Summary of manifold extraction quality:')\n",
    "print('-' * 60)\n",
    "print(f'{\"Method\":10s} | {\"Correlation\":12s} | {\"Mean Error\":12s} | {\"Quality\":8s}')\n",
    "print('-' * 60)\n",
    "\n",
    "for method, embedding in embeddings_dict_circ.items():\n",
    "    # Use manifold metrics API\n",
    "    alignment_metrics = compute_embedding_alignment_metrics(\n",
    "        embedding, true_angles_ds, 'circular'\n",
    "    )\n",
    "    r = alignment_metrics['correlation']\n",
    "    error = alignment_metrics['error']\n",
    "\n",
    "    # Quality assessment\n",
    "    if abs(r) > 0.95:\n",
    "        quality_str = 'Excellent'\n",
    "    elif abs(r) > 0.85:\n",
    "        quality_str = 'Good'\n",
    "    elif abs(r) > 0.70:\n",
    "        quality_str = 'Fair'\n",
    "    else:\n",
    "        quality_str = 'Poor'\n",
    "\n",
    "    print(f'{method:10s} | {r:12.3f} | {error:9.3f} rad | {quality_str:8s}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('CONCLUSIONS:')\n",
    "print('- Head direction cells have low intrinsic dimensionality (~1-2)')\n",
    "print('- Temporal shuffling destroys manifold structure (dimensionality increases)')\n",
    "print('- Nonlinear methods (Isomap, UMAP) better preserve circular topology')\n",
    "print('- PCA captures variance but may distort circular structure')\n",
    "print('- Higher n_neighbors helps preserve global structure')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e060946",
   "metadata": {},
   "source": [
    "## 6. INTENSE-guided DR\n",
    "\n",
    "Use [`compute_cell_feat_significance`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html) (INTENSE) to identify spatially selective neurons\n",
    "from a [`generate_mixed_population_exp`](https://driada.readthedocs.io/en/latest/api/experiment/synthetic.html) dataset, then compare DR\n",
    "quality on **all neurons** vs **selective neurons only** vs a **random\n",
    "subset**. Selective neurons produce cleaner spatial embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a761339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "\n",
    "def compute_spatial_correspondence_metrics(embedding, true_positions):\n",
    "    \"\"\"\n",
    "    Compute spatial-specific metrics for evaluating embedding quality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : ndarray, shape (n_samples, n_dims)\n",
    "        Low-dimensional embedding\n",
    "    true_positions : ndarray, shape (n_samples, n_spatial_dims)\n",
    "        True spatial positions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dictionary containing spatial correspondence metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # 1. SPATIAL DECODING ACCURACY\n",
    "    decoding_metrics = compute_spatial_decoding_accuracy(\n",
    "        embedding.T,\n",
    "        true_positions,\n",
    "        test_size=0.5,\n",
    "        n_estimators=20,\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=50,\n",
    "        random_state=42,\n",
    "    )\n",
    "    metrics.update(decoding_metrics)\n",
    "\n",
    "    # 2. SPATIAL INFORMATION CONTENT\n",
    "    mi_metrics = compute_spatial_information(\n",
    "        embedding.T, true_positions\n",
    "    )\n",
    "    metrics.update(mi_metrics)\n",
    "\n",
    "    # 3. DISTANCE CORRELATION\n",
    "    try:\n",
    "        dist_embed = pdist(embedding)\n",
    "        dist_true = pdist(true_positions)\n",
    "        metrics['distance_correlation'] = np.corrcoef(dist_embed, dist_true)[0, 1]\n",
    "    except:\n",
    "        metrics['distance_correlation'] = 0.0\n",
    "\n",
    "    # 4. PROCRUSTES ANALYSIS\n",
    "    try:\n",
    "        embedding_2d = embedding[:, :2] if embedding.shape[1] >= 2 else embedding\n",
    "        _, disparity, _ = procrustes_analysis(\n",
    "            true_positions, embedding_2d, scaling=True, reflection=True\n",
    "        )\n",
    "        metrics['procrustes_disparity'] = disparity\n",
    "    except:\n",
    "        metrics['procrustes_disparity'] = 1.0\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2580dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('INTENSE-Guided Dimensionality Reduction for Spatial Data')\n",
    "print('=' * 70)\n",
    "\n",
    "# 1. Generate mixed population data\n",
    "print('\\n1. Generating mixed population with spatial and non-spatial neurons...')\n",
    "\n",
    "n_neurons_intense = 50   # Minimal for notebook execution speed\n",
    "duration_intense = 300   # 5 minutes\n",
    "n_shuffles_1 = 100       # FFT makes shuffle count cheap\n",
    "n_shuffles_2 = 5000      # Better statistics with minimal overhead\n",
    "ds_intense = 5\n",
    "\n",
    "exp_intense = generate_mixed_population_exp(\n",
    "    n_neurons=n_neurons_intense,\n",
    "    manifold_type='2d_spatial',\n",
    "    manifold_fraction=0.5,  # 1/2 place cells, 1/2 feature cells\n",
    "    n_discrete_features=3,\n",
    "    n_continuous_features=3,\n",
    "    duration=duration_intense,\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f'  Created experiment with {exp_intense.n_cells} neurons, '\n",
    "      f'{exp_intense.n_frames} timepoints')\n",
    "print(f'  Available features: {list(exp_intense.dynamic_features.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run INTENSE with position_2d MultiTimeSeries\n",
    "print('\\n2. Running INTENSE analysis on 2D position (MultiTimeSeries)...')\n",
    "stats_i, significance_i, info_i, results_i = compute_cell_feat_significance(\n",
    "    exp_intense,\n",
    "    feat_bunch=['position_2d'],  # Using MultiTimeSeries only\n",
    "    find_optimal_delays=False,\n",
    "    mode='two_stage',\n",
    "    n_shuffles_stage1=n_shuffles_1,\n",
    "    n_shuffles_stage2=n_shuffles_2,\n",
    "    ds=ds_intense,\n",
    "    pval_thr=0.01,\n",
    "    multicomp_correction=None,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Categorize neurons by selectivity\n",
    "print('\\n3. Categorizing neurons by selectivity...')\n",
    "\n",
    "# Get neurons selective to spatial position\n",
    "sig_neurons_2d = list(exp_intense.get_significant_neurons(fbunch='position_2d').keys())\n",
    "spatial_neurons_i = sig_neurons_2d\n",
    "\n",
    "print(f'  Spatial neurons (position_2d): {len(sig_neurons_2d)}')\n",
    "print(f'  Non-spatial neurons: {exp_intense.n_cells - len(spatial_neurons_i)}')\n",
    "\n",
    "# Check if we have enough spatial neurons\n",
    "if len(spatial_neurons_i) < 5:\n",
    "    print('\\nWARNING: Not enough spatial neurons detected!')\n",
    "    print('Try running with more neurons or adjusting parameters.')\n",
    "\n",
    "# Extract true positions\n",
    "position_2d_i = exp_intense.dynamic_features['position_2d'].data\n",
    "x_pos_i = position_2d_i[0, :]\n",
    "y_pos_i = position_2d_i[1, :]\n",
    "true_positions_i = np.column_stack([x_pos_i, y_pos_i])\n",
    "\n",
    "# Downsample positions to match calcium data\n",
    "if ds_intense > 1:\n",
    "    true_positions_i = true_positions_i[::ds_intense]\n",
    "\n",
    "# VERIFICATION: Check ground truth vs detected spatial neurons\n",
    "print('\\n[CHECK] Analyzing ground truth vs detected spatial neurons...')\n",
    "\n",
    "# Get ground truth spatial neurons (first 50% are spatial by construction)\n",
    "n_true_spatial = int(exp_intense.n_cells * 0.5)\n",
    "true_spatial_neurons = list(range(n_true_spatial))\n",
    "true_nonspatial_neurons = list(range(n_true_spatial, exp_intense.n_cells))\n",
    "\n",
    "print(f'  Ground truth spatial neurons: {n_true_spatial} '\n",
    "      f'(indices 0-{n_true_spatial - 1})')\n",
    "print(f'  Ground truth non-spatial neurons: '\n",
    "      f'{exp_intense.n_cells - n_true_spatial} '\n",
    "      f'(indices {n_true_spatial}-{exp_intense.n_cells - 1})')\n",
    "\n",
    "# Check detection accuracy\n",
    "detected_spatial_set = set(spatial_neurons_i)\n",
    "true_spatial_set = set(true_spatial_neurons)\n",
    "true_nonspatial_set = set(true_nonspatial_neurons)\n",
    "\n",
    "true_positives = detected_spatial_set & true_spatial_set\n",
    "false_positives = detected_spatial_set & true_nonspatial_set\n",
    "false_negatives = true_spatial_set - detected_spatial_set\n",
    "\n",
    "print(f'  True positives (correctly detected spatial): {len(true_positives)}')\n",
    "print(f'  False positives (non-spatial detected as spatial): {len(false_positives)}')\n",
    "print(f'  False negatives (spatial missed): {len(false_negatives)}')\n",
    "\n",
    "precision_i = (\n",
    "    len(true_positives) / len(detected_spatial_set) if detected_spatial_set else 0\n",
    ")\n",
    "recall_i = len(true_positives) / len(true_spatial_set) if true_spatial_set else 0\n",
    "f1_i = (\n",
    "    2 * precision_i * recall_i / (precision_i + recall_i)\n",
    "    if (precision_i + recall_i) > 0 else 0\n",
    ")\n",
    "\n",
    "print(f'  Detection Precision: {precision_i:.3f}')\n",
    "print(f'  Detection Recall: {recall_i:.3f}')\n",
    "print(f'  Detection F1-score: {f1_i:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f62771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create scenarios to demonstrate benefit\n",
    "print('\\n4. Creating test scenarios...')\n",
    "\n",
    "# Get all neurons\n",
    "calcium_all_i = exp_intense.calcium.scdata[:, ::ds_intense]\n",
    "\n",
    "# Get spatial neurons (detected by INTENSE)\n",
    "calcium_spatial_i = exp_intense.calcium.scdata[spatial_neurons_i, ::ds_intense]\n",
    "\n",
    "# Get non-selective neurons\n",
    "all_neurons_i = set(range(exp_intense.n_cells))\n",
    "selective_neurons_i = set(spatial_neurons_i)\n",
    "for feat in ['d_feat_0', 'd_feat_1', 'd_feat_2',\n",
    "             'c_feat_0', 'c_feat_1', 'c_feat_2']:\n",
    "    try:\n",
    "        feat_neurons = exp_intense.get_significant_neurons(fbunch=feat)\n",
    "        if feat_neurons:\n",
    "            selective_neurons_i.update(feat_neurons.keys())\n",
    "    except:\n",
    "        pass\n",
    "non_selective_neurons_i = list(all_neurons_i - selective_neurons_i)\n",
    "calcium_non_selective_i = (\n",
    "    exp_intense.calcium.scdata[non_selective_neurons_i, ::ds_intense]\n",
    "    if non_selective_neurons_i else None\n",
    ")\n",
    "\n",
    "# Get random half of all neurons\n",
    "np.random.seed(42)\n",
    "random_half_idx_i = np.random.choice(\n",
    "    exp_intense.n_cells, size=exp_intense.n_cells // 2, replace=False\n",
    ")\n",
    "calcium_random_half_i = exp_intense.calcium.scdata[random_half_idx_i, ::ds_intense]\n",
    "\n",
    "print(f'  All neurons: {calcium_all_i.shape[0]} neurons')\n",
    "print(f'  Spatial neurons (INTENSE): {calcium_spatial_i.shape[0]} neurons')\n",
    "print(f'  Random half: {calcium_random_half_i.shape[0]} neurons')\n",
    "print(f'  Non-selective neurons: {len(non_selective_neurons_i)} neurons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define DR methods and scenarios\n",
    "print('\\n5. Applying dimensionality reduction methods...')\n",
    "\n",
    "dr_methods_i = {\n",
    "    'PCA': {'method': 'pca', 'params': {'dim': 2}},\n",
    "    'Isomap': {'method': 'isomap', 'params': {'dim': 2, 'n_neighbors': 30}},\n",
    "    'UMAP': {\n",
    "        'method': 'umap',\n",
    "        'params': {\n",
    "            'dim': 2, 'n_neighbors': 80,\n",
    "            'min_dist': 0.8, 'random_state': 42,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "results_intense = {}\n",
    "\n",
    "scenarios_i = [\n",
    "    ('All neurons', calcium_all_i),\n",
    "    ('Spatial neurons', calcium_spatial_i),\n",
    "    ('Random half', calcium_random_half_i),\n",
    "    ('Non-selective', calcium_non_selective_i),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e717f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DR methods on each scenario and compute spatial metrics\n",
    "for method_name, method_config in dr_methods_i.items():\n",
    "    print(f'\\n  {method_name}:')\n",
    "    results_intense[method_name] = {}\n",
    "\n",
    "    for scenario_name, calcium_data in scenarios_i:\n",
    "        if calcium_data is None:\n",
    "            print(f'    - {scenario_name}: No neurons in this category, skipping')\n",
    "            continue\n",
    "        if calcium_data.shape[0] < 10:\n",
    "            print(f'    - {scenario_name}: Too few neurons ({calcium_data.shape[0]}), skipping')\n",
    "            continue\n",
    "\n",
    "        print(f'    - {scenario_name}...')\n",
    "\n",
    "        try:\n",
    "            mvdata_i = MVData(calcium_data)\n",
    "\n",
    "            # Adjust n_neighbors for smaller datasets\n",
    "            params = method_config['params'].copy()\n",
    "            if 'n_neighbors' in params:\n",
    "                params['n_neighbors'] = min(\n",
    "                    params['n_neighbors'], calcium_data.shape[1] // 10\n",
    "                )\n",
    "\n",
    "            embedding_obj = mvdata_i.get_embedding(\n",
    "                method=method_config['method'], **params\n",
    "            )\n",
    "            embedding_i = embedding_obj.coords.T\n",
    "\n",
    "            metrics_i = compute_spatial_correspondence_metrics(\n",
    "                embedding_i, true_positions_i\n",
    "            )\n",
    "\n",
    "            results_intense[method_name][scenario_name] = {\n",
    "                'embedding': embedding_i,\n",
    "                'metrics': metrics_i,\n",
    "            }\n",
    "\n",
    "            print(\n",
    "                f\"      Spatial decoding R^2: {metrics_i['r2_avg']:.3f}, \"\n",
    "                f\"Distance corr: {metrics_i['distance_correlation']:.3f}, \"\n",
    "                f\"MI: {metrics_i['mi_total']:.3f}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'      Failed: {e}')\n",
    "            results_intense[method_name][scenario_name] = None\n",
    "\n",
    "    # Calculate improvements\n",
    "    if (\n",
    "        'All neurons' in results_intense[method_name]\n",
    "        and 'Spatial neurons' in results_intense[method_name]\n",
    "    ):\n",
    "        if (\n",
    "            results_intense[method_name]['All neurons']\n",
    "            and results_intense[method_name]['Spatial neurons']\n",
    "        ):\n",
    "            r2_all_i = results_intense[method_name]['All neurons']['metrics']['r2_avg']\n",
    "            r2_spatial_i = results_intense[method_name]['Spatial neurons']['metrics']['r2_avg']\n",
    "            improvement_i = (r2_spatial_i / max(r2_all_i, 0.001) - 1) * 100\n",
    "            print(f'    Spatial vs All improvement: {improvement_i:+.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cca589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize results\n",
    "print('\\n6. Creating visualizations...')\n",
    "\n",
    "# Prepare embeddings for grid plot\n",
    "grid_embeddings = {}\n",
    "grid_metrics = {}\n",
    "for method_name in dr_methods_i.keys():\n",
    "    grid_embeddings[method_name] = {}\n",
    "    grid_metrics[method_name] = {}\n",
    "    for scenario in ['All neurons', 'Spatial neurons', 'Random half', 'Non-selective']:\n",
    "        if scenario in results_intense[method_name] and results_intense[method_name][scenario]:\n",
    "            grid_embeddings[method_name][scenario] = (\n",
    "                results_intense[method_name][scenario]['embedding']\n",
    "            )\n",
    "            grid_metrics[method_name][scenario] = {\n",
    "                'R^2': results_intense[method_name][scenario]['metrics']['r2_avg']\n",
    "            }\n",
    "\n",
    "labels_i = np.arange(len(true_positions_i))\n",
    "\n",
    "fig1 = plot_embeddings_grid(\n",
    "    embeddings=grid_embeddings,\n",
    "    labels=labels_i,\n",
    "    metrics=grid_metrics,\n",
    "    colormap='viridis',\n",
    "    figsize=(18, 12),\n",
    "    n_cols=4,\n",
    "    dpi=DEFAULT_DPI,\n",
    ")\n",
    "\n",
    "fig1.suptitle(\n",
    "    'INTENSE-Guided DR: Benefit of Spatial Neuron Selection', fontsize=14\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Create neuron selectivity summary\n",
    "selectivity_counts = {\n",
    "    'Spatial\\n(any)': len(spatial_neurons_i),\n",
    "    'Non-spatial': exp_intense.n_cells - len(spatial_neurons_i),\n",
    "}\n",
    "\n",
    "fig_summary = plot_neuron_selectivity_summary(\n",
    "    selectivity_counts=selectivity_counts,\n",
    "    total_neurons=exp_intense.n_cells,\n",
    "    figsize=(8, 6),\n",
    "    dpi=DEFAULT_DPI,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5841644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Quality metrics comparison figure\n",
    "print('\\n7. Creating quality metrics comparison...')\n",
    "\n",
    "fig2, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig2.suptitle('Dimensionality reduction quality metrics comparison', fontsize=16)\n",
    "\n",
    "metrics_to_show = [\n",
    "    ('r2_avg', 'Spatial Decoding R^2'),\n",
    "    ('distance_correlation', 'Distance Correlation'),\n",
    "    ('mi_total', 'Spatial Information (MI)'),\n",
    "    ('procrustes_disparity', 'Procrustes Disparity'),\n",
    "]\n",
    "\n",
    "scenarios_to_show = [\n",
    "    ('All neurons', 'All'),\n",
    "    ('Spatial neurons', 'Spatial'),\n",
    "    ('Random half', 'Random'),\n",
    "    ('Non-selective', 'Non-sel'),\n",
    "]\n",
    "\n",
    "for idx, (metric_key, metric_title) in enumerate(metrics_to_show):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    method_names = list(dr_methods_i.keys())\n",
    "\n",
    "    x = np.arange(len(scenarios_to_show))\n",
    "    width = 0.25\n",
    "\n",
    "    for i, method in enumerate(method_names):\n",
    "        values = []\n",
    "        for scenario, _ in scenarios_to_show:\n",
    "            if scenario in results_intense[method] and results_intense[method][scenario]:\n",
    "                value = results_intense[method][scenario]['metrics'][metric_key]\n",
    "                values.append(value)\n",
    "            else:\n",
    "                values.append(0)\n",
    "\n",
    "        offset = (i - len(method_names) / 2 + 0.5) * width\n",
    "        bars = ax.bar(x + offset, values, width, label=method, alpha=0.8)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            if value != 0:\n",
    "                height = bar.get_height()\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2.0, height,\n",
    "                    f'{value:.2f}', ha='center', va='bottom', fontsize=8,\n",
    "                )\n",
    "\n",
    "    ax.set_ylabel(metric_title)\n",
    "    ax.set_title(metric_title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(\n",
    "        [label for _, label in scenarios_to_show], rotation=45, ha='right'\n",
    "    )\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if 'r2' in metric_key:\n",
    "        ax.set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f07a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Summary statistics\n",
    "print('\\n' + '=' * 70)\n",
    "print('SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "print('\\nBest performing method for spatial reconstruction:')\n",
    "best_method_i = None\n",
    "best_score_i = -1\n",
    "for method_name in results_intense.keys():\n",
    "    if (\n",
    "        'Spatial neurons' in results_intense[method_name]\n",
    "        and results_intense[method_name]['Spatial neurons']\n",
    "    ):\n",
    "        score = results_intense[method_name]['Spatial neurons']['metrics']['r2_avg']\n",
    "        if score > best_score_i:\n",
    "            best_score_i = score\n",
    "            best_method_i = method_name\n",
    "\n",
    "if best_method_i:\n",
    "    print(f'  {best_method_i} with spatial neurons')\n",
    "    print(f'  Spatial decoding R^2: {best_score_i:.3f}')\n",
    "\n",
    "print('\\nSpatial decoding R^2 comparison:')\n",
    "for method_name in results_intense.keys():\n",
    "    print(f'\\n  {method_name}:')\n",
    "    scenarios_order = [\n",
    "        'All neurons', 'Spatial neurons', 'Random half', 'Non-selective',\n",
    "    ]\n",
    "    for scenario in scenarios_order:\n",
    "        if scenario in results_intense[method_name] and results_intense[method_name][scenario]:\n",
    "            r2 = results_intense[method_name][scenario]['metrics']['r2_avg']\n",
    "            print(f'    {scenario:20s}: {r2:.3f}')\n",
    "\n",
    "    # Calculate key comparisons\n",
    "    if (\n",
    "        'All neurons' in results_intense[method_name]\n",
    "        and results_intense[method_name]['All neurons']\n",
    "    ):\n",
    "        r2_all_s = results_intense[method_name]['All neurons']['metrics']['r2_avg']\n",
    "\n",
    "        if (\n",
    "            'Spatial neurons' in results_intense[method_name]\n",
    "            and results_intense[method_name]['Spatial neurons']\n",
    "        ):\n",
    "            r2_spatial_s = results_intense[method_name]['Spatial neurons']['metrics']['r2_avg']\n",
    "            imp_s = (r2_spatial_s / max(r2_all_s, 0.001) - 1) * 100\n",
    "            print(f'    -> Spatial vs All improvement: {imp_s:+.1f}%')\n",
    "\n",
    "        if (\n",
    "            'Random half' in results_intense[method_name]\n",
    "            and results_intense[method_name]['Random half']\n",
    "        ):\n",
    "            r2_random_s = results_intense[method_name]['Random half']['metrics']['r2_avg']\n",
    "            ratio_s = r2_random_s / max(r2_all_s, 0.001)\n",
    "            print(f'    -> Random half / All ratio: {ratio_s:.2f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
