{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95ceb86",
   "metadata": {},
   "source": [
    "# Advanced capabilities\n",
    "\n",
    "The previous notebooks introduced the building blocks separately:\n",
    "selectivity (02), manifolds (03), and networks (04).  This notebook\n",
    "shows how they combine -- and how\n",
    "[**DRIADA**](https://driada.readthedocs.io) extends beyond calcium\n",
    "imaging to any system that produces time-varying population activity.\n",
    "\n",
    "| Step | Notebook | What it does |\n",
    "|---|---|---|\n",
    "| Load & inspect | [01 -- Data loading](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/01_data_loading_and_neurons.ipynb) | Wrap your recording into an `Experiment`, reconstruct spikes, assess quality |\n",
    "| Single-neuron selectivity | [02 -- INTENSE](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/02_selectivity_detection_intense.ipynb) | Detect which neurons encode which behavioral variables |\n",
    "| Population geometry | [03 -- Dimensionality reduction](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/03_population_geometry_dr.ipynb) | Extract low-dimensional manifolds from population activity |\n",
    "| Network analysis | [04 -- Networks](https://colab.research.google.com/github/iabs-neuro/driada/blob/main/notebooks/04_network_analysis.ipynb) | Build and analyze cell-cell interaction graphs |\n",
    "| **Putting it together** | **05 -- this notebook** | Combine INTENSE + DR, leave-one-out importance, RSA, RNN analysis |\n",
    "\n",
    "**Sections:**\n",
    "\n",
    "1. **Embedding selectivity (DR -> INTENSE)** -- Reverse the usual\n",
    "   direction: run INTENSE on embedding components to discover what each\n",
    "   DR dimension encodes. Identifies functional clusters in the\n",
    "   population geometry.\n",
    "2. **Leave-one-out neuron importance** -- Remove each neuron and\n",
    "   measure manifold degradation. Validates whether neurons important\n",
    "   for the embedding are the same ones identified by INTENSE.\n",
    "3. **Representational similarity analysis (RSA)** -- Compare\n",
    "   population-level representations across regions, sessions, or\n",
    "   conditions using Representational Dissimilarity Matrices (RDMs).\n",
    "4. **Beyond calcium: DRIADA on RNN activations** -- Full pipeline on\n",
    "   simulated RNN units: INTENSE + DR + network analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: revert to '!pip install -q driada' after v1.0.0 PyPI release\n",
    "!pip install -q git+https://github.com/iabs-neuro/driada.git@main\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78887532",
   "metadata": {},
   "source": [
    "## 1. Embedding selectivity (DR -> INTENSE)\n",
    "\n",
    "Notebooks 2--3 used INTENSE to select neurons for DR. Here we reverse\n",
    "the direction: create an embedding first (PCA, UMAP), then treat each\n",
    "component as a *feature* and run INTENSE on it. This answers: **which\n",
    "neurons drive which embedding dimensions?**\n",
    "\n",
    "Key APIs:\n",
    "- [`compute_embedding_selectivity`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html#driada.intense.pipelines.compute_embedding_selectivity) -- INTENSE on embedding components.\n",
    "  Internally, each embedding component is added as a temporary dynamic\n",
    "  feature and tested via standard INTENSE, so parameters like `n_shuffles`\n",
    "  and `pval_thr` pass through to [`compute_cell_feat_significance`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html#driada.intense.pipelines.compute_cell_feat_significance).\n",
    "- [`get_functional_organization`](https://driada.readthedocs.io/en/latest/api/integration.html#driada.integration.manifold_analysis.get_functional_organization) -- cluster and participation analysis\n",
    "- [`compare_embeddings`](https://driada.readthedocs.io/en/latest/api/integration.html#driada.integration.manifold_analysis.compare_embeddings) -- cross-method comparison\n",
    "\n",
    "The synthetic data below is created with\n",
    "[`generate_tuned_selectivity_exp`](https://driada.readthedocs.io/en/latest/api/experiment/synthetic.html#driada.experiment.synthetic.generators.generate_tuned_selectivity_exp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from driada.experiment.synthetic import generate_tuned_selectivity_exp\n",
    "from driada.intense import compute_embedding_selectivity\n",
    "from driada.integration import get_functional_organization, compare_embeddings\n",
    "from driada.utils.visual import plot_embedding_comparison\n",
    "\n",
    "print('1. Generating synthetic population...')\n",
    "\n",
    "population = [\n",
    "    {'name': 'hd_cells', 'count': 12,\n",
    "     'features': ['head_direction']},\n",
    "    {'name': 'place_cells', 'count': 10,\n",
    "     'features': ['position_2d']},\n",
    "    {'name': 'speed_cells', 'count': 8,\n",
    "     'features': ['speed']},\n",
    "    {'name': 'conjunctive', 'count': 5,\n",
    "     'features': ['head_direction', 'speed'], 'combination': 'and'},\n",
    "    {'name': 'non_selective', 'count': 15,\n",
    "     'features': []},\n",
    "]\n",
    "\n",
    "exp_emb = generate_tuned_selectivity_exp(\n",
    "    population, duration=300, fps=20, seed=42, verbose=True\n",
    ")\n",
    "\n",
    "gt_groups = {}\n",
    "idx = 0\n",
    "for group in population:\n",
    "    for _ in range(group['count']):\n",
    "        gt_groups[idx] = group['name']\n",
    "        idx += 1\n",
    "\n",
    "print(f'  {exp_emb.n_cells} neurons, {exp_emb.n_frames} frames')\n",
    "for group in population:\n",
    "    print(f'    {group[\"name\"]:20s}: {group[\"count\"]} neurons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(14, 5))\n",
    "\n",
    "n_show = min(5, exp_emb.n_cells)\n",
    "time_sec = np.arange(exp_emb.n_frames) / exp_emb.fps\n",
    "\n",
    "ax = axes[0]\n",
    "for i in range(n_show):\n",
    "    ax.plot(time_sec, exp_emb.calcium.data[i], linewidth=0.6, label=f'neuron {i}')\n",
    "ax.set_ylabel('dF/F0')\n",
    "ax.set_title(f'Synthetic neural traces ({exp_emb.n_cells} neurons)')\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.imshow(exp_emb.calcium.data, aspect='auto', cmap='hot', interpolation='none')\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Neuron')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n2. Creating embeddings...')\n",
    "\n",
    "n_pca_components = 4\n",
    "pca_emb = exp_emb.create_embedding('pca', n_components=n_pca_components, ds=5)\n",
    "print(f'  PCA: {pca_emb.shape} (n_frames, n_components)')\n",
    "\n",
    "umap_emb = exp_emb.create_embedding(\n",
    "    'umap', n_components=3, n_neighbors=50, min_dist=0.8, random_state=42, ds=5\n",
    ")\n",
    "print(f'  UMAP: {umap_emb.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e059f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n3. Computing embedding selectivity (INTENSE on components)...')\n",
    "\n",
    "results_emb = compute_embedding_selectivity(\n",
    "    exp_emb,\n",
    "    embedding_methods=['pca', 'umap'],\n",
    "    mode='two_stage',\n",
    "    n_shuffles_stage1=50,\n",
    "    n_shuffles_stage2=1000,\n",
    "    find_optimal_delays=False,\n",
    "    pval_thr=0.01,\n",
    "    ds=5,\n",
    "    verbose=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "n_total = exp_emb.n_cells\n",
    "for method in ['pca', 'umap']:\n",
    "    r = results_emb[method]\n",
    "    n_sig = len(r['significant_neurons'])\n",
    "    print(f'\\n  {method.upper()} summary:')\n",
    "    print(f'    {n_sig}/{n_total} neurons significantly selective '\n",
    "          f'({100 * n_sig / n_total:.0f}%)')\n",
    "    for comp_idx in range(r['n_components']):\n",
    "        n_sel = len(r['component_selectivity'][comp_idx])\n",
    "        if n_sel > 0:\n",
    "            print(f'    component {comp_idx}: {n_sel} selective neurons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n4. Functional organization (PCA)...')\n",
    "\n",
    "org = get_functional_organization(\n",
    "    exp_emb, 'pca', intense_results=results_emb['pca']['intense_results']\n",
    ")\n",
    "\n",
    "print('\\n  Component importance (variance explained):')\n",
    "for i, imp in enumerate(org['component_importance']):\n",
    "    print(f'    component {i}: {imp:.3f}')\n",
    "\n",
    "print(f'\\n  Participating neurons: {org[\"n_participating_neurons\"]}/{n_total}')\n",
    "print(f'  Mean components per neuron: {org[\"mean_components_per_neuron\"]:.2f}')\n",
    "\n",
    "print('\\n  Component specialization:')\n",
    "for comp_idx, spec in org['component_specialization'].items():\n",
    "    n_sel = spec['n_selective_neurons']\n",
    "    rate = spec['selectivity_rate']\n",
    "    if n_sel > 0:\n",
    "        group_counts = {}\n",
    "        for nid in spec['selective_neurons']:\n",
    "            g = gt_groups.get(nid, 'unknown')\n",
    "            group_counts[g] = group_counts.get(g, 0) + 1\n",
    "        groups_str = ', '.join(f'{g}={c}' for g, c in sorted(group_counts.items()))\n",
    "        print(f'    component {comp_idx}: {n_sel} neurons ({rate:.0%}) -- {groups_str}')\n",
    "\n",
    "print(f'\\n  Functional clusters: {len(org[\"functional_clusters\"])}')\n",
    "for i, cluster in enumerate(org['functional_clusters']):\n",
    "    comps = cluster['components']\n",
    "    size = cluster['size']\n",
    "    group_counts = {}\n",
    "    for nid in cluster['neurons']:\n",
    "        g = gt_groups.get(nid, 'unknown')\n",
    "        group_counts[g] = group_counts.get(g, 0) + 1\n",
    "    groups_str = ', '.join(f'{g}={c}' for g, c in sorted(group_counts.items()))\n",
    "    print(f'    cluster {i}: components {comps}, {size} neurons -- {groups_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n5. Comparing PCA vs UMAP functional organization...')\n",
    "\n",
    "intense_dict = {\n",
    "    m: results_emb[m]['intense_results'] for m in ['pca', 'umap']\n",
    "}\n",
    "comparison = compare_embeddings(\n",
    "    exp_emb, ['pca', 'umap'], intense_results_dict=intense_dict\n",
    ")\n",
    "\n",
    "for method in comparison['methods']:\n",
    "    n_part = comparison['n_participating_neurons'][method]\n",
    "    mean_comp = comparison['mean_components_per_neuron'][method]\n",
    "    n_clust = comparison['n_functional_clusters'][method]\n",
    "    print(f'  {method.upper():6s}: {n_part} participating neurons, '\n",
    "          f'{mean_comp:.2f} mean components, {n_clust} clusters')\n",
    "\n",
    "if 'participation_overlap' in comparison:\n",
    "    for pair, overlap in comparison['participation_overlap'].items():\n",
    "        print(f'  Participation overlap ({pair}): {overlap:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f82c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Functional organization analysis', fontsize=14)\n",
    "\n",
    "# (a) Component importance\n",
    "ax = axes[0, 0]\n",
    "comp_imp = org['component_importance']\n",
    "ax.bar(range(len(comp_imp)), comp_imp, color='steelblue', edgecolor='white')\n",
    "ax.set_xlabel('PCA component')\n",
    "ax.set_ylabel('Variance explained (fraction)')\n",
    "ax.set_title('Component importance')\n",
    "ax.set_xticks(range(len(comp_imp)))\n",
    "\n",
    "# (b) Component specialization by neuron group\n",
    "ax = axes[0, 1]\n",
    "group_names = [g['name'] for g in population]\n",
    "group_colors = plt.cm.Set2(np.linspace(0, 1, len(group_names)))\n",
    "color_map = dict(zip(group_names, group_colors))\n",
    "\n",
    "comp_indices = sorted(org['component_specialization'].keys())\n",
    "bottom = np.zeros(len(comp_indices))\n",
    "for gname in group_names:\n",
    "    counts = []\n",
    "    for comp_idx in comp_indices:\n",
    "        spec = org['component_specialization'][comp_idx]\n",
    "        c = sum(1 for nid in spec['selective_neurons']\n",
    "                if gt_groups.get(nid) == gname)\n",
    "        counts.append(c)\n",
    "    ax.bar(comp_indices, counts, bottom=bottom,\n",
    "           label=gname, color=color_map[gname], edgecolor='white')\n",
    "    bottom += counts\n",
    "ax.set_xlabel('PCA component')\n",
    "ax.set_ylabel('Selective neurons')\n",
    "ax.set_title('Component specialization by group')\n",
    "ax.legend(fontsize=7, loc='upper right')\n",
    "ax.set_xticks(comp_indices)\n",
    "\n",
    "# (c) Neuron participation histogram\n",
    "ax = axes[1, 0]\n",
    "participation = org.get('neuron_participation', {})\n",
    "if participation:\n",
    "    n_comps_per_neuron = [len(comps) for comps in participation.values()]\n",
    "    max_comps = max(n_comps_per_neuron) if n_comps_per_neuron else 1\n",
    "    bins = np.arange(0.5, max_comps + 1.5, 1)\n",
    "    ax.hist(n_comps_per_neuron, bins=bins, color='steelblue',\n",
    "            edgecolor='white', rwidth=0.8)\n",
    "    ax.set_xlabel('Number of components')\n",
    "    ax.set_ylabel('Number of neurons')\n",
    "    ax.set_title('Neuron participation distribution')\n",
    "    ax.set_xticks(range(1, max_comps + 1))\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No participating neurons',\n",
    "            ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title('Neuron participation distribution')\n",
    "\n",
    "# (d) PCA vs UMAP comparison\n",
    "ax = axes[1, 1]\n",
    "methods = comparison['methods']\n",
    "n_parts = [comparison['n_participating_neurons'][m] for m in methods]\n",
    "n_clusts = [comparison['n_functional_clusters'][m] for m in methods]\n",
    "x = np.arange(len(methods))\n",
    "w = 0.35\n",
    "ax.bar(x - w / 2, n_parts, w, label='Participating neurons',\n",
    "       color='steelblue', edgecolor='white')\n",
    "ax.bar(x + w / 2, n_clusts, w, label='Functional clusters',\n",
    "       color='coral', edgecolor='white')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.upper() for m in methods])\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('PCA vs UMAP comparison')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "if 'participation_overlap' in comparison:\n",
    "    for pair, overlap in comparison['participation_overlap'].items():\n",
    "        ax.annotate(f'Overlap: {overlap:.2f}',\n",
    "                    xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "                    ha='center', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca974910",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_emb = 5  # must match ds used in create_embedding\n",
    "hd = exp_emb.dynamic_features['head_direction'].data[::ds_emb]\n",
    "spd = exp_emb.dynamic_features['speed'].data[::ds_emb]\n",
    "\n",
    "fig_cmp = plot_embedding_comparison(\n",
    "    embeddings={'PCA': pca_emb[:, :2], 'UMAP': umap_emb[:, :2]},\n",
    "    features={'head_direction': hd, 'speed': spd},\n",
    "    with_trajectory=False,\n",
    "    compute_metrics=False,\n",
    "    scatter_size=8,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065817cf",
   "metadata": {},
   "source": [
    "## 2. Leave-one-out neuron importance\n",
    "\n",
    "Remove each neuron from the population and measure how much the manifold\n",
    "degrades. Neurons with high INTENSE MI should also be most important for\n",
    "the embedding -- this validates INTENSE results from a DR perspective.\n",
    "\n",
    "The analysis uses ground truth reconstruction metrics: alignment\n",
    "correlation, decoding accuracy, and reconstruction error (comparing the\n",
    "embedding against known head direction angles). INTENSE selectivity is\n",
    "computed with\n",
    "[`compute_cell_feat_significance`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html#driada.intense.pipelines.compute_cell_feat_significance)\n",
    "for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad4897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from driada.experiment.synthetic import generate_tuned_selectivity_exp\n",
    "from driada.dim_reduction import MVData\n",
    "from driada.dim_reduction.manifold_metrics import (\n",
    "    compute_reconstruction_error,\n",
    "    compute_decoding_accuracy,\n",
    "    compute_embedding_alignment_metrics,\n",
    "    circular_structure_preservation,\n",
    ")\n",
    "from driada.intense import compute_cell_feat_significance\n",
    "from driada.utils.visual import visualize_circular_manifold\n",
    "\n",
    "print('[1/4] Generating mixed population experiment...')\n",
    "population_loo = [\n",
    "    {'name': 'hd_broad', 'count': 15, 'features': ['head_direction'],\n",
    "     'tuning_params': {'kappa': 2.0}},\n",
    "    {'name': 'hd_medium', 'count': 15, 'features': ['head_direction'],\n",
    "     'tuning_params': {'kappa': 5.0}},\n",
    "    {'name': 'hd_sharp', 'count': 15, 'features': ['head_direction'],\n",
    "     'tuning_params': {'kappa': 10.0}},\n",
    "    {'name': 'nonselective', 'count': 15, 'features': []},\n",
    "]\n",
    "exp_loo = generate_tuned_selectivity_exp(\n",
    "    population=population_loo, duration=600, seed=42\n",
    ")\n",
    "print(f'  Created: {exp_loo.n_cells} neurons, {exp_loo.calcium.data.shape[1]} timepoints')\n",
    "group_desc = ' + '.join(\n",
    "    f'{g[\"count\"]} {g[\"name\"]}' for g in population_loo\n",
    ")\n",
    "print(f'  Population: {group_desc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_method = 'isomap'\n",
    "dr_params = {'dim': 2, 'nn': 20, 'max_deleted_nodes': 0.3}\n",
    "ds_loo = 5\n",
    "\n",
    "print(f'\\n[2/4] Running LOO-DR analysis with {dr_method}...')\n",
    "\n",
    "neural_data_loo = exp_loo.calcium.scdata  # (n_neurons, n_timepoints)\n",
    "n_neurons_loo = neural_data_loo.shape[0]\n",
    "\n",
    "# Ground truth: head direction angles\n",
    "ground_truth_loo = exp_loo.dynamic_features['head_direction'].data[::ds_loo]\n",
    "\n",
    "\n",
    "def _compute_loo_embedding(data_matrix, method, params, downsampling):\n",
    "    \"\"\"Compute embedding from a neuron x timepoints matrix.\"\"\"\n",
    "    mv = MVData(data_matrix, downsampling=downsampling)\n",
    "    emb = mv.get_embedding(method=method, **params)\n",
    "    coords = emb.coords.T  # (n_samples, n_dims)\n",
    "    # Handle lost nodes\n",
    "    gt = ground_truth_loo.copy()\n",
    "    if hasattr(emb, 'graph') and hasattr(emb.graph, 'lost_nodes'):\n",
    "        lost = set(emb.graph.lost_nodes)\n",
    "        if lost:\n",
    "            surviving = [i for i in range(len(gt)) if i not in lost]\n",
    "            gt = gt[surviving]\n",
    "    return coords, gt\n",
    "\n",
    "\n",
    "def _compute_metrics(coords, gt):\n",
    "    \"\"\"Compute ground truth reconstruction metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    try:\n",
    "        result = compute_reconstruction_error(coords, gt, manifold_type='circular')\n",
    "        metrics['reconstruction_error'] = result['error'] if isinstance(result, dict) else result\n",
    "    except Exception:\n",
    "        metrics['reconstruction_error'] = np.nan\n",
    "    try:\n",
    "        result = compute_decoding_accuracy(coords, gt, manifold_type='circular')\n",
    "        metrics['decoding_accuracy'] = result['test_r2']\n",
    "    except Exception:\n",
    "        metrics['decoding_accuracy'] = np.nan\n",
    "    try:\n",
    "        result = compute_embedding_alignment_metrics(coords, gt, manifold_type='circular')\n",
    "        metrics['alignment_corr'] = result['correlation']\n",
    "    except Exception:\n",
    "        metrics['alignment_corr'] = np.nan\n",
    "    return metrics\n",
    "\n",
    "\n",
    "print('  Computing baseline...')\n",
    "baseline_coords, baseline_gt = _compute_loo_embedding(\n",
    "    neural_data_loo, dr_method, dr_params, ds_loo\n",
    ")\n",
    "baseline_metrics = _compute_metrics(baseline_coords, baseline_gt)\n",
    "\n",
    "loo_metric_rows = [{'neuron': 'all', **baseline_metrics}]\n",
    "print(f'  LOO analysis for {n_neurons_loo} neurons...')\n",
    "for nidx in tqdm(range(n_neurons_loo), desc=f'LOO {dr_method}'):\n",
    "    mask = np.ones(n_neurons_loo, dtype=bool)\n",
    "    mask[nidx] = False\n",
    "    try:\n",
    "        coords_i, gt_i = _compute_loo_embedding(\n",
    "            neural_data_loo[mask], dr_method, dr_params, ds_loo\n",
    "        )\n",
    "        m = _compute_metrics(coords_i, gt_i)\n",
    "    except Exception:\n",
    "        m = {k: np.nan for k in ['reconstruction_error', 'decoding_accuracy', 'alignment_corr']}\n",
    "    loo_metric_rows.append({'neuron': nidx, **m})\n",
    "\n",
    "loo_results = pd.DataFrame(loo_metric_rows).set_index('neuron')\n",
    "\n",
    "baseline_row = loo_results.loc['all']\n",
    "importance_scores = []\n",
    "for nidx in range(n_neurons_loo):\n",
    "    row = loo_results.loc[nidx]\n",
    "    if row.isna().all():\n",
    "        importance_scores.append(np.nan)\n",
    "        continue\n",
    "    # Higher error when removed = more important (flip sign)\n",
    "    error_deg = -(baseline_row['reconstruction_error'] - row['reconstruction_error']) / (baseline_row['reconstruction_error'] + 1e-10)\n",
    "    align_deg = (baseline_row['alignment_corr'] - row['alignment_corr']) / (baseline_row['alignment_corr'] + 1e-10)\n",
    "    decode_deg = (baseline_row['decoding_accuracy'] - row['decoding_accuracy']) / (baseline_row['decoding_accuracy'] + 1e-10)\n",
    "    importance_scores.append((error_deg + align_deg + decode_deg) / 3)\n",
    "\n",
    "importance = pd.Series(importance_scores, index=range(n_neurons_loo), name='importance')\n",
    "\n",
    "print(f'\\nLOO-DR Results:')\n",
    "print(f'  Baseline metrics:')\n",
    "print(f'    alignment_corr:      {baseline_row[\"alignment_corr\"]:.4f}')\n",
    "print(f'    decoding_accuracy:   {baseline_row[\"decoding_accuracy\"]:.4f}')\n",
    "print(f'    reconstruction_error: {baseline_row[\"reconstruction_error\"]:.4f}')\n",
    "\n",
    "if not importance.isna().all():\n",
    "    print(f'\\n  Top 5 most important neurons:')\n",
    "    for neuron, score in importance.nlargest(5).items():\n",
    "        row = loo_results.loc[neuron]\n",
    "        align_delta = baseline_row['alignment_corr'] - row['alignment_corr']\n",
    "        decode_delta = baseline_row['decoding_accuracy'] - row['decoding_accuracy']\n",
    "        error_delta = row['reconstruction_error'] - baseline_row['reconstruction_error']\n",
    "        print(f'    Neuron {neuron}: importance={score:.4f}')\n",
    "        print(f'      align: {align_delta:+.4f}, decode: {decode_delta:+.4f}, '\n",
    "              f'error: {error_delta:+.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  Verifying circular structure...')\n",
    "ds = 5\n",
    "mvdata_vis = MVData(exp_loo.calcium.data, downsampling=ds)\n",
    "baseline_emb = mvdata_vis.get_embedding(method=dr_method, **dr_params)\n",
    "baseline_coords = baseline_emb.coords.T  # (n_samples, 2)\n",
    "ground_truth_full = exp_loo.dynamic_features['head_direction'].data[::ds]\n",
    "\n",
    "# Handle lost nodes if any\n",
    "if hasattr(baseline_emb, 'graph') and hasattr(baseline_emb.graph, 'lost_nodes'):\n",
    "    lost = set(baseline_emb.graph.lost_nodes)\n",
    "    surviving = [i for i in range(len(ground_truth_full)) if i not in lost]\n",
    "    ground_truth_aligned = ground_truth_full[surviving]\n",
    "    print(f'    Note: {len(lost)} nodes lost in graph construction')\n",
    "else:\n",
    "    ground_truth_aligned = ground_truth_full\n",
    "\n",
    "circular_metrics = circular_structure_preservation(\n",
    "    baseline_coords, true_angles=ground_truth_aligned, k_neighbors=3\n",
    ")\n",
    "print(f'  Circular structure verification:')\n",
    "print(f'    Distance CV: {circular_metrics[\"distance_cv\"]:.3f} (lower = more circular)')\n",
    "print(f'    Consecutive preservation: {circular_metrics[\"consecutive_preservation\"]:.1%}')\n",
    "print(f'    Circular correlation: {circular_metrics[\"circular_correlation\"]:.3f}')\n",
    "\n",
    "fig = visualize_circular_manifold(\n",
    "    [baseline_coords], ground_truth_aligned, [dr_method.upper()]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0101c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[3/4] Running INTENSE analysis...')\n",
    "stats_loo, significant_loo, info_loo, intense_res_loo = compute_cell_feat_significance(\n",
    "    exp_loo,\n",
    "    feat_bunch=['head_direction_2d'],\n",
    "    n_shuffles_stage1=100,\n",
    "    n_shuffles_stage2=5000,\n",
    "    find_optimal_delays=True,\n",
    "    ds=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[4/4] Comparing LOO importance with INTENSE selectivity...')\n",
    "\n",
    "intense_mi_values = np.full(n_neurons_loo, np.nan)\n",
    "intense_pval_values = np.full(n_neurons_loo, np.nan)\n",
    "for nid in range(n_neurons_loo):\n",
    "    if nid in stats_loo and 'head_direction_2d' in stats_loo[nid]:\n",
    "        intense_mi_values[nid] = stats_loo[nid]['head_direction_2d'].get('me', np.nan)\n",
    "        intense_pval_values[nid] = stats_loo[nid]['head_direction_2d'].get('pval', np.nan)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'loo_importance': importance.values,\n",
    "    'intense_mi': intense_mi_values,\n",
    "    'intense_pval': intense_pval_values,\n",
    "}, index=range(n_neurons_loo))\n",
    "\n",
    "valid_data = combined.dropna(subset=['loo_importance', 'intense_mi'])\n",
    "\n",
    "if len(valid_data) >= 5:\n",
    "    corr, pval_corr = spearmanr(valid_data['loo_importance'], valid_data['intense_mi'])\n",
    "    print(f'\\n' + '=' * 70)\n",
    "    print('KEY RESULT: Correlation between LOO importance and INTENSE selectivity')\n",
    "    print('=' * 70)\n",
    "    print(f'  Spearman correlation: r = {corr:.3f}')\n",
    "    print(f'  p-value: {pval_corr:.3e}')\n",
    "    if pval_corr < 0.05 and corr > 0:\n",
    "        print('  -> Significant POSITIVE correlation: neurons important for')\n",
    "        print('     manifold reconstruction ARE the ones selective for head_direction')\n",
    "    elif pval_corr < 0.05:\n",
    "        print('  -> Significant NEGATIVE correlation (unexpected)')\n",
    "    else:\n",
    "        print('  -> No significant correlation found')\n",
    "\n",
    "    print(f'\\n  Top 5 neurons by INTENSE MI vs their LOO importance:')\n",
    "    top_intense = valid_data.nlargest(5, 'intense_mi')\n",
    "    for idx, row in top_intense.iterrows():\n",
    "        print(f'    Neuron {idx}: MI={row[\"intense_mi\"]:.3f} bits, '\n",
    "              f'LOO={row[\"loo_importance\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ef84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# (a) Scatter plot: LOO importance vs INTENSE MI\n",
    "ax = axes[0]\n",
    "valid = combined.dropna(subset=['loo_importance', 'intense_mi'])\n",
    "ax.scatter(valid['intense_mi'], valid['loo_importance'], alpha=0.6, s=30)\n",
    "ax.set_xlabel('INTENSE MI (bits)')\n",
    "ax.set_ylabel('LOO importance')\n",
    "ax.set_title(f'LOO importance vs INTENSE MI\\nr={corr:.3f}, p={pval_corr:.2e}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (b) Importance ranking\n",
    "ax = axes[1]\n",
    "sorted_imp = importance.dropna().sort_values(ascending=False)\n",
    "colors = ['steelblue' if i < 45 else 'lightcoral' for i in sorted_imp.index]\n",
    "ax.bar(range(len(sorted_imp)), sorted_imp.values, color=colors, width=1.0)\n",
    "ax.set_xlabel('Neuron (sorted by importance)')\n",
    "ax.set_ylabel('LOO importance')\n",
    "ax.set_title('Neuron importance ranking')\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "ax.legend(handles=[\n",
    "    Patch(color='steelblue', label='HD neurons (0-44)'),\n",
    "    Patch(color='lightcoral', label='Non-selective (45-59)'),\n",
    "], fontsize=8)\n",
    "\n",
    "# (c) INTENSE MI distribution by group\n",
    "ax = axes[2]\n",
    "mi_values = combined['intense_mi'].values\n",
    "hd_mi = mi_values[:45]\n",
    "non_mi = mi_values[45:]\n",
    "ax.boxplot([hd_mi[~np.isnan(hd_mi)], non_mi[~np.isnan(non_mi)]],\n",
    "           labels=['HD neurons', 'Non-selective'])\n",
    "ax.set_ylabel('INTENSE MI (bits)')\n",
    "ax.set_title('MI distribution by group')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b692fd",
   "metadata": {},
   "source": [
    "## 3. Representational similarity analysis (RSA)\n",
    "\n",
    "Compare population-level representations across regions, sessions, or\n",
    "conditions using **Representational Dissimilarity Matrices** (RDMs).\n",
    "An RDM captures pairwise dissimilarity between stimulus conditions,\n",
    "abstracting away neuron identity and number.\n",
    "\n",
    "DRIADA's `rsa` module provides:\n",
    "- [`compute_rdm_unified`](https://driada.readthedocs.io/en/latest/api/rsa/core.html#driada.rsa.core.compute_rdm_unified) -- RDM from neural data + condition labels\n",
    "- [`rsa_compare`](https://driada.readthedocs.io/en/latest/api/rsa/core.html#driada.rsa.core.rsa_compare) -- Compare two populations directly\n",
    "- `compare_rdms` -- Correlate two pre-computed RDMs\n",
    "- [`bootstrap_rdm_comparison`](https://driada.readthedocs.io/en/latest/api/rsa/core.html#driada.rsa.core.bootstrap_rdm_comparison) -- Statistical significance via bootstrap\n",
    "- `plot_rdm`, `plot_rdm_comparison` -- Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fa47b",
   "metadata": {},
   "source": [
    "### 3.1 RDM from stimulus-selective populations\n",
    "\n",
    "We create a population with **two stimulus categories**: stimuli A & B\n",
    "share neurons (Category A), stimuli C & D share neurons (Category B).\n",
    "Cross-category pairs share no neurons. The RDM should reveal this 2x2\n",
    "block structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from driada import rsa\n",
    "from driada.experiment.synthetic import generate_tuned_selectivity_exp\n",
    "\n",
    "\n",
    "def create_stimulus_labels_from_events(exp, event_names):\n",
    "    \"\"\"Convert binary event features to categorical stimulus labels.\n",
    "\n",
    "    Timepoints with no event or multiple simultaneous events are labeled -1.\n",
    "    \"\"\"\n",
    "    n_timepoints = exp.calcium.data.shape[1]\n",
    "    labels = np.full(n_timepoints, -1, dtype=int)\n",
    "\n",
    "    event_count = np.zeros(n_timepoints, dtype=int)\n",
    "    for event_name in event_names:\n",
    "        event_data = exp.dynamic_features[event_name].data\n",
    "        event_count += (event_data > 0).astype(int)\n",
    "\n",
    "    for idx, event_name in enumerate(event_names):\n",
    "        event_data = exp.dynamic_features[event_name].data\n",
    "        single_event = (event_data > 0) & (event_count == 1)\n",
    "        labels[single_event] = idx\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "population_rsa = [\n",
    "    {'name': 'cat_a_shared', 'count': 20,\n",
    "     'features': ['event_0', 'event_1'], 'combination': 'or'},\n",
    "    {'name': 'event_0_specific', 'count': 15, 'features': ['event_0']},\n",
    "    {'name': 'event_1_specific', 'count': 15, 'features': ['event_1']},\n",
    "    {'name': 'cat_b_shared', 'count': 20,\n",
    "     'features': ['event_2', 'event_3'], 'combination': 'or'},\n",
    "    {'name': 'event_2_specific', 'count': 15, 'features': ['event_2']},\n",
    "    {'name': 'event_3_specific', 'count': 15, 'features': ['event_3']},\n",
    "]\n",
    "\n",
    "print('Generating stimulus-selective neurons (100 neurons, 4 conditions)...')\n",
    "exp_rsa = generate_tuned_selectivity_exp(\n",
    "    population=population_rsa,\n",
    "    n_discrete_features=4,\n",
    "    duration=600,\n",
    "    event_active_fraction=0.08,\n",
    "    event_avg_duration=1.0,\n",
    "    baseline_rate=0.05,\n",
    "    peak_rate=2.0,\n",
    "    seed=42,\n",
    "    verbose=False,\n",
    "    reconstruct_spikes='threshold',\n",
    ")\n",
    "\n",
    "print('Computing RDM from spike patterns...')\n",
    "stimulus_labels = create_stimulus_labels_from_events(\n",
    "    exp_rsa, ['event_0', 'event_1', 'event_2', 'event_3']\n",
    ")\n",
    "\n",
    "valid_mask = stimulus_labels >= 0\n",
    "rdm1, labels1 = rsa.compute_rdm_unified(\n",
    "    exp_rsa.spikes.data[:, valid_mask],\n",
    "    items=stimulus_labels[valid_mask],\n",
    "    metric='euclidean',\n",
    ")\n",
    "\n",
    "print(f'RDM shape: {rdm1.shape}')\n",
    "print(f'Stimulus conditions: {labels1}')\n",
    "\n",
    "label_names = ['Stim A', 'Stim B', 'Stim C', 'Stim D']\n",
    "fig = rsa.plot_rdm(\n",
    "    rdm1, labels=label_names[:len(labels1)],\n",
    "    title='Neural RDM - stimulus conditions', show_values=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cef5fe",
   "metadata": {},
   "source": [
    "### 3.2 Comparing representations between regions\n",
    "\n",
    "Generate two \"regions\" with partially shared tuning and compare their\n",
    "representations directly using [`rsa_compare`](https://driada.readthedocs.io/en/latest/api/rsa/core.html#driada.rsa.core.rsa_compare) (no pre-computed RDMs\n",
    "needed). Try multiple distance metrics and comparison methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_items = 20\n",
    "n_neurons_v1 = 100\n",
    "n_neurons_v2 = 150\n",
    "\n",
    "# Create base patterns that both regions respond to\n",
    "base_patterns = np.random.randn(n_items, 50)\n",
    "\n",
    "# V1: Direct representation with noise\n",
    "v1_data = base_patterns @ np.random.randn(50, n_neurons_v1)\n",
    "v1_data += 0.2 * np.random.randn(n_items, n_neurons_v1)\n",
    "\n",
    "# V2: Transformed representation with noise\n",
    "transform = np.random.randn(50, 50)\n",
    "v2_data = (base_patterns @ transform) @ np.random.randn(50, n_neurons_v2)\n",
    "v2_data += 0.2 * np.random.randn(n_items, n_neurons_v2)\n",
    "\n",
    "print('Comparing V1 and V2 representations...')\n",
    "similarity = rsa.rsa_compare(v1_data, v2_data)\n",
    "print(f'V1-V2 similarity (Spearman): {similarity:.3f}')\n",
    "\n",
    "print('\\nDifferent distance metrics:')\n",
    "for metric in ['correlation', 'euclidean', 'cosine']:\n",
    "    sim = rsa.rsa_compare(v1_data, v2_data, metric=metric)\n",
    "    print(f'  {metric}: {sim:.3f}')\n",
    "\n",
    "print('\\nDifferent comparison methods:')\n",
    "for comparison in ['spearman', 'pearson', 'kendall']:\n",
    "    sim = rsa.rsa_compare(v1_data, v2_data, comparison=comparison)\n",
    "    print(f'  {comparison}: {sim:.3f}')\n",
    "\n",
    "rdm_v1 = rsa.compute_rdm(v1_data)\n",
    "rdm_v2 = rsa.compute_rdm(v2_data)\n",
    "\n",
    "fig = rsa.plot_rdm_comparison(\n",
    "    [rdm_v1, rdm_v2], titles=['V1 representation', 'V2 representation']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5ce44",
   "metadata": {},
   "source": [
    "### 3.3 Cross-session comparison & bootstrap testing\n",
    "\n",
    "Compare the same population structure recorded in two sessions (different\n",
    "noise). Bootstrap significance testing quantifies whether the RDM\n",
    "correlation is reliably above chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e69756",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_sessions = [\n",
    "    {'name': 'cat_a_shared', 'count': 14,\n",
    "     'features': ['event_0', 'event_1'], 'combination': 'or'},\n",
    "    {'name': 'event_0_only', 'count': 10, 'features': ['event_0']},\n",
    "    {'name': 'event_1_only', 'count': 10, 'features': ['event_1']},\n",
    "    {'name': 'cat_b_shared', 'count': 14,\n",
    "     'features': ['event_2', 'event_3'], 'combination': 'or'},\n",
    "    {'name': 'event_2_only', 'count': 10, 'features': ['event_2']},\n",
    "    {'name': 'event_3_only', 'count': 10, 'features': ['event_3']},\n",
    "    {'name': 'cat_c_shared', 'count': 12,\n",
    "     'features': ['event_4', 'event_5'], 'combination': 'or'},\n",
    "    {'name': 'event_4_only', 'count': 10, 'features': ['event_4']},\n",
    "    {'name': 'event_5_only', 'count': 10, 'features': ['event_5']},\n",
    "]\n",
    "\n",
    "event_names_6 = ['event_0', 'event_1', 'event_2', 'event_3', 'event_4', 'event_5']\n",
    "\n",
    "print('Generating session 1 (100 neurons, 6 conditions, 3 categories)...')\n",
    "exp_s1 = generate_tuned_selectivity_exp(\n",
    "    population=population_sessions, n_discrete_features=6, duration=600,\n",
    "    event_active_fraction=0.08, event_avg_duration=1.0,\n",
    "    baseline_rate=0.05, peak_rate=2.0,\n",
    "    seed=42, verbose=False, reconstruct_spikes='threshold',\n",
    ")\n",
    "\n",
    "print('Generating session 2 (same structure, different noise)...')\n",
    "exp_s2 = generate_tuned_selectivity_exp(\n",
    "    population=population_sessions, n_discrete_features=6, duration=600,\n",
    "    event_active_fraction=0.08, event_avg_duration=1.0,\n",
    "    baseline_rate=0.05, peak_rate=2.0,\n",
    "    seed=123, verbose=False, reconstruct_spikes='threshold',\n",
    ")\n",
    "\n",
    "stim_labels_1 = create_stimulus_labels_from_events(exp_s1, event_names_6)\n",
    "stim_labels_2 = create_stimulus_labels_from_events(exp_s2, event_names_6)\n",
    "\n",
    "valid_1 = stim_labels_1 >= 0\n",
    "valid_2 = stim_labels_2 >= 0\n",
    "\n",
    "rdm_s1, labels_s1 = rsa.compute_rdm_unified(\n",
    "    exp_s1.spikes.data[:, valid_1], items=stim_labels_1[valid_1],\n",
    "    metric='euclidean',\n",
    ")\n",
    "rdm_s2, labels_s2 = rsa.compute_rdm_unified(\n",
    "    exp_s2.spikes.data[:, valid_2], items=stim_labels_2[valid_2],\n",
    "    metric='euclidean',\n",
    ")\n",
    "\n",
    "similarity_sessions = rsa.compare_rdms(rdm_s1, rdm_s2, method='spearman')\n",
    "print(f'Cross-session RDM similarity: {similarity_sessions:.3f}')\n",
    "\n",
    "label_names_6 = ['Stim A', 'Stim B', 'Stim C', 'Stim D', 'Stim E', 'Stim F']\n",
    "fig = rsa.plot_rdm_comparison(\n",
    "    [rdm_s1, rdm_s2], labels=label_names_6[:len(labels_s1)],\n",
    "    titles=['Session 1', 'Session 2'],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b620e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running bootstrap significance test (Pearson)...')\n",
    "bootstrap_results = rsa.bootstrap_rdm_comparison(\n",
    "    exp_s1.spikes.data[:, valid_1],\n",
    "    exp_s2.spikes.data[:, valid_2],\n",
    "    stim_labels_1[valid_1],\n",
    "    stim_labels_2[valid_2],\n",
    "    metric='euclidean',\n",
    "    comparison_method='pearson',\n",
    "    n_bootstrap=100,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f'Observed similarity: {bootstrap_results[\"observed\"]:.3f}')\n",
    "print(f'95% CI: [{bootstrap_results[\"ci_lower\"]:.3f}, '\n",
    "      f'{bootstrap_results[\"ci_upper\"]:.3f}]')\n",
    "print(f'Bootstrap stability p-value: {bootstrap_results[\"p_value\"]:.3f}')\n",
    "print('  (Tests if observed is extreme relative to bootstrap mean;')\n",
    "print('   ~0.5 means stable. CI above 0 confirms reliable similarity.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a9c4b",
   "metadata": {},
   "source": [
    "### 3.4 MVData integration\n",
    "\n",
    "RSA works seamlessly with [`MVData`](https://driada.readthedocs.io/en/latest/api/dim_reduction/data_structures.html#driada.dim_reduction.data.MVData) objects from the DR pipeline. Pass\n",
    "an `MVData` object directly to [`compute_rdm_unified`](https://driada.readthedocs.io/en/latest/api/rsa/core.html#driada.rsa.core.compute_rdm_unified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from driada.dim_reduction.data import MVData\n",
    "\n",
    "n_features = 100\n",
    "n_timepoints = 1000\n",
    "n_conditions = 5\n",
    "\n",
    "condition_duration = n_timepoints // n_conditions\n",
    "conditions = np.repeat(np.arange(n_conditions), condition_duration)\n",
    "\n",
    "patterns = np.random.randn(n_conditions, n_features)\n",
    "data_mvdata = np.zeros((n_features, n_timepoints))\n",
    "for i, cond in enumerate(conditions):\n",
    "    data_mvdata[:, i] = patterns[cond] + 0.1 * np.random.randn(n_features)\n",
    "\n",
    "mvdata_rsa = MVData(data_mvdata)\n",
    "\n",
    "print('Computing RDM from MVData object...')\n",
    "rdm_mv, labels_mv = rsa.compute_rdm_unified(mvdata_rsa, items=conditions)\n",
    "\n",
    "print(f'RDM shape: {rdm_mv.shape}')\n",
    "print(f'Unique conditions: {labels_mv}')\n",
    "\n",
    "fig = rsa.plot_rdm(\n",
    "    rdm_mv, labels=[f'Cond {i}' for i in labels_mv],\n",
    "    title='RDM from MVData', show_values=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e7c6d",
   "metadata": {},
   "source": [
    "## 4. Beyond calcium: DRIADA on RNN activations\n",
    "\n",
    "DRIADA is not limited to calcium imaging. Any `(n_units x n_timepoints)`\n",
    "matrix works. This section demonstrates the full pipeline on simulated\n",
    "RNN units: behavioral input generation, RNN simulation, then INTENSE +\n",
    "DR + network analysis.\n",
    "\n",
    "Data is loaded with\n",
    "[`load_exp_from_aligned_data`](https://driada.readthedocs.io/en/latest/api/experiment/loading.html#driada.experiment.exp_build.load_exp_from_aligned_data),\n",
    "selectivity tested with [`compute_cell_feat_significance`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html#driada.intense.pipelines.compute_cell_feat_significance), pairwise\n",
    "dependencies found with\n",
    "[`compute_cell_cell_significance`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html#driada.intense.pipelines.compute_cell_cell_significance),\n",
    "and the resulting adjacency matrix wrapped in a\n",
    "[`Network`](https://driada.readthedocs.io/en/latest/api/network/core.html#driada.network.net_base.Network)\n",
    "object.\n",
    "\n",
    "The `Experiment` constructor accepts multiple neural data key aliases:\n",
    "`calcium`, `activations`, `neural_data`, `activity`, `rates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7422ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "\n",
    "import driada\n",
    "from driada.dim_reduction import MVData\n",
    "from driada.experiment import load_exp_from_aligned_data\n",
    "from driada.intense import compute_cell_cell_significance\n",
    "from driada.network import Network\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'n_units': 64,\n",
    "    'tau': 0.2,\n",
    "    'g_rec': 1.2,\n",
    "    'w_in': 0.8,\n",
    "    'noise_sigma': 0.05,\n",
    "    'duration': 300,  # 5 min (shorter for notebook speed)\n",
    "    'fps': 20,\n",
    "    'tau_smooth': 1.0,\n",
    "    'obs_noise': 0.01,\n",
    "    'n_shuffles_stage1': 100,\n",
    "    'n_shuffles_stage2': 5000,\n",
    "    'pval_thr': 0.001,\n",
    "    'ds': 5,\n",
    "    'cc_n_shuffles_stage2': 5000,\n",
    "    'cc_pval_thr': 0.01,\n",
    "    'seed': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[1] GENERATING BEHAVIORAL INPUTS')\n",
    "print('-' * 40)\n",
    "\n",
    "rng = np.random.default_rng(CONFIG['seed'])\n",
    "n_frames = CONFIG['duration'] * CONFIG['fps']\n",
    "dt = 1.0 / CONFIG['fps']\n",
    "\n",
    "# Smooth random walk for position\n",
    "step_std = 0.3 * np.sqrt(dt)\n",
    "x = np.empty(n_frames)\n",
    "y = np.empty(n_frames)\n",
    "x[0], y[0] = 0.5, 0.5\n",
    "dx_raw = rng.normal(0, step_std, n_frames)\n",
    "dy_raw = rng.normal(0, step_std, n_frames)\n",
    "for t in range(1, n_frames):\n",
    "    x[t] = x[t - 1] + dx_raw[t]\n",
    "    y[t] = y[t - 1] + dy_raw[t]\n",
    "    # Reflecting boundaries\n",
    "    if x[t] < 0: x[t] = -x[t]\n",
    "    elif x[t] > 1: x[t] = 2.0 - x[t]\n",
    "    if y[t] < 0: y[t] = -y[t]\n",
    "    elif y[t] > 1: y[t] = 2.0 - y[t]\n",
    "\n",
    "# Derived features\n",
    "dx = np.diff(x, prepend=x[0])\n",
    "dy = np.diff(y, prepend=y[0])\n",
    "speed = np.sqrt(dx**2 + dy**2) * CONFIG['fps']\n",
    "head_direction = np.arctan2(dy, dx) % (2 * np.pi)\n",
    "\n",
    "# Trial type: block-structured categorical\n",
    "trial_type = np.zeros(n_frames, dtype=int)\n",
    "t = 0\n",
    "while t < n_frames:\n",
    "    label = rng.integers(0, 3)\n",
    "    block_len = max(int(rng.exponential(7.0) * CONFIG['fps']), int(CONFIG['fps']))\n",
    "    trial_type[t:t + block_len] = label\n",
    "    t += block_len\n",
    "\n",
    "# Sparse binary event\n",
    "event = (rng.random(n_frames) < 0.03).astype(float)\n",
    "\n",
    "inputs = {'x': x, 'y': y, 'speed': speed, 'head_direction': head_direction,\n",
    "          'trial_type': trial_type, 'event': event}\n",
    "print(f'  Frames: {n_frames}, features: {list(inputs.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f016d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[2] SIMULATING RNN')\n",
    "print('-' * 40)\n",
    "\n",
    "n_units = CONFIG['n_units']\n",
    "tau = CONFIG['tau']\n",
    "g = CONFIG['g_rec']\n",
    "w_in = CONFIG['w_in']\n",
    "sigma = CONFIG['noise_sigma']\n",
    "\n",
    "# Stack input channels\n",
    "input_names = ['x', 'y', 'speed', 'head_direction', 'trial_type', 'event']\n",
    "u = np.stack([inputs[k].astype(float) for k in input_names], axis=0)\n",
    "n_input = u.shape[0]\n",
    "\n",
    "# Random fixed weights\n",
    "W_rec = rng.normal(0, g / np.sqrt(n_units), (n_units, n_units))\n",
    "W_in = rng.normal(0, w_in / np.sqrt(n_input), (n_units, n_input))\n",
    "\n",
    "# Euler integration\n",
    "state = np.zeros(n_units)\n",
    "raw = np.empty((n_units, n_frames))\n",
    "for t in range(n_frames):\n",
    "    r = np.maximum(state, 0)  # ReLU\n",
    "    raw[:, t] = r\n",
    "    noise = rng.normal(0, sigma, n_units)\n",
    "    state += (dt / tau) * (-state + W_rec @ r + W_in @ u[:, t] + noise)\n",
    "\n",
    "# Exponential smoothing (mimics slow indicator dynamics)\n",
    "alpha = dt / CONFIG['tau_smooth']\n",
    "activations = np.empty_like(raw)\n",
    "activations[:, 0] = raw[:, 0]\n",
    "for t in range(1, n_frames):\n",
    "    activations[:, t] = (1 - alpha) * activations[:, t - 1] + alpha * raw[:, t]\n",
    "\n",
    "activations += rng.normal(0, CONFIG['obs_noise'], activations.shape)\n",
    "activations = np.maximum(activations, 0)\n",
    "\n",
    "mean_rate = activations.mean()\n",
    "frac_active = (activations > 0).mean()\n",
    "print(f'  Activations: {activations.shape}')\n",
    "print(f'  Mean rate: {mean_rate:.3f}, fraction active: {frac_active:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[3] LOADING INTO DRIADA')\n",
    "print('-' * 40)\n",
    "\n",
    "data_rnn = {'activations': activations, **inputs}\n",
    "\n",
    "exp_rnn = load_exp_from_aligned_data(\n",
    "    data_source='RNN',\n",
    "    exp_params={'name': 'random_rnn'},\n",
    "    data=data_rnn,\n",
    "    feature_types={'head_direction': 'circular', 'speed': 'linear'},\n",
    "    aggregate_features={('x', 'y'): 'position_2d'},\n",
    "    static_features={'fps': float(CONFIG['fps'])},\n",
    "    create_circular_2d=True,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f'  Experiment: {exp_rnn.n_cells} units, {exp_rnn.n_frames} frames')\n",
    "print(f'  Features: {list(exp_rnn.dynamic_features.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d117ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[4] INTENSE SELECTIVITY ANALYSIS')\n",
    "print('-' * 40)\n",
    "\n",
    "stats_rnn, significance_rnn, info_rnn, results_rnn = (\n",
    "    driada.compute_cell_feat_significance(\n",
    "        exp_rnn,\n",
    "        mode='two_stage',\n",
    "        n_shuffles_stage1=CONFIG['n_shuffles_stage1'],\n",
    "        n_shuffles_stage2=CONFIG['n_shuffles_stage2'],\n",
    "        pval_thr=CONFIG['pval_thr'],\n",
    "        ds=CONFIG['ds'],\n",
    "        verbose=True,\n",
    "    )\n",
    ")\n",
    "significant_neurons_rnn = exp_rnn.get_significant_neurons()\n",
    "\n",
    "# Per-feature summary\n",
    "feat_counts = {}\n",
    "for feats in significant_neurons_rnn.values():\n",
    "    for f in feats:\n",
    "        feat_counts[f] = feat_counts.get(f, 0) + 1\n",
    "n_mixed = sum(1 for feats in significant_neurons_rnn.values() if len(feats) > 1)\n",
    "\n",
    "print(f'\\n  Selective units: {len(significant_neurons_rnn)} / {exp_rnn.n_cells}')\n",
    "for feat, cnt in sorted(feat_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f'    {feat}: {cnt} units')\n",
    "print(f'  Mixed selectivity (>1 feature): {n_mixed} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8356884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[5] DIMENSIONALITY REDUCTION')\n",
    "print('-' * 40)\n",
    "\n",
    "mvdata_rnn = MVData(exp_rnn.calcium.data)\n",
    "emb_rnn = mvdata_rnn.get_embedding(method='pca')\n",
    "print(f'  PCA embedding: {emb_rnn.coords.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10647607",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[6] CELL-CELL FUNCTIONAL NETWORK')\n",
    "print('-' * 40)\n",
    "\n",
    "sim_mat_rnn, sig_mat_rnn, pval_mat_rnn, cell_ids_rnn, cc_info_rnn = (\n",
    "    compute_cell_cell_significance(\n",
    "        exp_rnn,\n",
    "        data_type='calcium',\n",
    "        ds=CONFIG['ds'],\n",
    "        n_shuffles_stage1=CONFIG['n_shuffles_stage1'],\n",
    "        n_shuffles_stage2=CONFIG['cc_n_shuffles_stage2'],\n",
    "        pval_thr=CONFIG['cc_pval_thr'],\n",
    "        multicomp_correction='holm',\n",
    "        verbose=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "n_sig_rnn = int(np.sum(np.triu(sig_mat_rnn, k=1)))\n",
    "n_pairs_rnn = len(cell_ids_rnn) * (len(cell_ids_rnn) - 1) // 2\n",
    "print(f'\\n  Significant pairs: {n_sig_rnn} / {n_pairs_rnn}')\n",
    "\n",
    "weighted_rnn = sp.csr_matrix(sim_mat_rnn * sig_mat_rnn)\n",
    "net_rnn = Network(\n",
    "    adj=weighted_rnn, preprocessing='giant_cc', name='RNN functional network'\n",
    ")\n",
    "print(f'  Network: {net_rnn.n} nodes, {net_rnn.graph.number_of_edges()} edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becc8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 14))\n",
    "fps = CONFIG['fps']\n",
    "\n",
    "show_frames = min(50 * fps, n_frames)\n",
    "t_sec = np.arange(show_frames) / fps\n",
    "\n",
    "# ---- Row 1: Data overview ----\n",
    "\n",
    "# (1,1) Input signals\n",
    "ax = fig.add_subplot(3, 3, 1)\n",
    "signals = [\n",
    "    ('x', inputs['x'][:show_frames]),\n",
    "    ('speed', inputs['speed'][:show_frames]),\n",
    "    ('HD', inputs['head_direction'][:show_frames] / (2 * np.pi)),\n",
    "    ('trial', inputs['trial_type'][:show_frames].astype(float) / 2),\n",
    "]\n",
    "for i, (label, sig) in enumerate(signals):\n",
    "    ax.plot(t_sec, sig + i * 1.2, lw=0.5)\n",
    "    ax.text(-1, i * 1.2 + 0.4, label, fontsize=7, ha='right')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Input signals')\n",
    "\n",
    "# (1,2) RNN activity raster\n",
    "ax = fig.add_subplot(3, 3, 2)\n",
    "order = np.argsort(activations.mean(axis=1))\n",
    "ax.imshow(\n",
    "    activations[order, :show_frames], aspect='auto', cmap='inferno',\n",
    "    extent=[0, show_frames / fps, 0, activations.shape[0]],\n",
    ")\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Unit (sorted)')\n",
    "ax.set_title('RNN activations')\n",
    "\n",
    "# (1,3) INTENSE selectivity heatmap\n",
    "ax = fig.add_subplot(3, 3, 3)\n",
    "feat_names_rnn = [\n",
    "    f for f in exp_rnn.dynamic_features\n",
    "    if f not in ('x', 'y', 'head_direction')\n",
    "]\n",
    "mi_matrix_rnn = np.zeros((exp_rnn.n_cells, len(feat_names_rnn)))\n",
    "for uid, feats in significant_neurons_rnn.items():\n",
    "    idx_n = int(uid)\n",
    "    for fname in feats:\n",
    "        if fname in feat_names_rnn:\n",
    "            pair_stats = exp_rnn.get_neuron_feature_pair_stats(uid, fname)\n",
    "            col = feat_names_rnn.index(fname)\n",
    "            mi_matrix_rnn[idx_n, col] = pair_stats.get('me', 0)\n",
    "im = ax.imshow(mi_matrix_rnn, aspect='auto', cmap='viridis')\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Unit')\n",
    "ax.set_xticks(range(len(feat_names_rnn)))\n",
    "ax.set_xticklabels(feat_names_rnn, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_title('Selectivity (MI, significant only)')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046, label='MI (bits)')\n",
    "\n",
    "# ---- Row 2: PCA embedding colored by different variables ----\n",
    "coords_rnn = emb_rnn.coords\n",
    "ds_rnn = 10\n",
    "x_pc = coords_rnn[0, ::ds_rnn]\n",
    "y_pc = coords_rnn[1, ::ds_rnn]\n",
    "\n",
    "color_vars = [\n",
    "    ('x position', inputs['x'][::ds_rnn]),\n",
    "    ('head direction', inputs['head_direction'][::ds_rnn]),\n",
    "    ('trial type', inputs['trial_type'][::ds_rnn].astype(float)),\n",
    "]\n",
    "cmaps = ['viridis', 'twilight', 'Set1']\n",
    "for i, (label, cvar) in enumerate(color_vars):\n",
    "    ax = fig.add_subplot(3, 3, 4 + i)\n",
    "    sc = ax.scatter(x_pc, y_pc, c=cvar, cmap=cmaps[i], s=1, alpha=0.3,\n",
    "                    rasterized=True)\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "    ax.set_title(f'PCA colored by {label}')\n",
    "    plt.colorbar(sc, ax=ax, fraction=0.046)\n",
    "\n",
    "# ---- Row 3: Network analysis ----\n",
    "\n",
    "# (3,1) Similarity matrix\n",
    "ax = fig.add_subplot(3, 3, 7)\n",
    "im = ax.imshow(sim_mat_rnn, cmap='hot', aspect='auto')\n",
    "ax.set_xlabel('Unit')\n",
    "ax.set_ylabel('Unit')\n",
    "ax.set_title('Cell-cell similarity (MI)')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "# (3,2) Network graph\n",
    "ax = fig.add_subplot(3, 3, 8)\n",
    "if net_rnn.graph.number_of_edges() > 0:\n",
    "    pos = nx.spring_layout(net_rnn.graph, seed=CONFIG['seed'])\n",
    "    nx.draw_networkx_nodes(net_rnn.graph, pos, ax=ax, node_size=30,\n",
    "                           node_color='steelblue')\n",
    "    nx.draw_networkx_edges(net_rnn.graph, pos, ax=ax, alpha=0.2, width=0.5)\n",
    "    ax.set_title(f'Functional network ({net_rnn.n} nodes, '\n",
    "                 f'{net_rnn.graph.number_of_edges()} edges)')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No significant edges',\n",
    "            ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title('Functional network')\n",
    "ax.axis('off')\n",
    "\n",
    "# (3,3) Summary text\n",
    "ax = fig.add_subplot(3, 3, 9)\n",
    "ax.axis('off')\n",
    "n_selective = len(significant_neurons_rnn)\n",
    "n_mixed_rnn = sum(1 for feats in significant_neurons_rnn.values() if len(feats) > 1)\n",
    "n_sig_pairs_rnn = int(np.sum(np.triu(sig_mat_rnn, k=1)))\n",
    "total_pairs_rnn = exp_rnn.n_cells * (exp_rnn.n_cells - 1) // 2\n",
    "density_rnn = n_sig_pairs_rnn / total_pairs_rnn if total_pairs_rnn > 0 else 0\n",
    "\n",
    "text = (\n",
    "    f\"RNN: {CONFIG['n_units']} units, g={CONFIG['g_rec']}\\n\"\n",
    "    f\"Recording: {CONFIG['duration']}s at {CONFIG['fps']} Hz\\n\\n\"\n",
    "    f\"INTENSE selectivity:\\n\"\n",
    "    f\"  Selective units: {n_selective}/{exp_rnn.n_cells}\\n\"\n",
    "    f\"  Mixed selectivity: {n_mixed_rnn}\\n\\n\"\n",
    "    f\"Functional network:\\n\"\n",
    "    f\"  Significant pairs: {n_sig_pairs_rnn}/{total_pairs_rnn}\\n\"\n",
    "    f\"  Density: {density_rnn:.3f}\\n\"\n",
    "    f\"  Nodes in GCC: {net_rnn.n}\"\n",
    ")\n",
    "ax.text(0.05, 0.95, text, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
