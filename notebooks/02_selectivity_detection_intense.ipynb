{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebd2f83",
   "metadata": {},
   "source": [
    "# Detecting neuron selectivity with INTENSE\n",
    "\n",
    "Which neurons encode which behavioral variables -- and how do you tell\n",
    "real selectivity from chance?  [**DRIADA**](https://driada.readthedocs.io)\n",
    "answers this with **INTENSE**, an information-theoretic significance\n",
    "testing pipeline.  This notebook walks through the method from first\n",
    "principles to a full production run.\n",
    "\n",
    "| Step | Notebook | What it does |\n",
    "|---|---|---|\n",
    "| Load & inspect | [01 -- Data loading](01_data_loading_and_neurons.ipynb) | Wrap your recording into an `Experiment`, reconstruct spikes, assess quality |\n",
    "| **Single-neuron selectivity** | **02 -- this notebook** | Detect which neurons encode which behavioral variables |\n",
    "| Population geometry | [03 -- Dimensionality reduction](03_population_geometry_dr.ipynb) | Extract low-dimensional manifolds from population activity |\n",
    "| Functional networks | [04 -- Networks](04_functional_networks.ipynb) | Build and analyze cell-cell interaction graphs |\n",
    "| Putting it together | [05 -- Advanced](05_advanced_capabilities.ipynb) | Combine INTENSE + DR, leave-one-out importance, RSA, RNN analysis |\n",
    "\n",
    "**What you will learn:**\n",
    "\n",
    "1. **Information theory fundamentals** -- mutual information estimation\n",
    "   (GCMI vs KSG), similarity metrics, time-delayed MI, conditional MI,\n",
    "   and interaction information.\n",
    "2. **Basic INTENSE workflow** -- generate a synthetic population, run\n",
    "   two-stage significance testing, and extract results.\n",
    "3. **Complete pipeline with ground truth validation** -- all feature types,\n",
    "   Holm correction, disentanglement, delay optimization, and validation\n",
    "   against known selectivity.\n",
    "4. **Feature-feature relations** -- discover which behavioral variables\n",
    "   are themselves correlated before analyzing neurons.\n",
    "5. **Mixed selectivity & disentanglement** -- neurons responding to\n",
    "   multiple correlated features: true mixed selectivity vs redundant\n",
    "   detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8768652",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q driada\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import driada\n",
    "from driada.information import (\n",
    "    TimeSeries,\n",
    "    get_mi,\n",
    "    get_sim,\n",
    "    get_tdmi,\n",
    "    conditional_mi,\n",
    "    interaction_information,\n",
    ")\n",
    "from driada.information.info_base import MultiTimeSeries\n",
    "from driada.experiment.synthetic import generate_tuned_selectivity_exp\n",
    "from driada.intense import compute_feat_feat_significance\n",
    "from driada.intense.io import save_results, load_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444cc769",
   "metadata": {},
   "source": [
    "## 1. Information theory fundamentals\n",
    "\n",
    "Before running INTENSE, understand the building blocks: mutual information\n",
    "estimation, similarity metrics, temporal lags, and conditional dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7222eb6d",
   "metadata": {},
   "source": [
    "### Creating TimeSeries\n",
    "\n",
    "Wrap numpy arrays as [`TimeSeries`](https://driada.readthedocs.io/en/latest/api/information/core.html) with type hints (`linear`, `categorical`,\n",
    "`circular`). The type determines which MI estimator and preprocessing\n",
    "DRIADA uses internally. Continuous-continuous pairs use GCMI (Gaussian copula,\n",
    "via `mi_gg`) or KSG; continuous-discrete pairs use `mi_model_gd`;\n",
    "discrete-discrete pairs use exact MI from the joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "n = 5000\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Create TimeSeries from numpy arrays\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[1] Creating TimeSeries objects\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "continuous = rng.normal(size=n)\n",
    "ts_cont = TimeSeries(continuous, ts_type=\"linear\", name=\"continuous\")\n",
    "print(f\"  Continuous: type={ts_cont.type_info}, len={len(ts_cont.data)}\")\n",
    "\n",
    "discrete = rng.choice([0, 1, 2], size=n, p=[0.5, 0.3, 0.2])\n",
    "ts_disc = TimeSeries(discrete, ts_type=\"categorical\", name=\"discrete\")\n",
    "print(f\"  Discrete:   type={ts_disc.type_info}, len={len(ts_disc.data)}\")\n",
    "\n",
    "circular = rng.uniform(0, 2 * np.pi, size=n)\n",
    "ts_circ = TimeSeries(circular, ts_type=\"circular\", name=\"circular\")\n",
    "print(f\"  Circular:   type={ts_circ.type_info}, len={len(ts_circ.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab52e02",
   "metadata": {},
   "source": [
    "### Pairwise MI: GCMI vs KSG\n",
    "\n",
    "[`get_mi`](https://driada.readthedocs.io/en/latest/api/information/core.html)`()` estimates mutual information between two `TimeSeries`.\n",
    "**GCMI** (Gaussian Copula MI) is fast but only captures monotonic\n",
    "dependency. **KSG** (Kraskov-Stoegbauer-Grassberger) captures arbitrary\n",
    "dependency but is slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d79c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 2. get_mi -- pairwise MI\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[2] Pairwise mutual information (get_mi)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "x = rng.normal(size=n)\n",
    "noise = rng.normal(size=n)\n",
    "y_corr = x + 0.5 * noise          # correlated with x\n",
    "y_indep = rng.normal(size=n)       # independent of x\n",
    "\n",
    "ts_x = TimeSeries(x)\n",
    "ts_y_corr = TimeSeries(y_corr)\n",
    "ts_y_indep = TimeSeries(y_indep)\n",
    "\n",
    "mi_corr = get_mi(ts_x, ts_y_corr)\n",
    "mi_indep = get_mi(ts_x, ts_y_indep)\n",
    "print(f\"  MI(X, Y_correlated)  = {mi_corr:.4f} bits\")\n",
    "print(f\"  MI(X, Y_independent) = {mi_indep:.4f} bits\")\n",
    "print(f\"  Correlated MI >> independent MI: {mi_corr > 5 * mi_indep}\")\n",
    "\n",
    "# Compare estimators on monotonic vs non-monotonic relationships.\n",
    "# GCMI reduces to -0.5*log(1-rho^2) where rho is Spearman rank correlation,\n",
    "# so it only captures monotonic dependency. KSG captures any dependency.\n",
    "mi_gcmi = get_mi(ts_x, ts_y_corr, estimator=\"gcmi\")\n",
    "mi_ksg = get_mi(ts_x, ts_y_corr, estimator=\"ksg\")\n",
    "print(f\"\\n  Monotonic relationship (y = x + noise):\")\n",
    "print(f\"    GCMI: {mi_gcmi:.4f} bits\")\n",
    "print(f\"    KSG:  {mi_ksg:.4f} bits\")\n",
    "print(f\"    (agree because relationship is monotonic)\")\n",
    "\n",
    "# Non-monotonic: y = x^2. Spearman rho ~ 0 due to exact symmetry, so GCMI ~ 0.\n",
    "x_sym = rng.uniform(-3, 3, size=n)\n",
    "y_quad = x_sym ** 2 + 0.3 * rng.normal(size=n)\n",
    "ts_x_sym = TimeSeries(x_sym)\n",
    "ts_y_quad = TimeSeries(y_quad)\n",
    "mi_gcmi_q = get_mi(ts_x_sym, ts_y_quad, estimator=\"gcmi\")\n",
    "mi_ksg_q = get_mi(ts_x_sym, ts_y_quad, estimator=\"ksg\")\n",
    "print(f\"\\n  Non-monotonic relationship (y = x^2 + noise):\")\n",
    "print(f\"    GCMI: {mi_gcmi_q:.4f} bits  (blind to symmetric dependency)\")\n",
    "print(f\"    KSG:  {mi_ksg_q:.4f} bits  (captures it)\")\n",
    "print(f\"    KSG >> GCMI: {mi_ksg_q > 3 * mi_gcmi_q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dabc2",
   "metadata": {},
   "source": [
    "### Similarity metrics\n",
    "\n",
    "[`get_sim`](https://driada.readthedocs.io/en/latest/api/information/core.html)`()` wraps MI, Pearson r, and Spearman rho in a unified interface.\n",
    "Available metrics include `mi`, `pearsonr`, `spearmanr`, `kendalltau`,\n",
    "`fast_pearsonr`, `av` (activity ratio for binary-gated signals), and any\n",
    "scipy.stats correlation function by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 3. get_sim -- compare metrics on the same data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[3] Similarity metrics comparison (get_sim)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "metrics = [\"mi\", \"pearsonr\", \"spearmanr\"]\n",
    "for metric in metrics:\n",
    "    val = get_sim(ts_x, ts_y_corr, metric=metric)\n",
    "    print(f\"  {metric:12s}(X, Y_corr) = {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44648d5f",
   "metadata": {},
   "source": [
    "### Time-delayed MI\n",
    "\n",
    "[`get_tdmi`](https://driada.readthedocs.io/en/latest/api/information/core.html)`()` sweeps temporal lags to find the shift that maximizes MI.\n",
    "This is useful for detecting delayed neural responses to behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2588dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 4. get_tdmi -- time-delayed MI\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[4] Time-delayed MI (get_tdmi)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a signal with known lag=15 autocorrelation\n",
    "base = rng.normal(size=n)\n",
    "lag = 15\n",
    "lagged = np.zeros(n)\n",
    "lagged[lag:] = base[:-lag]\n",
    "signal = base + 0.3 * rng.normal(size=n) + 0.8 * lagged\n",
    "\n",
    "max_shift = 50\n",
    "tdmi_values = np.array(get_tdmi(signal, max_shift=max_shift))\n",
    "best_lag = np.argmax(tdmi_values) + 1  # get_tdmi starts at min_shift=1\n",
    "print(f\"  True lag: {lag}\")\n",
    "print(f\"  TDMI peak lag: {best_lag}\")\n",
    "print(f\"  TDMI at peak: {tdmi_values[best_lag - 1]:.4f} bits\")\n",
    "print(f\"  Lag correctly detected: {abs(best_lag - lag) <= 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184be6e",
   "metadata": {},
   "source": [
    "### Conditional MI and interaction information\n",
    "\n",
    "**Conditional MI** ([`conditional_mi`](https://driada.readthedocs.io/en/latest/api/information/core.html)) `I(X;Y|Z)` removes shared variance with Z.\n",
    "**Interaction information** ([`interaction_information`](https://driada.readthedocs.io/en/latest/api/information/core.html)) distinguishes synergy (>0) from\n",
    "redundancy (<0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ef3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 5. conditional_mi -- I(X;Y|Z)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[5] Conditional MI: I(X;Y|Z)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "z = rng.normal(size=n)\n",
    "x_from_z = z + 0.3 * rng.normal(size=n)\n",
    "y_from_z = z + 0.3 * rng.normal(size=n)\n",
    "\n",
    "ts_xz = TimeSeries(x_from_z)\n",
    "ts_yz = TimeSeries(y_from_z)\n",
    "ts_z = TimeSeries(z)\n",
    "\n",
    "mi_xy = get_mi(ts_xz, ts_yz)\n",
    "cmi_xy_z = conditional_mi(ts_xz, ts_yz, ts_z)\n",
    "\n",
    "print(f\"  I(X;Y)   = {mi_xy:.4f} bits  (shared via Z)\")\n",
    "print(f\"  I(X;Y|Z) = {cmi_xy_z:.4f} bits  (residual after conditioning)\")\n",
    "print(f\"  Conditioning reduces MI: {cmi_xy_z < mi_xy * 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 6. interaction_information -- synergy vs redundancy\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[6] Interaction information: synergy vs redundancy\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Redundancy: Y and Z provide overlapping info about X\n",
    "x_r = rng.normal(size=n)\n",
    "y_r = TimeSeries(x_r + 0.2 * rng.normal(size=n))\n",
    "z_r = TimeSeries(x_r + 0.2 * rng.normal(size=n))\n",
    "ts_xr = TimeSeries(x_r)\n",
    "\n",
    "ii_redund = interaction_information(ts_xr, y_r, z_r)\n",
    "print(f\"  Redundancy example: II = {ii_redund:.4f} (expected < 0)\")\n",
    "\n",
    "# Synergy: XOR-like relationship\n",
    "a = rng.choice([0, 1], size=n).astype(float)\n",
    "b = rng.choice([0, 1], size=n).astype(float)\n",
    "xor_signal = (a + b + 0.1 * rng.normal(size=n))\n",
    "\n",
    "ts_xor = TimeSeries(xor_signal)\n",
    "ts_a = TimeSeries(a, ts_type=\"binary\")\n",
    "ts_b = TimeSeries(b, ts_type=\"binary\")\n",
    "\n",
    "ii_synergy = interaction_information(ts_xor, ts_a, ts_b)\n",
    "print(f\"  Synergy example:    II = {ii_synergy:.4f} (expected > 0)\")\n",
    "print(f\"  Redundancy is negative: {ii_redund < 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d479cd",
   "metadata": {},
   "source": [
    "## 2. Basic INTENSE workflow\n",
    "\n",
    "The minimal pipeline: [`generate_tuned_selectivity_exp`](https://driada.readthedocs.io/en/latest/api/experiment/synthetic.html) creates a synthetic\n",
    "population with known selectivity, [`compute_cell_feat_significance`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html) runs\n",
    "two-stage significance testing, and results are extracted from the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate synthetic experiment with meaningful features\n",
    "print(\"1. Generating synthetic experiment...\")\n",
    "print(\"   - 10 neurons with realistic tuning\")\n",
    "print(\"   - Head direction cells (circular tuning)\")\n",
    "print(\"   - Speed cells (linear tuning)\")\n",
    "print(\"   - Event cells (discrete responses)\")\n",
    "print(\"   - 10 minutes recording\")\n",
    "\n",
    "# Define simple population with meaningful selectivity\n",
    "population = [\n",
    "    {\"name\": \"hd_cells\", \"count\": 2, \"features\": [\"head_direction\"]},\n",
    "    {\"name\": \"speed_cells\", \"count\": 2, \"features\": [\"speed\"]},\n",
    "    {\"name\": \"event_cells\", \"count\": 2, \"features\": [\"event_0\"]},\n",
    "    {\"name\": \"nonselective\", \"count\": 4, \"features\": []},\n",
    "]\n",
    "\n",
    "exp = generate_tuned_selectivity_exp(\n",
    "    population=population,\n",
    "    duration=600,\n",
    "    fps=20,\n",
    "    seed=47,\n",
    "    n_discrete_features=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"   [OK] Created experiment with {exp.n_cells} neurons and {exp.n_frames} timepoints\"\n",
    ")\n",
    "print(f\"   [OK] Features: {list(exp.dynamic_features.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Analyze neuronal selectivity\n",
    "print(\"2. Running INTENSE analysis...\")\n",
    "print(\"   - Two-stage statistical testing\")\n",
    "print(\"   - Mutual information metric\")\n",
    "print(\"   - Multiple comparison correction\")\n",
    "\n",
    "stats, significance, info, results = driada.compute_cell_feat_significance(\n",
    "    exp,\n",
    "    mode=\"two_stage\",\n",
    "    n_shuffles_stage1=100,\n",
    "    n_shuffles_stage2=10000,\n",
    "    pval_thr=0.001,\n",
    "    multicomp_correction=None,\n",
    "    ds=5,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"   [OK] Analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22690c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract significant results\n",
    "print(\"3. Extracting significant results...\")\n",
    "\n",
    "significant_neurons = exp.get_significant_neurons()\n",
    "total_pairs = sum(len(features) for features in significant_neurons.values())\n",
    "\n",
    "print(f\"   [OK] Found {len(significant_neurons)} neurons with significant selectivity\")\n",
    "print(f\"   [OK] Total significant neuron-feature pairs: {total_pairs}\")\n",
    "\n",
    "# Step 4: Display results\n",
    "print(\"\\n4. Results summary:\")\n",
    "\n",
    "if significant_neurons:\n",
    "    print(\"   Significant neuron-feature relationships:\")\n",
    "    for cell_id in list(significant_neurons.keys())[:3]:  # Show first 3\n",
    "        for feat_name in significant_neurons[cell_id]:\n",
    "            pair_stats = exp.get_neuron_feature_pair_stats(cell_id, feat_name)\n",
    "\n",
    "            print(f\"   - Neuron {cell_id} <-> Feature '{feat_name}':\")\n",
    "            print(f\"     - Mutual Information: {pair_stats.get('me', 0):.4f} bits\")\n",
    "            if \"pval\" in pair_stats:\n",
    "                print(f\"     - P-value: {pair_stats['pval']:.2e}\")\n",
    "            # opt_delay is in frames; convert to seconds using experiment fps\n",
    "            opt_delay_frames = pair_stats.get(\"opt_delay\", 0)\n",
    "            opt_delay_sec = opt_delay_frames / exp.fps if exp.fps else 0\n",
    "            print(f\"     - Optimal delay: {opt_delay_sec:.2f}s ({opt_delay_frames} frames)\")\n",
    "\n",
    "    if len(significant_neurons) > 3:\n",
    "        remaining = len(significant_neurons) - 3\n",
    "        print(f\"   ... and {remaining} more significant neurons\")\n",
    "else:\n",
    "    print(\"   No significant relationships found with current parameters.\")\n",
    "    print(\"   Try using different synthetic data or adjusting p-value threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d170073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create visualization\n",
    "print(\"5. Creating visualization...\")\n",
    "\n",
    "if significant_neurons:\n",
    "    # Plot first significant neuron-feature pair\n",
    "    cell_id = list(significant_neurons.keys())[0]\n",
    "    feat_name = significant_neurons[cell_id][0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    driada.intense.plot_neuron_feature_pair(exp, cell_id, feat_name, ax=ax)\n",
    "    plt.title(f\"Neuron {cell_id} selectivity to {feat_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"   No visualization created (no significant relationships found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ded48",
   "metadata": {},
   "source": [
    "## 3. Complete pipeline with ground truth validation\n",
    "\n",
    "Production workflow: all feature types (circular, spatial, linear,\n",
    "discrete), Holm correction, disentanglement, delay optimization,\n",
    "and ground truth validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population configuration - defines neuron groups and their selectivity\n",
    "POPULATION = [\n",
    "    {\"name\": \"hd_cells\", \"count\": 4, \"features\": [\"head_direction\"]},\n",
    "    {\"name\": \"place_cells\", \"count\": 4, \"features\": [\"position_2d\"]},\n",
    "    {\"name\": \"speed_cells\", \"count\": 4, \"features\": [\"speed\"]},\n",
    "    {\"name\": \"event_cells\", \"count\": 4, \"features\": [\"event_0\"]},\n",
    "    {\"name\": \"mixed_cells\", \"count\": 4, \"features\": [\"head_direction\", \"event_0\"]},\n",
    "    {\"name\": \"nonselective\", \"count\": 4, \"features\": []},\n",
    "]\n",
    "\n",
    "# Analysis parameters\n",
    "CONFIG = {\n",
    "    # Recording parameters\n",
    "    \"duration\": 900,        # seconds\n",
    "    \"fps\": 20,              # sampling rate\n",
    "    \"seed\": 42,\n",
    "    # Tuning parameters\n",
    "    \"kappa\": 4.0,           # von Mises concentration (HD cells)\n",
    "    # Calcium dynamics\n",
    "    \"baseline_rate\": 0.02,  # baseline firing rate\n",
    "    \"peak_rate\": 2.0,       # peak response\n",
    "    \"decay_time\": 1.5,      # calcium decay time\n",
    "    \"calcium_noise\": 0.01,  # noise level\n",
    "    # Discrete event parameters\n",
    "    \"n_discrete_features\": 2,\n",
    "    \"event_active_fraction\": 0.08,  # ~8% active time per event\n",
    "    \"event_avg_duration\": 0.8,      # seconds\n",
    "    # INTENSE analysis parameters\n",
    "    \"n_shuffles_stage1\": 100,   # stage 1 screening shuffles\n",
    "    \"n_shuffles_stage2\": 10000,  # stage 2 confirmation (FFT makes this fast)\n",
    "    \"pval_thr\": 0.05,           # p-value threshold after correction\n",
    "    \"multicomp_correction\": \"holm\",  # multiple comparison correction\n",
    "}\n",
    "\n",
    "# Custom tuning defaults based on config\n",
    "tuning_defaults = {\n",
    "    \"head_direction\": {\"kappa\": CONFIG[\"kappa\"]},\n",
    "}\n",
    "\n",
    "exp3 = generate_tuned_selectivity_exp(\n",
    "    population=POPULATION,\n",
    "    tuning_defaults=tuning_defaults,\n",
    "    duration=CONFIG[\"duration\"],\n",
    "    fps=CONFIG[\"fps\"],\n",
    "    baseline_rate=CONFIG[\"baseline_rate\"],\n",
    "    peak_rate=CONFIG[\"peak_rate\"],\n",
    "    decay_time=CONFIG[\"decay_time\"],\n",
    "    calcium_noise=CONFIG[\"calcium_noise\"],\n",
    "    n_discrete_features=CONFIG[\"n_discrete_features\"],\n",
    "    event_active_fraction=CONFIG[\"event_active_fraction\"],\n",
    "    event_avg_duration=CONFIG[\"event_avg_duration\"],\n",
    "    seed=CONFIG[\"seed\"],\n",
    "    verbose=True,\n",
    ")\n",
    "ground_truth = exp3.ground_truth\n",
    "\n",
    "# Remap ground truth: INTENSE tests head_direction_2d (cos/sin), not raw angle\n",
    "ground_truth[\"expected_pairs\"] = [\n",
    "    (nid, \"head_direction_2d\" if f == \"head_direction\" else f)\n",
    "    for nid, f in ground_truth[\"expected_pairs\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run INTENSE with full options\n",
    "print(\"\\nRunning INTENSE analysis...\")\n",
    "print(f\"  Stage 1: {CONFIG['n_shuffles_stage1']} shuffles\")\n",
    "print(f\"  Stage 2: {CONFIG['n_shuffles_stage2']} shuffles\")\n",
    "print(f\"  P-value threshold: {CONFIG['pval_thr']}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Build feature list: exclude x/y marginals (use position_2d) and raw\n",
    "# circular features (use their _2d cos/sin representation instead)\n",
    "feat_bunch = [\n",
    "    feat_name for feat_name in exp3.dynamic_features.keys()\n",
    "    if feat_name not in [\"x\", \"y\", \"head_direction\"]\n",
    "]\n",
    "print(f\"  Features to test: {feat_bunch}\")\n",
    "\n",
    "# Run INTENSE with disentanglement to handle correlated features\n",
    "# - find_optimal_delays=True: Search for best temporal alignment between\n",
    "#   neural activity and features (compensates for calcium dynamics)\n",
    "# - with_disentanglement=True: Identify redundant detections caused by\n",
    "#   feature correlations (e.g., HD cells detecting position due to\n",
    "#   trajectory patterns where animal faces certain directions at certain locations)\n",
    "stats3, significance3, info3, results3, disent_results3 = driada.compute_cell_feat_significance(\n",
    "    exp3,\n",
    "    feat_bunch=feat_bunch,\n",
    "    mode=\"two_stage\",\n",
    "    n_shuffles_stage1=CONFIG[\"n_shuffles_stage1\"],\n",
    "    n_shuffles_stage2=CONFIG[\"n_shuffles_stage2\"],\n",
    "    find_optimal_delays=True,  # Find best temporal alignment\n",
    "    ds=5,  # Downsampling factor for speed\n",
    "    pval_thr=CONFIG[\"pval_thr\"],\n",
    "    multicomp_correction=CONFIG[\"multicomp_correction\"],\n",
    "    use_precomputed_stats=False,  # Force fresh computation\n",
    "    with_disentanglement=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "analysis_time = time.time() - start_time\n",
    "significant_neurons3 = exp3.get_significant_neurons()\n",
    "\n",
    "total_pairs = sum(len(features) for features in significant_neurons3.values())\n",
    "print(f\"\\n  Completed in {analysis_time:.1f} seconds\")\n",
    "print(f\"  Significant neurons: {len(significant_neurons3)}/{exp3.n_cells}\")\n",
    "print(f\"  Total significant pairs: {total_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347ae92",
   "metadata": {},
   "source": [
    "### Ground truth validation\n",
    "\n",
    "Compare detections to known selectivity. `validate_against_ground_truth`\n",
    "computes sensitivity, precision, and F1 per neuron type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02baa1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate against ground truth using IntenseResults method\n",
    "metrics = results3.validate_against_ground_truth(ground_truth, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8845e3",
   "metadata": {},
   "source": [
    "### Disentanglement\n",
    "\n",
    "Which multi-feature detections are redundant (one feature explains\n",
    "the other) vs true mixed selectivity? Disentanglement removes\n",
    "redundant pairs and improves precision. It uses conditional MI -- for each\n",
    "neuron with multiple significant features, it tests whether\n",
    "`I(neuron; F1 | F2) > 0` to determine if F1 contributes information\n",
    "beyond F2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc10be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply disentanglement corrections and compute updated metrics\n",
    "print(\"DISENTANGLEMENT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if disent_results3 is not None:\n",
    "    summary = disent_results3.get(\"summary\", {})\n",
    "    per_neuron_disent = disent_results3.get(\"per_neuron_disent\", {})\n",
    "\n",
    "    if \"overall_stats\" in summary:\n",
    "        stats_d = summary[\"overall_stats\"]\n",
    "        print(f\"\\n  Neuron-feature pairs analyzed: {stats_d.get('total_neuron_pairs', 0)}\")\n",
    "        print(f\"  Redundancy rate: {stats_d.get('redundancy_rate', 0):.1f}%\")\n",
    "        print(f\"  True mixed selectivity rate: {stats_d.get('true_mixed_selectivity_rate', 0):.1f}%\")\n",
    "\n",
    "    # Build corrected significant_neurons using final_sels\n",
    "    corrected = {}\n",
    "    n_removed = 0\n",
    "    for neuron_id, features in significant_neurons3.items():\n",
    "        if neuron_id in per_neuron_disent:\n",
    "            final = per_neuron_disent[neuron_id].get(\"final_sels\", features)\n",
    "            n_removed += len(features) - len(final)\n",
    "            if final:\n",
    "                corrected[neuron_id] = final\n",
    "        else:\n",
    "            corrected[neuron_id] = features\n",
    "\n",
    "    # Compute corrected metrics against ground truth\n",
    "    expected_pairs = set(ground_truth[\"expected_pairs\"])\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for neuron_id, features in corrected.items():\n",
    "        for feat_name in features:\n",
    "            if (neuron_id, feat_name) in expected_pairs:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "    fn = len(expected_pairs) - tp\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * precision * sensitivity / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "\n",
    "    # Show before/after\n",
    "    print(f\"\\n  Pairs removed by disentanglement: {n_removed}\")\n",
    "    print(f\"\\n  {'Metric':<15} {'Before':<12} {'After'}\")\n",
    "    print(f\"  {'-'*40}\")\n",
    "    print(f\"  {'Sensitivity':<15} {metrics['sensitivity']:>10.1%}   {sensitivity:>10.1%}\")\n",
    "    print(f\"  {'Precision':<15} {metrics['precision']:>10.1%}   {precision:>10.1%}\")\n",
    "    print(f\"  {'F1 Score':<15} {metrics['f1']:>10.1%}   {f1:>10.1%}\")\n",
    "    print(f\"  {'False Pos':<15} {metrics['false_positives']:>10}   {fp:>10}\")\n",
    "else:\n",
    "    print(\"  Disentanglement not performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9b5be",
   "metadata": {},
   "source": [
    "### Optimal delays\n",
    "\n",
    "Temporal offset maximizing MI between neural activity and behavior.\n",
    "Positive delays mean neural activity lags behind behavior (expected\n",
    "for calcium imaging due to indicator dynamics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaae168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimal delays for significant neuron-feature pairs\n",
    "print(\"OPTIMAL DELAYS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "optimal_delays = info3.get(\"optimal_delays\")\n",
    "if optimal_delays is not None:\n",
    "    fps = CONFIG[\"fps\"]\n",
    "    print(f\"\\n  Delay optimization compensates for calcium indicator dynamics.\")\n",
    "    print(f\"  Positive delays = neural activity lags behavior (expected).\")\n",
    "    print(f\"  Sampling rate: {fps} Hz\")\n",
    "\n",
    "    # Report delays for significant pairs, grouped by neuron type\n",
    "    neuron_types = ground_truth.get(\"neuron_types\", {})\n",
    "    type_delays = {}\n",
    "\n",
    "    for neuron_id, features in significant_neurons3.items():\n",
    "        neuron_type = neuron_types.get(neuron_id, \"unknown\")\n",
    "        if neuron_type not in type_delays:\n",
    "            type_delays[neuron_type] = []\n",
    "\n",
    "        for feat_name in features:\n",
    "            if feat_name in feat_bunch:\n",
    "                feat_idx = feat_bunch.index(feat_name)\n",
    "                delay_frames = optimal_delays[neuron_id, feat_idx]\n",
    "                delay_sec = delay_frames / fps\n",
    "                type_delays[neuron_type].append((neuron_id, feat_name, delay_frames, delay_sec))\n",
    "\n",
    "    print(f\"\\n  Optimal delays for significant pairs:\")\n",
    "    for neuron_type in sorted(type_delays.keys()):\n",
    "        delays = type_delays[neuron_type]\n",
    "        if delays:\n",
    "            # Calculate mean delay for this type\n",
    "            mean_delay_sec = np.mean([d[3] for d in delays])\n",
    "            print(f\"\\n  {neuron_type} (mean: {mean_delay_sec:.2f}s):\")\n",
    "            for neuron_id, feat_name, delay_frames, delay_sec in delays:\n",
    "                print(f\"    Neuron {neuron_id:2d} -> {feat_name:15s}: {delay_frames:4d} frames ({delay_sec:+.2f}s)\")\n",
    "else:\n",
    "    print(\"  No delay optimization performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: selectivity heatmap\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 1. Selectivity heatmap (main plot)\n",
    "ax1 = fig.add_subplot(2, 2, (1, 2))\n",
    "\n",
    "feature_names = feat_bunch\n",
    "n_neurons = exp3.n_cells\n",
    "n_features = len(feature_names)\n",
    "\n",
    "# Create MI matrix using 'me'\n",
    "mi_matrix = np.zeros((n_neurons, n_features))\n",
    "for neuron_id, features in significant_neurons3.items():\n",
    "    for feat_name in features:\n",
    "        if feat_name in feature_names:\n",
    "            feat_idx = feature_names.index(feat_name)\n",
    "            pair_stats = exp3.get_neuron_feature_pair_stats(neuron_id, feat_name)\n",
    "            mi_matrix[neuron_id, feat_idx] = pair_stats.get(\"me\", 0)\n",
    "\n",
    "im = ax1.imshow(mi_matrix, aspect=\"auto\", cmap=\"viridis\")\n",
    "ax1.set_xlabel(\"Features\")\n",
    "ax1.set_ylabel(\"Neurons\")\n",
    "ax1.set_title(\"INTENSE selectivity heatmap (MI values)\")\n",
    "ax1.set_xticks(range(n_features))\n",
    "ax1.set_xticklabels(feature_names, rotation=45, ha=\"right\")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax1, shrink=0.8)\n",
    "cbar.set_label(\"Mutual information (bits)\")\n",
    "\n",
    "# Add neuron type annotations\n",
    "type_colors = {\n",
    "    \"hd_cells\": \"red\",\n",
    "    \"place_cells\": \"blue\",\n",
    "    \"speed_cells\": \"green\",\n",
    "    \"event_cells\": \"orange\",\n",
    "    \"mixed_cells\": \"purple\",\n",
    "    \"nonselective\": \"gray\",\n",
    "}\n",
    "for neuron_id, neuron_type in ground_truth[\"neuron_types\"].items():\n",
    "    color = type_colors.get(neuron_type, \"gray\")\n",
    "    ax1.scatter(-0.7, neuron_id, c=color, s=20, marker=\"s\")\n",
    "\n",
    "# 2. Detection rates by type\n",
    "ax2 = fig.add_subplot(2, 2, 3)\n",
    "types = list(metrics[\"type_stats\"].keys())\n",
    "sensitivities = [metrics[\"type_stats\"][t][\"sensitivity\"] * 100 for t in types]\n",
    "colors = [type_colors.get(t, \"gray\") for t in types]\n",
    "\n",
    "bars = ax2.bar(range(len(types)), sensitivities, color=colors)\n",
    "ax2.set_xticks(range(len(types)))\n",
    "ax2.set_xticklabels([t.replace(\"_\", \"\\n\") for t in types], fontsize=8)\n",
    "ax2.set_ylabel(\"Detection rate (%)\")\n",
    "ax2.set_title(\"Detection rate by neuron type\")\n",
    "ax2.set_ylim(0, 105)\n",
    "ax2.axhline(y=100, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, pct in zip(bars, sensitivities):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            f\"{pct:.0f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "# 3. Summary statistics (before and after disentanglement)\n",
    "ax3 = fig.add_subplot(2, 2, 4)\n",
    "ax3.axis(\"off\")\n",
    "\n",
    "summary_text = (\n",
    "    f\"VALIDATION SUMMARY\\n\"\n",
    "    f\"{'=' * 30}\\n\\n\"\n",
    "    f\"{'Metric':<12} {'Raw':>8}\\n\"\n",
    "    f\"{'-' * 30}\\n\"\n",
    "    f\"{'Sensitivity':<12} {metrics['sensitivity']:>7.1%}\\n\"\n",
    "    f\"{'Precision':<12} {metrics['precision']:>7.1%}\\n\"\n",
    "    f\"{'F1 Score':<12} {metrics['f1']:>7.1%}\\n\\n\"\n",
    "    f\"Detection counts:\\n\"\n",
    "    f\"  True Positives:  {metrics['true_positives']}\\n\"\n",
    "    f\"  False Positives: {metrics['false_positives']}\\n\"\n",
    "    f\"  False Negatives: {metrics['false_negatives']}\\n\\n\"\n",
    "    f\"Population:\\n\"\n",
    "    f\"  Neurons: {exp3.n_cells}, Features: {len(exp3.dynamic_features)}\\n\"\n",
    "    f\"  Expected pairs: {len(ground_truth['expected_pairs'])}\\n\"\n",
    ")\n",
    "ax3.text(0.05, 0.95, summary_text, transform=ax3.transAxes,\n",
    "        fontfamily=\"monospace\", fontsize=9, verticalalignment=\"top\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381bcfa",
   "metadata": {},
   "source": [
    "### Save and load results\n",
    "\n",
    "Persist INTENSE results to disk with [`save_results`](https://driada.readthedocs.io/en/latest/api/intense/base.html) and reload them\n",
    "with [`load_results`](https://driada.readthedocs.io/en/latest/api/intense/base.html) for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/load round-trip\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    results_path = os.path.join(tmpdir, \"intense_results.npz\")\n",
    "    save_results(results3, results_path)\n",
    "    file_mb = os.path.getsize(results_path) / 1024 / 1024\n",
    "    print(f\"  Saved results: {results_path} ({file_mb:.1f} MB)\")\n",
    "\n",
    "    loaded = load_results(results_path)\n",
    "    print(f\"  Reloaded: {len(loaded.stats)} neurons\")\n",
    "    print(f\"  Stats keys match: {set(str(k) for k in results3.stats.keys()) == set(loaded.stats.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fac713",
   "metadata": {},
   "source": [
    "## 4. Feature-feature relations\n",
    "\n",
    "Before disentangling neuron selectivity, check which behavioral variables\n",
    "are themselves correlated. [`compute_feat_feat_significance`](https://driada.readthedocs.io/en/latest/api/intense/pipelines.html) tests all\n",
    "feature pairs with FFT-based circular shuffles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fresh experiment for feature-feature analysis\n",
    "POPULATION_FF = [\n",
    "    {\"name\": \"hd_cells\", \"count\": 4, \"features\": [\"head_direction\"]},\n",
    "    {\"name\": \"place_cells\", \"count\": 4, \"features\": [\"position_2d\"]},\n",
    "    {\"name\": \"speed_cells\", \"count\": 4, \"features\": [\"speed\"]},\n",
    "    {\"name\": \"event_cells\", \"count\": 4, \"features\": [\"event_0\"]},\n",
    "    {\"name\": \"nonselective\", \"count\": 4, \"features\": []},\n",
    "]\n",
    "\n",
    "fps_ff = 20\n",
    "\n",
    "print(\"[1] Generating synthetic experiment\")\n",
    "print(\"-\" * 40)\n",
    "exp4 = generate_tuned_selectivity_exp(\n",
    "    population=POPULATION_FF,\n",
    "    duration=600,\n",
    "    fps=fps_ff,\n",
    "    n_discrete_features=2,\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"  Features: {list(exp4.dynamic_features.keys())}\")\n",
    "\n",
    "# Add derived feature with known correlation to speed\n",
    "print(\"\\n[2] Adding derived feature\")\n",
    "print(\"-\" * 40)\n",
    "speed_data = exp4.dynamic_features[\"speed\"].data\n",
    "\n",
    "# 1-second moving average of speed.\n",
    "# Preserves enough variance for significant MI with the raw signal.\n",
    "kernel_size = int(1 * fps_ff)\n",
    "kernel = np.ones(kernel_size) / kernel_size\n",
    "smoothed = np.convolve(speed_data, kernel, mode=\"same\")\n",
    "exp4.dynamic_features[\"speed_smoothed\"] = TimeSeries(\n",
    "    smoothed, ts_type=\"linear\", name=\"speed_smoothed\"\n",
    ")\n",
    "print(f\"  Added speed_smoothed (1-second moving average of speed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb56e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feat-feat significance.\n",
    "# Use feat_bunch to select features explicitly.\n",
    "# Exclude raw head_direction -- the pipeline uses head_direction_2d\n",
    "# (cos/sin encoding) which preserves circular topology.\n",
    "print(\"[3] Computing feature-feature significance\")\n",
    "print(\"-\" * 40)\n",
    "features_to_test = [\n",
    "    \"head_direction_2d\", \"speed\", \"position_2d\",\n",
    "    \"event_0\", \"event_1\", \"speed_smoothed\",\n",
    "]\n",
    "print(f\"  Testing: {features_to_test}\")\n",
    "print(f\"  (head_direction excluded -- use head_direction_2d for circular data)\")\n",
    "\n",
    "sim_mat, sig_mat, pval_mat, feature_names_ff, info_ff = compute_feat_feat_significance(\n",
    "    exp4,\n",
    "    feat_bunch=features_to_test,\n",
    "    n_shuffles_stage1=100,\n",
    "    n_shuffles_stage2=1000,\n",
    "    pval_thr=0.01,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce761ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"[4] Results summary\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "display_names = []\n",
    "for name in feature_names_ff:\n",
    "    if isinstance(name, (list, tuple)):\n",
    "        display_names.append(\", \".join(str(n) for n in name))\n",
    "    else:\n",
    "        display_names.append(str(name))\n",
    "\n",
    "n_ff = len(feature_names_ff)\n",
    "print(f\"\\n  Features analyzed: {n_ff}\")\n",
    "print(f\"  Feature names: {display_names}\")\n",
    "\n",
    "print(f\"\\n  Significant pairs:\")\n",
    "n_sig = 0\n",
    "for i in range(n_ff):\n",
    "    for j in range(i + 1, n_ff):\n",
    "        if sig_mat[i, j]:\n",
    "            n_sig += 1\n",
    "            print(\n",
    "                f\"    {display_names[i]:20s} <-> {display_names[j]:20s}  \"\n",
    "                f\"MI={sim_mat[i, j]:.4f}  p={pval_mat[i, j]:.2e}\"\n",
    "            )\n",
    "if n_sig == 0:\n",
    "    print(\"    (none)\")\n",
    "print(f\"\\n  Total significant pairs: {n_sig}/{n_ff * (n_ff - 1) // 2}\")\n",
    "\n",
    "# Create MI heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "plot_mat = sim_mat.copy().astype(float)\n",
    "np.fill_diagonal(plot_mat, np.nan)\n",
    "\n",
    "im = ax.imshow(plot_mat, cmap=\"Blues\", aspect=\"equal\")\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label(\"Mutual information (bits)\")\n",
    "\n",
    "for i in range(n_ff):\n",
    "    for j in range(n_ff):\n",
    "        if i != j and sig_mat[i, j]:\n",
    "            ax.text(j, i, \"*\", ha=\"center\", va=\"center\",\n",
    "                    fontsize=14, fontweight=\"bold\", color=\"red\")\n",
    "\n",
    "for i in range(n_ff):\n",
    "    ax.add_patch(plt.Rectangle((i - 0.5, i - 0.5), 1, 1,\n",
    "                               fill=True, facecolor=\"0.85\", edgecolor=\"none\"))\n",
    "\n",
    "ax.set_xticks(range(n_ff))\n",
    "ax.set_xticklabels(display_names, rotation=45, ha=\"right\", fontsize=8)\n",
    "ax.set_yticks(range(n_ff))\n",
    "ax.set_yticklabels(display_names, fontsize=8)\n",
    "ax.set_title(\"Feature-feature mutual information (* = significant)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf175d",
   "metadata": {},
   "source": [
    "## 5. Mixed selectivity & disentanglement\n",
    "\n",
    "Neurons responding to multiple correlated features: true mixed selectivity\n",
    "vs redundant detections. Here we generate a 30-neuron population with\n",
    "explicit mixed selectivity groups and apply disentanglement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff27cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with known mixed selectivity patterns\n",
    "print(\"=== GENERATING MIXED SELECTIVITY DATA ===\")\n",
    "\n",
    "# Define population with real features for mixed selectivity demonstration\n",
    "population5 = [\n",
    "    {\"name\": \"hd_cells\", \"count\": 5, \"features\": [\"head_direction\"]},\n",
    "    {\"name\": \"speed_cells\", \"count\": 5, \"features\": [\"speed\"]},\n",
    "    {\"name\": \"event_cells\", \"count\": 5, \"features\": [\"event_0\"]},\n",
    "    {\"name\": \"mixed_hd_speed\", \"count\": 5, \"features\": [\"head_direction\", \"speed\"], \"combination\": \"weighted_sum\"},\n",
    "    {\"name\": \"mixed_hd_event\", \"count\": 5, \"features\": [\"head_direction\", \"event_0\"], \"combination\": \"weighted_sum\"},\n",
    "    {\"name\": \"mixed_speed_event\", \"count\": 5, \"features\": [\"speed\", \"event_0\"], \"combination\": \"weighted_sum\"},\n",
    "]\n",
    "\n",
    "exp5 = generate_tuned_selectivity_exp(\n",
    "    population=population5,\n",
    "    duration=900,\n",
    "    fps=20,\n",
    "    seed=42,\n",
    "    n_discrete_features=1,\n",
    "    baseline_rate=0.1,\n",
    "    peak_rate=2.0,\n",
    "    decay_time=2.0,\n",
    "    calcium_noise=0.05,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Generated experiment: {exp5.n_cells} neurons, {len(exp5.dynamic_features)} features, {exp5.n_frames/exp5.fps:.1f}s recording\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run INTENSE analysis with disentanglement\n",
    "print(\"=== RUNNING INTENSE ANALYSIS ===\")\n",
    "\n",
    "# Run comprehensive analysis with disentanglement\n",
    "results5 = driada.compute_cell_feat_significance(\n",
    "    exp5,\n",
    "    mode=\"two_stage\",\n",
    "    n_shuffles_stage1=100,  # Stage 1 screening\n",
    "    n_shuffles_stage2=10000,  # FFT optimization makes high counts fast\n",
    "    verbose=False,\n",
    "    with_disentanglement=True,  # Enable disentanglement analysis\n",
    "    multifeature_map=driada.intense.DEFAULT_MULTIFEATURE_MAP,\n",
    "    # Uses default gamma_zi distribution (better for MI null distribution)\n",
    "    pval_thr=0.05,  # Slightly less conservative threshold\n",
    ")\n",
    "\n",
    "stats5, significance5, info5, intense_results5, disentanglement_results5 = results5\n",
    "\n",
    "# Extract significant relationships\n",
    "significant_neurons5 = exp5.get_significant_neurons()\n",
    "\n",
    "# Also get neurons with mixed selectivity (at least 2 features)\n",
    "mixed_candidates5 = exp5.get_significant_neurons(min_nspec=2)\n",
    "\n",
    "# Count multifeature relationships\n",
    "multifeature_count = 0\n",
    "for cell_id, features in significant_neurons5.items():\n",
    "    for feat in features:\n",
    "        if feat in exp5.dynamic_features and isinstance(\n",
    "            exp5.dynamic_features[feat], MultiTimeSeries\n",
    "        ):\n",
    "            multifeature_count += 1\n",
    "\n",
    "print(\n",
    "    f\"Found {len(significant_neurons5)} significant neurons, {len(mixed_candidates5)} with mixed selectivity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3168076d",
   "metadata": {},
   "source": [
    "### Disentanglement analysis\n",
    "\n",
    "Extract and interpret disentanglement results from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process disentanglement results from the pipeline\n",
    "print(\"=== DISENTANGLEMENT ANALYSIS ===\")\n",
    "\n",
    "if mixed_candidates5 and disentanglement_results5 is not None:\n",
    "    # Extract results from the pipeline\n",
    "    disent_matrix5 = disentanglement_results5.get(\"disent_matrix\")\n",
    "    count_matrix5 = disentanglement_results5.get(\"count_matrix\")\n",
    "    feat_names5 = disentanglement_results5.get(\"feature_names\", [])\n",
    "\n",
    "    if disent_matrix5 is not None and count_matrix5 is not None:\n",
    "        print(\"Disentanglement analysis completed by pipeline\")\n",
    "        print(\n",
    "            f\"Matrix shape: {disent_matrix5.shape}, Non-zero entries: {np.count_nonzero(count_matrix5)}\"\n",
    "        )\n",
    "        print(f\"Feature names analyzed: {feat_names5}\")\n",
    "\n",
    "        # Show summary if available\n",
    "        if \"summary\" in disentanglement_results5:\n",
    "            summary5 = disentanglement_results5[\"summary\"]\n",
    "            if \"overall_stats\" in summary5:\n",
    "                stats5s = summary5[\"overall_stats\"]\n",
    "                print(\"\\nOverall statistics:\")\n",
    "                print(f\"  Total neuron pairs: {stats5s.get('total_neuron_pairs', 0)}\")\n",
    "                print(f\"  Redundancy rate: {stats5s.get('redundancy_rate', 0):.1f}%\")\n",
    "                print(\n",
    "                    f\"  True mixed selectivity rate: {stats5s.get('true_mixed_selectivity_rate', 0):.1f}%\"\n",
    "                )\n",
    "    else:\n",
    "        print(\"Disentanglement matrices not found in results.\")\n",
    "        disent_matrix5, count_matrix5, feat_names5 = None, None, None\n",
    "else:\n",
    "    print(\"No mixed selectivity candidates or disentanglement results.\")\n",
    "    disent_matrix5, count_matrix5, feat_names5 = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret: redundancy vs independence vs synergy\n",
    "print(\"=== INTERPRETING DISENTANGLEMENT RESULTS ===\")\n",
    "\n",
    "if disent_matrix5 is not None and count_matrix5 is not None:\n",
    "    redundancy_cases = []\n",
    "    synergy_cases = []\n",
    "    independence_cases = []\n",
    "\n",
    "    # Calculate relative disentanglement matrix\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        rel_disent_matrix = np.divide(disent_matrix5, count_matrix5) * 100\n",
    "        rel_disent_matrix[count_matrix5 == 0] = np.nan\n",
    "\n",
    "    # Extract disentanglement cases based on matrix values\n",
    "    for i in range(len(feat_names5)):\n",
    "        for j in range(\n",
    "            i + 1, len(feat_names5)\n",
    "        ):  # Only upper triangle to avoid duplicates\n",
    "            if count_matrix5[i, j] > 0:  # Only consider pairs with data\n",
    "                feat1 = feat_names5[i]\n",
    "                feat2 = feat_names5[j]\n",
    "\n",
    "                # Get disentanglement score (percentage)\n",
    "                disent_score = rel_disent_matrix[i, j]\n",
    "\n",
    "                if not np.isnan(disent_score):\n",
    "                    # Classify based on disentanglement score\n",
    "                    if disent_score < 30:  # Redundancy: feat2 dominates\n",
    "                        redundancy_cases.append(\n",
    "                            (f\"{feat1}-{feat2}\", (feat1, feat2), disent_score / 100)\n",
    "                        )\n",
    "                    elif disent_score > 70:  # Synergy: feat1 dominates\n",
    "                        synergy_cases.append(\n",
    "                            (f\"{feat1}-{feat2}\", (feat1, feat2), disent_score / 100)\n",
    "                        )\n",
    "                    else:  # Independence: balanced\n",
    "                        independence_cases.append(\n",
    "                            (f\"{feat1}-{feat2}\", (feat1, feat2), disent_score / 100)\n",
    "                        )\n",
    "\n",
    "    # Summary statistics\n",
    "    total_pairs = len(redundancy_cases) + len(synergy_cases) + len(independence_cases)\n",
    "    print(\n",
    "        f\"Found {len(redundancy_cases)} redundancy, {len(independence_cases)} independence, {len(synergy_cases)} synergy cases\"\n",
    "    )\n",
    "\n",
    "    # Show a few examples if available\n",
    "    if redundancy_cases and len(redundancy_cases) > 0:\n",
    "        feat1, feat2 = redundancy_cases[0][1]\n",
    "        print(f\"Example redundancy: {feat1} <-> {feat2}\")\n",
    "    elif synergy_cases and len(synergy_cases) > 0:\n",
    "        feat1, feat2 = synergy_cases[0][1]\n",
    "        print(f\"Example synergy: {feat1} + {feat2}\")\n",
    "else:\n",
    "    print(\"No disentanglement data to interpret.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d80d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectivity heatmap visualization\n",
    "print(\"Creating neuron-feature selectivity heatmap...\")\n",
    "\n",
    "try:\n",
    "    fig_select, ax_select, stats_select = driada.intense.plot_selectivity_heatmap(\n",
    "        exp5, significant_neurons5, metric=\"mi\", use_log_scale=False, figsize=(12, 10)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"  - {stats_select['n_selective']} selective neurons ({stats_select['selectivity_rate']:.1f}%)\"\n",
    "    )\n",
    "    print(f\"  - {stats_select['n_pairs']} neuron-feature pairs\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error creating selectivity heatmap: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disentanglement heatmap\n",
    "if disent_matrix5 is not None and count_matrix5 is not None:\n",
    "    try:\n",
    "        print(\"Creating disentanglement heatmap...\")\n",
    "        fig_disent, ax_disent = driada.intense.plot_disentanglement_heatmap(\n",
    "            disent_matrix5,\n",
    "            count_matrix5,\n",
    "            feat_names5,\n",
    "            title=\"Feature disentanglement analysis\",\n",
    "            figsize=(10, 8),\n",
    "        )\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating disentanglement heatmap: {str(e)}\")\n",
    "else:\n",
    "    print(\"No disentanglement results to visualize\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
